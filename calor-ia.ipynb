{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/santyagoamaya/calor-ia?scriptVersionId=270090647\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"681e3d88","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:23:29.318741Z","iopub.status.busy":"2025-10-22T22:23:29.318329Z","iopub.status.idle":"2025-10-22T22:24:11.285302Z","shell.execute_reply":"2025-10-22T22:24:11.284079Z"},"papermill":{"duration":41.979955,"end_time":"2025-10-22T22:24:11.28792","exception":false,"start_time":"2025-10-22T22:23:29.307965","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m396.1/396.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","langchain 0.3.18 requires langsmith<0.4,>=0.1.17, but you have langsmith 0.4.37 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m"]}],"source":["#Remove conflicting packages from the Kaggle base environment.Â¶\n","!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n","!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'"]},{"cell_type":"code","execution_count":2,"id":"91449a9c","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:11.306708Z","iopub.status.busy":"2025-10-22T22:24:11.306312Z","iopub.status.idle":"2025-10-22T22:24:27.974968Z","shell.execute_reply":"2025-10-22T22:24:27.973186Z"},"papermill":{"duration":16.680127,"end_time":"2025-10-22T22:24:27.97689","exception":false,"start_time":"2025-10-22T22:24:11.296763","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n","  warn(\n"]}],"source":["import numpy as np \n","import pandas as pd \n","import os\n","import operator\n","import re # Import regex for parsing\n","import json\n","import io\n","import sys\n","\n","\n","\n","from langchain_core.messages import BaseMessage, HumanMessage, AIMessage # Import AIMessage\n","# from langchain_openai import ChatOpenAI # Replace with your desired LLM provider\n","from langgraph.graph import StateGraph, END\n","# Kaggle and Google AI\n","from kaggle_secrets import UserSecretsClient\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from google import genai\n","from google.genai import types\n","from google.api_core import retry\n","\n","# IPython Display\n","from IPython.display import Markdown, Image, display\n","\n","# PDF Processing\n","import pypdf\n","\n","# ChromaDB\n","#import chromadb\n","#from chromadb import Documents, EmbeddingFunction, Embeddings\n","\n","\n","from typing import TypedDict, Annotated, Optional, Literal, List, Dict, Any\n","from typing_extensions import TypedDict # \n","from langchain_core.messages import BaseMessage \n","from contextlib import redirect_stdout\n","\n","# Langchain and Langgraph\n","from langgraph.graph import StateGraph, START, END\n","from langgraph.graph.message import add_messages\n","from langchain_core.messages import BaseMessage, HumanMessage, AIMessage # Ensure these are imported\n","\n","\n","# Pretty Print\n","from pprint import pprint\n","user_secrets = UserSecretsClient()\n","secret_value_0 = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n","train, test, submission = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')"]},{"cell_type":"markdown","id":"645e2b65","metadata":{"papermill":{"duration":0.008467,"end_time":"2025-10-22T22:24:27.996154","exception":false,"start_time":"2025-10-22T22:24:27.987687","status":"completed"},"tags":[]},"source":["# **ğŸ¯ğŸš€We will defined langgraph who will coordinate our data analyst and data scienctist nodesğŸ¯ğŸš€**\n","# **Lets build a ğŸ¤– herarchical ğŸ¤– architecture**\n","# 1. supervisor\n","# 2. data scientist\n","# 3. data analyst\n","# 4. outputs to Human interpreter "]},{"cell_type":"code","execution_count":3,"id":"801f4aa8","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.013756Z","iopub.status.busy":"2025-10-22T22:24:28.013408Z","iopub.status.idle":"2025-10-22T22:24:28.021759Z","shell.execute_reply":"2025-10-22T22:24:28.020471Z"},"papermill":{"duration":0.019745,"end_time":"2025-10-22T22:24:28.023926","exception":false,"start_time":"2025-10-22T22:24:28.004181","status":"completed"},"tags":[]},"outputs":[],"source":["# Using simple dicts for messages initially for clarity, but BaseMessage[] is better practice# Example: messages: Annotated[list[BaseMessage], add_messages]\n","# Using simple dicts for messages initially for clarity, but BaseMessage[] is better practice# Example: messages: Annotated[list[BaseMessage], add_messages]\n","# --- State Definition ---\n","class GraphState(TypedDict):\n","    \"\"\"\n","    ğŸ—‚ï¸ Central state definition for the CALOR-IA multi-agent workflow.\n","    Reflects using DataFrame variable names instead of file paths.\n","    \"\"\"\n","    messages: Annotated[List[BaseMessage], operator.add]\n","    supervisor_tasks: Optional[List[str]]\n","    current_task_description: Optional[str]\n","    analyst_output: Optional[Dict[str, Any]]\n","    analyst_llm_output: Optional[Dict[str, Any]]\n","    scientist_llm_output: Optional[Dict[str, Any]]\n","    scientist_output: Optional[Dict[str, Any]]\n","    code_final: Optional[Dict[str, Any]]\n","    processed_train_name: Optional[str]     \n","    processed_test_name: Optional[str]\n","    loop_process: bool \n","    loop:  Optional[int]\n","    error: Optional[str]\n","    next_agent: Optional[Literal[\"DataAnalyst\",  \"DataScientist\", \"DataAnalystIA\",\"DataScientistIA\", \"Supervisor\", \"code_compiler_AI\",\"HumanInterpreter\", \"__end__\"]] \n","    interpreter_finished : bool\n","    final_answer_generated: bool # Added for clarity\n"]},{"cell_type":"code","execution_count":4,"id":"88941242","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.043634Z","iopub.status.busy":"2025-10-22T22:24:28.043243Z","iopub.status.idle":"2025-10-22T22:24:28.052164Z","shell.execute_reply":"2025-10-22T22:24:28.050742Z"},"papermill":{"duration":0.021428,"end_time":"2025-10-22T22:24:28.054342","exception":false,"start_time":"2025-10-22T22:24:28.032914","status":"completed"},"tags":[]},"outputs":[],"source":["CALOR_IA_DATA_ANALYST_SYSINT = (\n","\"system\",\n","\"\"\"\n","ğŸ¤– You are CALOR-IA Data Analyst Bot.\n","ğŸ¯ Mission\n","1ï¸âƒ£ Execute the data analysis task provided in the current_task_description from the Supervisor.\n","2ï¸âƒ£ Write clean, runnable Python ğŸ code for data loading, cleaning, exploration, and visualization (using pandas, matplotlib, seaborn, etc.).\n","3ï¸âƒ£ Analyze data (potentially loaded from intermediate_data_path if provided) and extract valuable insights.\n","4ï¸âƒ£ **Prepare data for the Data Scientist, with a strong emphasis on creating high-impact features that are highly relevant to the task's expected goals and downstream modeling requirements. Actively evaluate engineered features to demonstrate their potential value.** Update intermediate_data_path if new data artifacts are created.\n","5ï¸âƒ£ Place your results (code, summary of findings, paths to saved plots or data) into the analyst_output dictionary in the graph state.\n","6ï¸âƒ£ Collaborate with CALOR_IA_DATA_SCIENTIST_SYSINT via the Supervisor by providing necessary data artifacts and insights.\n","\n","ğŸ“ Style & Rules\n","  * Assure you use inputs train, test, submission = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')--\n","- Drop id column to avoid errors\n","- Do not create data, just prepare the datase and provide ideas to improve dataset\n","- train, test = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\n","- You must implement data augmentation:\n","    Practical Workflow Example\n","    Suppose you have a dataset of various food items (rows), containing both:\n","    â€“ Categorical features: type of cuisine, dietary restrictions (vegan, gluten-free), meal type (breakfast, lunch, dinner), etc.\n","    â€“ Numeric features: grams of fat, grams of protein, grams of carbohydrates, total weight.\n","    â€“ Target: total calories (a continuous numeric value).\n","    You might:\n","    (a) Use feature engineering: create macros ratio or density features.\n","    (b) Train a CTGAN to generate additional synthetic food items after confirming it handles both numeric and categorical columns well.\n","    (c) Append sampled rows that look realistic to your training data.\n","    (d) Evaluate your model (e.g., gradient boosted trees, random forest, or a neural network) to see if it outperforms a baseline.\n","    (e) Monitor overfitting or unrealistic data by analyzing the distribution of the synthesized rows.\n","\n","return python code do not loose any piece of code \n","\"\"\"\n",")\n","\n","CALOR_IA_DATA_SCIENTIST_SYSINT = (\n","    \"system\",\n","    \"\"\"\n","    ğŸ¤– You are CALOR-IA Data Scientist Bot.\n","\n","\n","      * Assure you use inputs train, test = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\n","\n","    ğŸ¯ Mission\n","     You are a data scientist how has to reduce REMSLE error and other functions, also you have to apply k flod validation to avoid overfitting\n","     You must improve the provided code, that could be implementing a model or make something of the provided pipeline of models and ensamble\n","     return python code do not loose any piece of code \n","\"\"\")\n","\n","CALOR_IA_CODE_COMPILER_SYSINT = (\n","    \"system\",\n","    \"\"\"\n","    ğŸ¤– CALOR-IA Code Compiler Bot: Blending code fragments for Kaggle compatibility\n","\n","\n","    * Assure you use inputs train, test = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\n","\n","    ğŸ¯ Primary Mission:\n","    1ï¸âƒ£ Blend/merge code fragments from different sources into ONE cohesive, runnable script\n","    !Note separe train['Calories'] as predicted value to avoid conflicts with train and test preprocessing \n","    Assure all libraries are imported correctly\n","    inputs : train, test, submission = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')\n","    2ï¸âƒ£ Ensure the code runs flawlessly in Kaggle environment\n","            \n","    ğŸ“‹ Technical Requirements:\n","    \n","    â€¢ Resolve all variable references between blended code sections\n","    â€¢ Ensure all imports are at the top of the script\n","    â€¢ Be carefull with shape handle \n","    â€¢ build robust prediction using a blend method of models\n","    â€¢ Assure ejecution won't run for ever\n","    â€¢ Fix any syntax errors or logical inconsistencies\n","    â€¢ Keep all viable modeling code (LightGBM, XGBoost, Random Forest, etc.)\n","    â€¢ Implement ensemble prediction by combining model outputs\n","    â€¢ Assure continuity on the logic is correct\n","    â€¢ Generate a final submission.csv file\n","    \n","    \n","    ğŸš« Common Issues to Fix:\n","    â€¢ Inconsistent variable names between code segments\n","    â€¢ Missing function definitions or imports\n","    â€¢ Incorrect file paths for Kaggle environment\n","    â€¢ Syntax errors or incomplete code blocks\n","    \n","    ğŸ” Return Format:\n","    Return ONLY the complete, ready-to-run Python script as a single coherent block.\n","    \n","    Let's create Kaggle-ready code that works right out of the box!\n","    \"\"\"\n",")"]},{"cell_type":"code","execution_count":5,"id":"fbe264ee","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.072192Z","iopub.status.busy":"2025-10-22T22:24:28.071784Z","iopub.status.idle":"2025-10-22T22:24:28.091722Z","shell.execute_reply":"2025-10-22T22:24:28.090461Z"},"papermill":{"duration":0.031182,"end_time":"2025-10-22T22:24:28.093611","exception":false,"start_time":"2025-10-22T22:24:28.062429","status":"completed"},"tags":[]},"outputs":[],"source":["def supervisor_node(state: GraphState) -> Dict[str, Any]:\n","    \"\"\"\n","    Central decision-making node. Routes tasks to appropriate agents\n","    or ends the workflow based on the current state.\n","    Now routes to DataAnalyst_AI AFTER HumanInterpreter.\n","    \"\"\"\n","    print(\"\\n--- SUPERVISOR ---\")\n","    messages = state.get(\"messages\", [])\n","    last_message = messages[-1] if messages else None\n","    print(f\"Supervisor reviewing state. Last message type: {type(last_message).__name__ if last_message else 'None'}\")\n","    if hasattr(last_message, 'content'):\n","         content_display = str(last_message.content)\n","         print(f\"Supervisor received content: {content_display[:200]}{'...' if len(content_display) > 200 else ''}\")\n","\n","    # --- Retrieve outputs from potential sources ---\n","    analyst_orig_output = state.get('analyst_output')\n","    scientist_output_data = state.get('scientist_output')\n","    analyst_llm_output = state.get('analyst_llm_output')\n","    scientist_llm_output = state.get('scientist_llm_output')\n","    code = state.get('code_final')\n","    loop_process = state.get('loop_process', False)\n","    interpreter_finished = state.get('interpreter_finished', False)\n","    final_answer_generated = state.get('final_answer_generated', False)\n","    initial_train_name = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv')\n","    initial_test_name = pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\n","    tasks = state.get('supervisor_tasks', [])\n","    error_flag = state.get('error')\n","    loop = state.get('loop')\n","    print(f\"Original Analyst output available: {analyst_orig_output is not None}\")\n","    print(f\"Scientist output available: {scientist_output_data is not None}\")\n","    print(f\"Interpreter finished signal: {interpreter_finished}\")\n","    print(f\"Final answer generated flag: {final_answer_generated}\")\n","    print(f'loop =======> # {loop}')\n","    next_agent = None\n","    task = None\n","    new_message_content = None\n","    updated_state_for_return = {}\n","\n","    if loop == 0 and final_answer_generated:\n","        next_agent = '__end__'\n","        new_message_content = \"final script generated.\" \n","        return {\"next_agent\": next_agent}\n","\n","    if not loop_process and final_answer_generated:\n","        next_agent = \"DataAnalyst_AI\"\n","        new_message_content = \"another iteration over data analyst.\"     \n","        if loop == None:\n","            loop = 0\n","        updated_state_for_return['final_answer_generated'] = False\n","        \n","        \n","    if loop_process and not final_answer_generated:\n","        next_agent = \"code_compiler_AI\"\n","        new_message_content = \"Workflow complete. Final script and reviews have been generated.\"\n","        updated_state_for_return['final_answer_generated'] = True  # Force set this flag\n","\n","    # --- Decision Logic (Ordered by Priority) ---\n","    if loop_process:\n","        print('Analyst reviewed code and provide an improvement')\n","        loop_process = False\n","        next_agent = \"DataScientist_AI\"\n","        final_answer_generated = False\n","        new_message_content = \"Analyst AI review complete. Routing compiled script to Data Scientist AI for review.\"\n","        \n","    # 1. Check for final answer signal - highest priority\n","    if final_answer_generated:\n","        print(\"Supervisor: Final answer signal received. Ending workflow.\")\n","        next_agent = \"DataAnalyst_AI\"\n","        new_message_content = \"Workflow complete. lets loop over trough all specialist one more time.\"\n","    # 2. Check if Scientist AI has completed review\n","\n","    \n","    elif scientist_llm_output is not None and not final_answer_generated:\n","        print(\"Supervisor: Scientist AI review complete. Final answer should be ready.\")\n","        # Force end the workflow since we've gone through all steps\n","        next_agent = \"code_compiler_AI\"\n","        new_message_content = \"Workflow complete. Final script and reviews have been generated.\"\n","        updated_state_for_return['final_answer_generated'] = True  # Force set this flag\n","    \n","    # 3. Check if Analyst AI has finished its review - Next step is Scientist AI\n","    elif analyst_llm_output is not None and scientist_llm_output is None:\n","        print(\"Supervisor: Analyst AI review complete. Routing to Scientist AI for review.\")\n","        next_agent = \"DataScientist_AI\"\n","        new_message_content = \"Analyst AI review complete. Routing compiled script to Data Scientist AI for review.\"\n","    \n","    # 4. Check if Interpreter has finished - Start AI review chain\n","    elif interpreter_finished and analyst_llm_output is None:\n","        # Reset the flag to prevent re-triggering\n","        updated_state_for_return['interpreter_finished'] = False\n","        print(\"Supervisor: Interpreter finished compiling script. Routing to Data Analyst AI for review.\")\n","        next_agent = \"DataAnalyst_AI\"\n","        new_message_content = \"Interpreter step complete. Routing compiled script for AI review.\"\n","    \n","    # 5. Check for error condition\n","    elif error_flag is not None:\n","        print(f\"Supervisor: Error detected in state: {error_flag}. Routing to Human Interpreter for review.\")\n","        next_agent = \"HumanInterpreter\"\n","    \n","    # 6. Check for Scientist output data - Route to Interpreter\n","    elif scientist_output_data is not None and not interpreter_finished:\n","        print(\"Supervisor: Scientist code generation complete. Routing to HumanInterpreter for compilation.\")\n","        next_agent = \"HumanInterpreter\"\n","        new_message_content = \"Scientist code generation complete. Compiling final script.\"\n","        updated_state_for_return[\"processed_train_name\"] = state.get(\"processed_train_name\")\n","        updated_state_for_return[\"processed_test_name\"] = state.get(\"processed_test_name\")\n","        print(f\"Supervisor propagating names from state for next step: train='{updated_state_for_return.get('processed_train_name')}', test='{updated_state_for_return.get('processed_test_name')}'\")\n","    \n","    # 7. Check for original DataAnalyst output - Route to DataScientist\n","    elif analyst_orig_output is not None and scientist_output_data is None:\n","        print(\"Supervisor: Original Data Analyst successfully generated code and data names. Routing to Data Scientist.\")\n","        updated_state_for_return[\"processed_train_name\"] = analyst_orig_output.get('processed_train_name')\n","        updated_state_for_return[\"processed_test_name\"] = analyst_orig_output.get('processed_test_name')\n","        print(f\"Supervisor propagating names from original analyst output to Scientist: train='{updated_state_for_return.get('processed_train_name')}', test='{updated_state_for_return.get('processed_test_name')}'\")\n","        task = \"Build and evaluate LightGBM and XGBoost models using the processed data.\"\n","        next_agent = \"DataScientist\"\n","    \n","    # 8. Initial state handling\n","    elif 'initial_input_message' in locals() or 'initial_input_message' in globals():\n","        print(\"Supervisor: Initial human message received. Assigning initial task to Original Data Analyst.\")\n","        next_agent = \"DataAnalyst\"\n","        new_message_content = \"Okay, starting the data analysis workflow with the Data Analyst.\"\n","        task = \"Perform initial EDA and preprocessing on train/test data.\"\n","    \n","    # 9. Unexpected state - End workflow\n","    else:\n","        print(\"Supervisor: Unexpected state - no initial message or prior agent output. Ending.\")\n","        error_flag = \"Workflow ended in an unexpected state.\"\n","        next_agent = \"__end__\"\n","        new_message_content = \"Workflow terminated due to an unexpected state.\"\n","\n","    # --- Prepare Return Dictionary ---\n","    updated_messages = list(messages)\n","    if new_message_content is not None:\n","        if not updated_messages or (hasattr(updated_messages[-1], 'content') and updated_messages[-1].content != new_message_content):\n","            updated_messages.append(AIMessage(content=str(new_message_content)))\n","\n","    return_dict = {\n","        \"messages\": updated_messages,\n","        \"supervisor_tasks\": tasks,\n","        \"current_task_description\": task,\n","        \"next_agent\": next_agent,\n","        \"error\": error_flag,\n","        \"final_answer_generated\": updated_state_for_return.get('final_answer_generated', final_answer_generated),\n","        \"interpreter_finished\": updated_state_for_return.get('interpreter_finished', interpreter_finished),\n","        \"initial_train_name\": initial_train_name,\n","        \"initial_test_name\": initial_test_name,\n","        \"loop_process\" : loop_process,\n","        'loop' : loop,\n","        **updated_state_for_return\n","    }\n","\n","    print(f\"DEBUG: Supervisor RETURNING next_agent: '{return_dict['next_agent']}' (Type: {type(return_dict['next_agent'])})\")\n","    print(f\"DEBUG: Supervisor RETURNING final_answer_generated: {return_dict.get('final_answer_generated')}\")\n","    print(f\"DEBUG: Supervisor RETURNING interpreter_finished: {return_dict.get('interpreter_finished')}\")\n","\n","    return return_dict"]},{"cell_type":"code","execution_count":6,"id":"01c06a81","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.111269Z","iopub.status.busy":"2025-10-22T22:24:28.110925Z","iopub.status.idle":"2025-10-22T22:24:28.124898Z","shell.execute_reply":"2025-10-22T22:24:28.1238Z"},"papermill":{"duration":0.025594,"end_time":"2025-10-22T22:24:28.127196","exception":false,"start_time":"2025-10-22T22:24:28.101602","status":"completed"},"tags":[]},"outputs":[],"source":["def data_analyst_node(state: GraphState) -> Dict[str, Any]:\n","    \"\"\"Data Analyst Node - Updated for DataFrame variable names.\"\"\"\n","    print(\"\\n--- DATA ANALYST ---\")\n","    task = state.get('current_task_description')\n","    # Assume initial DFs are named 'train_df' and 'test_df' for clarity\n","    # Adjust these names if your actual initial variables are different (e.g., 'train', 'test')\n","    initial_train_name = state.get('initial_train_name')# <-- Adjust if your initial var is different\n","    initial_test_name =  state.get('initial_test_name') # <-- Adjust if your initial var is different\n","\n","    if not task:\n","        error_output = {\"error\": \"Data Analyst received no task.\", \"next_agent\": \"Supervisor\"}\n","        print(\"Returning error output:\", error_output)\n","        return error_output\n","\n","    # Simulate the names of the DataFrames that the processing code will create\n","    processed_train_name = f\"processed_{initial_train_name}\" \n","    processed_test_name = f\"processed_{initial_test_name}\"   \n","\n","    print(f\"Analyst expects input DataFrame variable: '{initial_train_name}'\")\n","    print(f\"Analyst expects input DataFrame variable: '{initial_test_name}'\")\n","    print(f\"Analyst will simulate creating output DataFrames: '{processed_train_name}' and '{processed_test_name}'\")\n","    # inroducing SATYA create_predictor_features\n","    # --- Simulate Code (Make sure variable names match below) ---\n","    \n","    simulated_code =  generated_code = (f\"\"\"\n","import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, QuantileTransformer\n","from sklearn.impute import KNNImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestRegressor # This import might be needed in the generated code\n","\n","# --- Helper Functions (Copying the ones provided by user) ---\n","# Assuming these were defined or should be included in the generated code\n","\n","# Define helper functions here (copying from user's input)\n","# ... (your create_features, swish, root_mean_squared_error, build_swish_mlp, rmsle functions go here)\n","def create_features(df):\n","    '''\n","    Create advanced features from the input data.\n","    This function serves as a form of \"data augmentation\" for tabular data\n","    by deriving new, potentially more informative features from existing ones.\n","    '''\n","    df = df.copy()\n","    epsilon = 1e-6 # Small constant to avoid division by zero\n","\n","    # --- Basic Interaction & Ratio Features (>= 6 columns engineered) ---\n","\n","    # 1. Body Mass Index (BMI): Standard health metric, relates weight and height.\n","    df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2 + epsilon)\n","\n","    # 2. Duration * Heart_Rate: Represents total heartbeats during exercise, proxy for effort.\n","    df['Duration_HR'] = df['Duration'] * df['Heart_Rate']\n","\n","    # 3. Duration * Body_Temp: Represents total heat generated during exercise, proxy for intensity/effort.\n","    df['Duration_Temp'] = df['Duration'] * df['Body_Temp']\n","\n","    # 4. Weight / Height Ratio: Simpler ratio than BMI, might capture different linear relationships.\n","    df['Weight_Height_Ratio'] = df['Weight'] / (df['Height'] + epsilon)\n","\n","    # 5. Heart Rate per Minute: Already captured by Heart_Rate, but could consider rate relative to duration?\n","    #    Let's create Heart_Rate_per_Duration instead.\n","    df['Heart_Rate_per_Duration'] = df['Heart_Rate'] / (df['Duration'] + epsilon) # Rate of heartbeats per minute of exercise\n","\n","    # 6. Body Temperature per Minute: Change in temp per minute of exercise.\n","    df['Body_Temp_per_Duration'] = df['Body_Temp'] / (df['Duration'] + epsilon)\n","\n","    # --- More Complex Interaction Features ---\n","\n","    # 7. Exercise Intensity: Combines HR, Duration, and inversely related to Age (older might have lower max HR).\n","    df['Exercise_Intensity'] = (df['Heart_Rate'] * df['Duration']) / (df['Age'] + epsilon)\n","\n","    # 8. Metabolic Factor: Combines BMI, Duration, HR, and inversely related to Age. A complex interaction term.\n","    df['Metabolic_Factor'] = (df['BMI'] * df['Duration'] * df['Heart_Rate']) / (df['Age'] + epsilon)\n","\n","    # 9. Age * Duration Interaction: Older individuals exercising for longer might have different calorie burn patterns.\n","    df['Age_Duration_Interaction'] = df['Age'] * df['Duration']\n","\n","    # 10. Height * Weight Product: Simple interaction, might capture overall body size effect differently than BMI.\n","    df['Height_Weight_Product'] = df['Height'] * df['Weight']\n","\n","    # --- Polynomial Features (Example for Duration) ---\n","    # Captures non-linear relationship with Duration.\n","    df['Duration_Sq'] = df['Duration']**2\n","\n","    # --- Categorical Features (Discretization) ---\n","\n","    # 11. Age Group: Discretizing Age can help capture non-linear effects or group-specific patterns.\n","    df['Age_Group'] = pd.cut(df['Age'], bins=[0, 25, 35, 45, 55, 65, 100], labels=['<25', '25-35', '35-45', '45-55', '55-65', '65+'], right=False)\n","    df['Age_Group'] = df['Age_Group'].astype(object).fillna('Unknown')\n","\n","\n","    # 12. BMI Category: Standard health categories for BMI.\n","    df['BMI_Category'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, 35, 40, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese I', 'Obese II', 'Obese III'], right=False)\n","    df['BMI_Category'] = df['BMI_Category'].astype(object).fillna('Unknown')\n","\n","\n","    # 13. Heart Rate Zones (Simplified): Based on max HR (approx 220-Age) and exercise intensity.\n","    df['Max_HR_Estimate'] = 220 - df['Age']\n","    df['HR_Zone'] = pd.cut(df['Heart_Rate'] / (df['Max_HR_Estimate'] + epsilon),\n","                           bins=[0, 0.6, 0.7, 0.8, 0.9, 1.1], # Example zones based on % of max HR\n","                           labels=['<60%', '60-70%', '70-80%', '80-90%', '>90%'],\n","                           right=False)\n","    df['HR_Zone'] = df['HR_Zone'].astype(object).fillna('Unknown')\n","    df = df.drop('Max_HR_Estimate', axis=1) # Drop intermediate column\n","\n","\n","    # 14. Duration Category: Simple bins for exercise duration.\n","    df['Duration_Category'] = pd.cut(df['Duration'], bins=[0, 15, 30, 45, 60, 120, 300], labels=['<15min', '15-30min', '30-45min', '45-60min', '1-2hr', '>2hr'], right=False)\n","    df['Duration_Category'] = df['Duration_Category'].astype(object).fillna('Unknown')\n","\n","\n","    # Interaction between Sex and Duration/HeartRate/BodyTemp\n","    df['Sex_Male_Duration'] = df['Duration'] * (df['Sex'] == 'M')\n","    df['Sex_Female_Duration'] = df['Duration'] * (df['Sex'] == 'F')\n","    df['Sex_Male_HR'] = df['Heart_Rate'] * (df['Sex'] == 'M')\n","    df['Sex_Female_HR'] = df['Heart_Rate'] * (df['Sex'] == 'F')\n","\n","\n","    return df\n","\n","\n","# --- Load Data (Assuming 'train' and 'test' DataFrames are already loaded) ---\n","print(\"Using input DataFrames named '{initial_train_name}' and '{initial_test_name}'...\")\n","#Assuming 'y' and 'submission' also exist\n","If 'id' and 'Calories' need dropping, it should happen *before* feature engineering on the original DFs.\n","# Let's assume the initial DataFrames passed in via '{initial_train_name}' and '{initial_test_name}'\n","# are already prepped (ids dropped, target 'y' separated).\n","If not, add that step here:\n","y = {initial_train_name}['Calories'] \n","# {initial_train_name} = {initial_train_name}.drop('Calories', axis=1) # Example drop\n","\n","# Example: If train_df/test_df *contain* id/Calories\n","# train_df_working = {initial_train_name}.copy()\n","# test_df_working = {initial_test_name}.copy()\n","# if 'id' in train_df_working.columns: train_ids = train_df_working['id'] ; train_df_working = train_df_working.drop('id', axis=1)\n","# if 'Calories' in train_df_working.columns: y = train_df_working['Calories'] ; train_df_working = train_df_working.drop('Calories', axis=1)\n","# if 'id' in test_df_working.columns: test_ids = test_df_working['id'] ; test_df_working = test_df_working.drop('id', axis=1)\n","# print(f\"Initial train shape: {{train_df_working.shape}}\")\n","# print(f\"Initial test shape: {{test_df_working.shape}}\")\n","\n","\n","# --- Feature Engineering ---\n","print(\"\\nApplying feature engineering...\")\n","# Apply feature engineering to the working copies (or original DFs if pre-cleaned)\n","# Using the assumed initial variable names from the state.\n","X_train_fe = create_features({initial_train_name}) \n","X_test_fe = create_features({initial_test_name})\n","\n","print(f\"Train shape after feature engineering: {{X_train_fe.shape}}\")\n","print(f\"Test shape after feature engineering: {{X_test_fe.shape}}\")\n","\n","# Identify numerical and categorical features AFTER feature engineering\n","# Ensure handling of potential errors if a feature was expected but not created\n","all_features = list(X_train_fe.columns)\n","numerical_features = [col for col in all_features if X_train_fe[col].dtype in ['int64', 'float64']]\n","categorical_features = [col for col in all_features if X_train_fe[col].dtype == 'object'] # Using object dtype for pandas categorical/string\n","\n","print(f\"Numerical features ({{len(numerical_features)}}): {{numerical_features}}\")\n","print(f\"Categorical features ({{len(categorical_features)}}): {{categorical_features}}\")\n","\n","# --- Preprocessing Pipeline (KNN Imputation & Quantile Transformation + OneHot) ---\n","print(\"\\nSetting up preprocessing pipeline...\")\n","\n","# Check if numerical/categorical features exist before creating pipelines\n","transformers = []\n","if numerical_features:\n","    numerical_pipeline = Pipeline([\n","        ('imputer', KNNImputer(n_neighbors=5)),       # KNN imputation\n","        ('scaler', QuantileTransformer(output_distribution='normal')) # Robust scaling via QuantileTransformer\n","    ])\n","    transformers.append(('num', numerical_pipeline, numerical_features))\n","else:\n","    print(\"No numerical features found. Skipping numerical pipeline.\")\n","\n","if categorical_features:\n","    categorical_pipeline = Pipeline([\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)) # One-Hot Encoding\n","    ])\n","    transformers.append(('cat', categorical_pipeline, categorical_features))\n","else:\n","     print(\"No categorical features found. Skipping categorical pipeline.\")\n","\n","# Create ColumnTransformer only if there are transformers\n","if transformers:\n","    preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n","\n","    # Fit and Transform data\n","    print(\"Fitting and transforming data with preprocessor...\")\n","    # *** Assign results to the variable names specified by the analyst node ***\n","    {processed_train_name} = preprocessor.fit_transform(X_train_fe)\n","    {processed_test_name} = preprocessor.transform(X_test_fe)\n","\n","    print(f\"Processed train shape (variable '{processed_train_name}'): {{{processed_train_name}.shape}}\")\n","    print(f\"Processed test shape (variable '{processed_test_name}'): {{{processed_test_name}.shape}}\")\n","\n","else:\n","    print(\"No features identified for preprocessing. Processed data will be empty.\")\n","    # Handle case where no features are processed - might need to create empty arrays\n","    {processed_train_name} = np.empty((X_train_fe.shape[0], 0)) # Create empty numpy array with correct rows\n","    {processed_test_name} = np.empty((X_test_fe.shape[0], 0))\n","    print(f\"Processed train shape (variable '{processed_train_name}'): {{{processed_train_name}.shape}}\")\n","    print(f\"Processed test shape (variable '{processed_test_name}'): {{{processed_test_name}.shape}}\")\n","\n","\n","# The processed data is now available as numpy arrays named '{processed_train_name}' and '{processed_test_name}'\n","# The target variable 'y' is assumed to also be available from the initial loading step.\n","# This completes the Analyst's task of preparing the data for the Scientist.\n","\n","\"\"\")\n","\n","\n","    return {\n","        \"analyst_output\": {\n","            \"code\": simulated_code,\n","            \"processed_train_name\": processed_train_name,\n","            \"processed_test_name\": processed_test_name,\n","        },\n","        \"current_task_description\": None,\n","    }\n","    return{\n","        \"analyst_output\":{\n","            \"code\": simulated_code,\n","            \"processed_train_name\": processed_train_name,\n","            \"processed_test_name\": processed_test_name,\n","        },\n","         \"current_task_description\": None, # Task is complete for Analyst\n","    }"]},{"cell_type":"code","execution_count":7,"id":"9e13d7cd","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.14597Z","iopub.status.busy":"2025-10-22T22:24:28.145622Z","iopub.status.idle":"2025-10-22T22:24:28.168472Z","shell.execute_reply":"2025-10-22T22:24:28.167021Z"},"papermill":{"duration":0.035191,"end_time":"2025-10-22T22:24:28.170431","exception":false,"start_time":"2025-10-22T22:24:28.13524","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","from typing import Dict, Any\n","\n","# Assuming GraphState class is defined elsewhere\n","# Assuming necessary library imports (like pandas, numpy etc.) are at the top level of your script\n","\n","def data_scientist_node(state: GraphState) -> Dict[str, Any]:\n","    \"\"\"\n","    Data Scientist Node - Generates modeling code (LightGBM, XGBoost)\n","    using processed data names provided by the DataAnalyst node via the state.\n","    This version DOES NOT execute the generated code.\n","    \"\"\"\n","    print(\"\\n--- DATA SCIENTIST (Code Generation Only) ---\")\n","    task = state.get('current_task_description')\n","    if not task:\n","        print(\"Scientist: No task received.\")\n","        # Return an error structure within the expected output key\n","        return {\n","            \"scientist_output\": {\"error\": \"Data Scientist received no task.\"},\n","            \"next_agent\": \"Supervisor\"\n","            }\n","\n","    # --- Get processed DataFrame names from the analyst's output in state ---\n","    # The analyst node puts these names in 'analyst_output' dictionary\n","    analyst_result = state.get('analyst_output')\n","    if not analyst_result or not isinstance(analyst_result, dict):\n","        error_msg = \"Scientist Error: Analyst output not found or invalid in state.\"\n","        print(error_msg)\n","        return {\n","            \"scientist_output\": {\"error\": error_msg},\n","             \"next_agent\": \"Supervisor\"\n","             }\n","\n","\n","    # Get the variable names *as they will be created* by the analyst's code\n","    processed_train_name = analyst_result.get('processed_train_name')\n","    processed_test_name  = analyst_result.get('processed_test_name')\n","    # Assume the analyst's code also creates a variable named 'target'\n","    # This name is hardcoded based on the analyst's provided simulated code structur\n","\n","    if not processed_train_name or not processed_test_name:\n","        error_msg = \"Scientist Error: Processed DataFrame names not found within analyst output in state.\"\n","        print(error_msg)\n","        return {\n","            \"scientist_output\": {\"error\": error_msg},\n","             \"next_agent\": \"Supervisor\"\n","             }\n","\n","    print(f\"Scientist generating code for task: {task}\")\n","    print(f\"Will use processed train DataFrame variable: '{processed_train_name}'\")\n","    print(f\"Will use processed test DataFrame variable : '{processed_test_name}'\")\n","\n","\n","    # --- Generate Code String ---\n","    # Use single braces {} ONLY for variables substituted NOW ({processed_train_name}, etc.)\n","    # Use double braces {{}} for f-strings intended for LATER execution within the generated code.\n","    generated_code = (f'''\n","# --- Imports for the generated modeling script ---\n","import pandas as pd # Might be needed if submission is loaded/modified\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_log_error\n","import lightgbm as lgb\n","import xgboost as xgb\n","from catboost import CatBoostRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# expected_shape should be evaluated within the generated code\n","expected_shape = ({processed_test_name}.shape[0] if '{processed_test_name}' in locals() else 0)\n","\n","# Import TensorFlow components if available\n","try:\n","    import tensorflow as tf\n","    import tensorflow.keras.backend as K\n","    from tensorflow.keras.layers import Dense, Input\n","    from tensorflow.keras.models import Model\n","    from tensorflow.keras.optimizers import Adam\n","except ImportError:\n","    print(\"TensorFlow not available. TensorFlow model will be skipped.\")\n","    tf = None # Set tf to None to check availability later\n","\n","# --- Helper Functions (Copying the ones from Analyst's generated code) ---\n","# Ensure these match what the analyst generated, especially the RMSLE and TF functions\n","\n","def swish(x):\n","    # Need to handle both numpy (for direct use if needed) and tensorflow\n","    if tf and isinstance(x, tf.Tensor):\n","         return x * tf.keras.backend.sigmoid(x)\n","    else: # Assume numpy or similar\n","         # This numpy version is just for local testing outside TF context if needed\n","         # Actual TF model will use the TF version\n","         return x / (1 + np.exp(-x)) # Sigmoid approximation for numpy\n","\n","def root_mean_squared_error(y_true, y_pred):\n","    # Need to handle both numpy and tensorflow\n","    if tf and isinstance(y_true, tf.Tensor):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","    else: # Assume numpy\n","        return np.sqrt(np.mean(np.square(y_pred - y_true)))\n","\n","\n","def build_swish_mlp(input_shape):\n","    if tf is None:\n","        print(\"TensorFlow not available, cannot build swish MLP.\")\n","        return None\n","    inputs = Input(shape=input_shape)\n","    x = Dense(64, activation=swish)(inputs)\n","    x = Dense(128, activation=swish)(x)\n","    x = Dense(64, activation=swish)(x)\n","    x = Dense(1)(x)\n","    model = Model(inputs, x)\n","    # Use RootMeanSquaredError metric for TensorFlow\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","    return model\n","\n","# rmsle function from sklearn.metrics\n","def rmsle(y_true, y_pred):\n","    # Ensure predictions are non-negative for log calculation\n","    y_pred = np.maximum(0, y_pred)\n","    # Ensure inputs are numpy arrays and handle potential shape issues\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","    # Escape braces for f-string inside the generated code\n","    print(f\"{{Debug RMSLE shapes - True: {{y_true.shape}}, Pred: {{y_pred.shape}}}}\") # Corrected escaping\n","    if y_true.shape != y_pred.shape:\n","        print(f\"{{Warning: Shape mismatch between true and predicted values ({{y_true.shape}} vs {{y_pred.shape}}). Cannot compute RMSLE.}}\") # Corrected escaping\n","        return float('inf')\n","    # Add a small epsilon to avoid log(0)\n","    return np.sqrt(mean_squared_log_error(np.maximum(0, y_true) + 1e-9, y_pred + 1e-9))\n","\n","\n","# --- Data Splitting ---\n","print(\"\\nSplitting data into training and validation sets...\")\n","# Use the processed train data variable name from the analyst node\n","# Assuming the target 'y' is also available from the initial data loading/prep\n","# Escape the variables being checked and accessed so they evaluate in the generated code context\n","if '{processed_train_name}' in locals() and 'y' in locals() and {{processed_train_name}}.shape[0] == y.shape[0]:\n","    X_train, X_val, y_train, y_val = train_test_split({processed_train_name}, y, test_size=0.2, random_state=42)\n","    # Escape braces for f-strings inside the generated code\n","    print(f\"{{Train shapes: {{X_train.shape}}, {{y_train.shape}}}}\") # CORRECTED\n","    print(f\"{{Validation shapes: {{X_val.shape}}, {{y_val.shape}}}}\") # CORRECTED\n","    data_split_success = True\n","else:\n","    # Escape braces for f-strings inside the generated code\n","    print(f\"{{Error: Processed train data ('{processed_train_name}') or target ('y') not found or shapes mismatch. Skipping modeling.}}\")\n","    # Use {{ }} for the outer f-string, and {{variable}} for variables inside the inner f-string\n","    print(f\"{{'{processed_train_name}' in locals(): {{'{processed_train_name}' in locals()}}}}\") # CORRECTED\n","    print(f\"{{'y' in locals(): {{'y' in locals()}}}}\") # CORRECTED\n","    # Escape variables/accessors being checked and accessed\n","    if '{processed_train_name}' in locals(): print(f\"{{Shape of '{processed_train_name}': {{locals()['{processed_train_name}'].shape}}}}\") # CORRECTED\n","    if 'y' in locals(): print(f\"{{Shape of 'y': {{y.shape}}}}\") # CORRECTED\n","    X_train, X_val, y_train, y_val = np.array([]).reshape(0,-1), np.array([]).reshape(0,-1), np.array([]), np.array([]) # Create empty arrays\n","    data_split_success = False\n","\n","\n","if data_split_success and X_train.shape[0] > 0:\n","    # --- Scaling for Neural Network ---\n","    # NN often performs better with features scaled to a 0-1 range and log-transformed target\n","    print(\"\\nScaling features for Neural Network using MinMaxScaler...\")\n","    scaler_nn = MinMaxScaler()\n","    X_train_scaled = scaler_nn.fit_transform(X_train)\n","    X_val_scaled = scaler_nn.transform(X_val)\n","    # Scale the actual test set using the processed test data variable name\n","    # Escape variable being checked and accessed\n","    if '{processed_test_name}' in locals():\n","       X_test_scaled = scaler_nn.transform({{processed_test_name}})\n","       test_scaling_success = True\n","    else:\n","       print(f\"{{Error: Processed test data ('{processed_test_name}') not found for scaling.}}\")\n","       X_test_scaled = np.array([]).reshape(0,-1)\n","       test_scaling_success = False\n","\n","\n","    print(\"Log transforming target for NN...\")\n","    y_train_log = np.log1p(y_train)\n","    y_val_log = np.log1p(y_val)\n","\n","    # --- Model Training ---\n","\n","    # Train LightGBM (using split processed data - numpy arrays)\n","    print(\"\\nTraining LightGBM...\")\n","    lgb_model = lgb.LGBMRegressor(objective='regression', metric='rmse', n_estimators=2000, learning_rate=0.03, num_leaves=40, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1, lambda_l1=0.1, lambda_l2=0.1, seed=42, n_jobs=-1, verbose=-1)\n","    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n","    lgb_val_pred = lgb_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if '{processed_test_name}' in locals():\n","       lgb_test_pred = lgb_model.predict({{processed_test_name}})\n","    else:\n","       # Escape variable accessor being accessed in the generated code\n","       lgb_test_pred = np.zeros({{processed_test_name}}.shape[0] if '{processed_test_name}' in locals() else 0) # Predict zeros if test data missing\n","       print(f\"{{Warning: '{processed_test_name}' not available for LightGBM test prediction.}}\")\n","    print(\"LightGBM training complete.\")\n","\n","    # Train XGBoost (using split processed data - numpy arrays)\n","    print(\"\\nTraining XGBoost...\")\n","    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', n_estimators=2000, learning_rate=0.03, max_depth=7, subsample=0.8, colsample_bytree=0.8, lambda_l1=0.1, lambda_l2=0.1, seed=42, n_jobs=-1, tree_method='hist') # Use hist for faster training\n","    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n","    xgb_val_pred = xgb_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if '{processed_test_name}' in locals():\n","       xgb_test_pred = xgb_model.predict({{processed_test_name}})\n","    else:\n","        # Escape variable accessor being accessed in the generated code\n","        xgb_test_pred = np.zeros({{processed_test_name}}.shape[0] if '{processed_test_name}' in locals() else 0)\n","        print(f\"{{Warning: '{processed_test_name}' not available for XGBoost test prediction.}}\")\n","    print(\"XGBoost training complete.\")\n","\n","    # Train RandomForestRegressor (using split processed data - numpy arrays)\n","    print(\"\\nTraining RandomForestRegressor...\")\n","    rf_model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n","    rf_model.fit(X_train, y_train)\n","    rf_val_pred = rf_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if '{processed_test_name}' in locals():\n","        rf_test_pred = rf_model.predict({{processed_test_name}})\n","    else:\n","        # Escape variable accessor being accessed in the generated code\n","        rf_test_pred = np.zeros({{processed_test_name}}.shape[0] if '{processed_test_name}' in locals() else 0)\n","        print(f\"{{Warning: '{processed_test_name}' not available for RandomForest test prediction.}}\")\n","    print(\"RandomForestRegressor training complete.\")\n","\n","    # Train CatBoostRegressor (using split processed data - numpy arrays)\n","    print(\"\\nTraining CatBoostRegressor...\")\n","    cat_model = CatBoostRegressor(iterations=2000, learning_rate=0.03, depth=7, random_seed=42, verbose=0, early_stopping_rounds=100, l2_leaf_reg=3, loss_function='RMSE')\n","    cat_model.fit(X_train, y_train, eval_set=(X_val, y_val))\n","    cat_val_pred = cat_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if '{processed_test_name}' in locals():\n","        cat_test_pred = cat_model.predict({{processed_test_name}})\n","    else:\n","         # Escape variable accessor being accessed in the generated code\n","         cat_test_pred = np.zeros({{processed_test_name}}.shape[0] if '{processed_test_name}' in locals() else 0)\n","         print(f\"{{Warning: '{processed_test_name}' not available for CatBoost test prediction.}}\")\n","    print(\"CatBoostRegressor training complete.\")\n","\n","\n","    # Build and train TensorFlow model (using scaled data)\n","    if tf and test_scaling_success: # Only attempt TF if TF is available and test data was scaled\n","        print(\"\\nBuilding and training TensorFlow model...\")\n","        if X_train_scaled.shape[0] == 0 or X_train_scaled.shape[1] == 0:\n","             print(\"{{Warning: Scaled training data is empty. Skipping TensorFlow model training.}}\") # Corrected escaping\n","             tf_test_pred = np.zeros(X_test_scaled.shape[0])\n","             tf_val_pred = np.zeros(X_val_scaled.shape[0])\n","        else:\n","            tf_model = build_swish_mlp(input_shape=(X_train_scaled.shape[1],))\n","            if tf_model: # Check if model was built successfully\n","                early_stopping_tf = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=15, restore_best_weights=True, mode='min', verbose=0)\n","\n","                print(\"{{  Training TensorFlow model...}}\") # Corrected escaping\n","                history = tf_model.fit(X_train_scaled, y_train_log,\n","                                       epochs=200,\n","                                       batch_size=64,\n","                                       verbose=0,\n","                                       validation_data=(X_val_scaled, y_val_log),\n","                                       callbacks=[early_stopping_tf])\n","                print(\"{{  TensorFlow model training complete.}}\") # Corrected escaping\n","\n","                print(\"{{Making TensorFlow predictions...}}\") # Corrected escaping\n","                tfy_test_pred_log = tf_model.predict(X_test_scaled).flatten()\n","                tf_test_pred = np.expm1(tfy_test_pred_log)\n","                # Clip to original target range (need 'y' min/max - assuming 'y' is available and is the original target)\n","                if 'y' in locals():\n","                    # Escape accessor y.min() and y.max()\n","                    tf_test_pred = np.clip(tf_test_pred, {{y}}.min(), {{y}}.max())\n","                else:\n","                     # Fallback clipping or warning if y is not available\n","                     tf_test_pred = np.maximum(0, tf_test_pred) # At least ensure non-negative\n","                     print(\"{{Warning: Original target 'y' not available for TensorFlow prediction clipping.}}\") # Corrected escaping\n","\n","\n","                tfy_val_pred_log = tf_model.predict(X_val_scaled).flatten()\n","                tf_val_pred = np.expm1(tfy_val_pred_log)\n","                if 'y' in locals():\n","                    # Escape accessor y.min() and y.max()\n","                    tf_val_pred = np.clip(tf_val_pred, {{y}}.min(), {{y}}.max())\n","                else:\n","                    tf_val_pred = np.maximum(0, tf_val_pred)\n","                    print(\"{{Warning: Original target 'y' not available for TensorFlow validation prediction clipping.}}\") # Corrected escaping\n","                print(\"{{TensorFlow predictions made.}}\") # Corrected escaping\n","            else: # TF model build failed\n","                tf_test_pred = np.zeros(X_test_scaled.shape[0])\n","                tf_val_pred = np.zeros(X_val_scaled.shape[0])\n","                print(\"{{TensorFlow model could not be built. Skipping TF predictions.}}\") # Corrected escaping\n","\n","    else: # TF not available or test data not scaled\n","        print(\"\\nSkipping TensorFlow model training and prediction.\")\n","        # Ensure prediction variables exist even if TF is skipped\n","        # Need the shape of the processed test data to create zeros array\n","        # Escape variable accessor being accessed in the generated code\n","        test_data_shape = ({{processed_test_name}}.shape[0] if '{processed_test_name}' in locals() else 0)\n","        tf_test_pred = np.zeros(test_data_shape)\n","        tf_val_pred = np.zeros(X_val.shape[0]) # Use X_val shape for validation predictions\n","        print(\"{{TensorFlow prediction variables initialized to zeros.}}\") # Corrected escaping\n","\n","\n","    # --- Validation Scores ---\n","    print(f\"\\n--- Validation RMSLE Scores ---\")\n","    # Escape accessor y_val.shape\n","    if {{y_val}}.shape[0] > 0:\n","        # Check if prediction variables exist and have correct shape before computing scores\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'lgb_val_pred' in locals() and {{lgb_val_pred}}.shape == {{y_val}}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{{LightGBM Validation RMSLE: {{rmsle(y_val, lgb_val_pred):.5f}}}}\") # CORRECTED\n","        else:\n","           print(\"{{LightGBM Validation RMSLE: N/A (predictions missing or shape mismatch)}}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'xgb_val_pred' in locals() and {{xgb_val_pred}}.shape == {{y_val}}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{{XGBoost Validation RMSLE: {{rmsle(y_val, xgb_val_pred):.5f}}}}\") # CORRECTED\n","        else:\n","            print(\"{{XGBoost Validation RMSLE: N/A (predictions missing or shape mismatch)}}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'rf_val_pred' in locals() and {{rf_val_pred}}.shape == {{y_val}}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{{RandomForest Validation RMSLE: {{rmsle(y_val, rf_val_pred):.5f}}}}\") # CORRECTED\n","        else:\n","            print(\"{{RandomForest Validation RMSLE: N/A (predictions missing or shape mismatch)}}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'cat_val_pred' in locals() and {{cat_val_pred}}.shape == {{y_val}}.shape:\n","            # Escape braces for f-string inside the generated code\n","            print(f\"{{CatBoost Validation RMSLE: {{rmsle(y_val, cat_val_pred):.5f}}}}\") # CORRECTED\n","        else:\n","             print(\"{{CatBoost Validation RMSLE: N/A (predictions missing or shape mismatch)}}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'tf_val_pred' in locals() and {{tf_val_pred}}.shape == {{y_val}}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{{TensorFlow Validation RMSLE: {{rmsle(y_val, tf_val_pred):.5f}}}}\") # CORRECTED\n","        else:\n","            print(\"{{TensorFlow Validation RMSLE: N/A (predictions missing or shape mismatch)}}\")\n","\n","    else:\n","        print(\"{{No validation data available to compute scores.}}\")\n","    print(f\"------------------------------\")\n","\n","    # --- Ensemble Prediction ---\n","    print(\"\\nCreating ensemble prediction...\")\n","\n","    # Ensure all test prediction variables exist and have the same expected shape\n","    # The expected shape is the number of rows in the processed test data\n","\n","\n","    # List of models and their test predictions\n","    # Escape prediction variable accessors\n","    test_preds = {{\n","        'lgb': lgb_test_pred if 'lgb_test_pred' in locals() and {{lgb_test_pred}}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'xgb': xgb_test_pred if 'xgb_test_pred' in locals() and {{xgb_test_pred}}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'rf':  rf_test_pred  if 'rf_test_pred'  in locals() and {{rf_test_pred}}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'cat': cat_test_pred if 'cat_test_pred' in locals() and {{cat_test_pred}}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'tf':  tf_test_pred  if 'tf_test_pred'  in locals() and {{tf_test_pred}}.shape[0] == expected_shape else np.zeros(expected_shape),\n","    }}\n","    # Escape prediction variable accessors and y_val shape accessor\n","    val_preds = {{\n","        'lgb': lgb_val_pred if 'lgb_val_pred' in locals() and {{lgb_val_pred}}.shape == {{y_val}}.shape else np.zeros({{y_val}}.shape[0]),\n","        'xgb': xgb_val_pred if 'xgb_val_pred' in locals() and {{xgb_val_pred}}.shape == {{y_val}}.shape else np.zeros({{y_val}}.shape[0]),\n","        'rf':  rf_val_pred  if 'rf_val_pred'  in locals() and {{rf_val_pred}}.shape == {{y_val}}.shape else np.zeros({{y_val}}.shape[0]),\n","        'cat': cat_val_pred if 'cat_val_pred' in locals() and {{cat_val_pred}}.shape == {{y_val}}.shape else np.zeros({{y_val}}.shape[0]),\n","        'tf':  tf_val_pred  if 'tf_val_pred'  in locals() and {{tf_val_pred}}.shape == {{y_val}}.shape else np.zeros({{y_val}}.shape[0]),\n","    }}\n","\n","    # Calculate weights based on validation performance (using RMSLE)\n","    # Avoid division by zero or using infinite RMSLE from errors\n","    # Escape accessors for val_preds and y_val\n","    val_rmsles = {{\n","        'lgb': rmsle({{y_val}}, {{val_preds}}['lgb']) if {{y_val}}.shape[0] > 0 and np.sum(np.abs({{val_preds}}['lgb'])) > 0 else float('inf'),\n","        'xgb': rmsle({{y_val}}, {{val_preds}}['xgb']) if {{y_val}}.shape[0] > 0 and np.sum(np.abs({{val_preds}}['xgb'])) > 0 else float('inf'),\n","        'rf':  rmsle({{y_val}}, {{val_preds}}['rf'])  if {{y_val}}.shape[0] > 0 and np.sum(np.abs({{val_preds}}['rf'])) > 0 else float('inf'),\n","        'cat': rmsle({{y_val}}, {{val_preds}}['cat']) if {{y_val}}.shape[0] > 0 and np.sum(np.abs({{val_preds}}['cat'])) > 0 else float('inf'),\n","        'tf':  rmsle({{y_val}}, {{val_preds}}['tf'])  if {{y_val}}.shape[0] > 0 and np.sum(np.abs({{val_preds}}['tf'])) > 0 and tf is not None else float('inf'), # Only include TF if available\n","    }}\n","\n","    # Calculate inverse RMSLE (higher for better models), handle inf/zero\n","    # Escape dictionary view accessors\n","    inv_rmsles = {{k: (1.0 / v if v != 0 and v != float('inf') else 0) for k, v in {{val_rmsles}}.items()}}\n","\n","    # Normalize to get weights\n","    # Escape dictionary view accessors\n","    total_inv = sum({{inv_rmsles}}.values())\n","    if total_inv > 1e-9: # Avoid division by near zero\n","        # Escape dictionary view accessors\n","        weights = {{k: v / total_inv for k, v in {{inv_rmsles}}.items()}}\n","    else: # If all models failed or had infinite RMSLE, use equal weights or zero weights\n","        print(\"{{Warning: Cannot calculate meaningful ensemble weights (all models failed or infinite RMSLE). Using equal weights as fallback.}}\") # Corrected escaping\n","        # Escape dictionary view accessors\n","        valid_models = [m for m, inv in {{inv_rmsles}}.items() if inv > 0]\n","        if valid_models:\n","             equal_weight = 1.0 / len(valid_models)\n","             # Escape dictionary creation and looping\n","             weights = {{m: equal_weight for m in valid_models}}\n","             for m in models: # Ensure all keys are in weights dict\n","                 if m not in weights: weights[m] = 0\n","        else:\n","             print(\"{{Warning: No models trained successfully. Ensemble prediction will be zero.}}\") # Corrected escaping\n","             # Escape dictionary creation\n","             weights = {{m: 0 for m in models}}\n","\n","\n","    # Escape braces for f-string inside the generated code\n","    print(f\"{{Model weights: {{weights}}}}\") # CORRECTED\n","\n","    # Calculate weighted ensemble predictions\n","    ensemble_predictions = np.zeros(expected_shape)\n","    for model in models:\n","        # Escape weights and test_preds dictionary accessors\n","        ensemble_predictions += {{weights}}[model] * {{test_preds}}[model]\n","\n","\n","    # Ensure predictions are non-negative (calories can't be negative)\n","    ensemble_predictions = np.maximum(0, ensemble_predictions)\n","\n","    # --- Create Submission ---\n","    # Assume 'submission' DataFrame with an 'id' column is available from initial loading\n","    print(\"\\nPreparing submission file...\")\n","    # Escape submission DataFrame accessors (.columns, .shape[0]) and expected_shape\n","    if 'submission' in locals() and 'id' in {{submission}}.columns and {{submission}}.shape[0] == {{expected_shape}}:\n","        # Escape submission DataFrame item assignment and method call\n","        {{submission}}['Calories'] = ensemble_predictions\n","        {{submission}}.to_csv('ensemble_submission.csv', index=False)\n","        print(\"{{Submission file created: ensemble_submission.csv}}\") # Corrected escaping\n","    else:\n","        print(\"{{Error: Submission DataFrame not found, invalid, or shape mismatch. Cannot create submission file.}}\") # Corrected escaping\n","        print(\"{{Details:}}\") # Corrected escaping\n","        if 'submission' not in locals(): print(\"{{'submission' variable not found in generated code locals().}}\") # Corrected escaping\n","        # Escape submission DataFrame column check\n","        elif 'id' not in {{submission}}.columns: print(\"{{'submission' DataFrame missing 'id' column.}}\") # Corrected escaping\n","        # Escape submission DataFrame shape and expected_shape comparison within the *inner* f-string.\n","        # This needs to be escaped using {{ and }} in the outer f-string.\n","        elif {{submission}}.shape[0] != {{expected_shape}}: print(f\"{{Submission shape ({{submission.shape[0]}}) does not match test prediction shape ({{expected_shape}}).}}\") # CORRECTED\n","    # --- CORRECTED LINES END HERE ---\n","\n","\n","    print(\"\\nProcess complete!\") # This print does not contain a variable, so no inner f-string escaping needed\n","\n","else:\n","    # Escape variable accessor being accessed in the generated code\n","    print(\"\\\\nSkipping model training, validation, and submission due to data split failure or empty training data.\")\n","    # Ensure ensemble_predictions variable exists for potential return, even if zero\n","    ensemble_predictions = np.zeros({{processed_test_name}}.shape[0] if '{processed_test_name}' in locals() else 0)\n","\n","    ''') \n","                    \n","    # --- Code Generation Complete ---\n","    print(\"--- Code Generation Complete ---\")\n","\n","    # --- Prepare Results (Code Only) ---\n","    # Since we are not executing, we only return the generated code.\n","    # No stdout, error capture, or artifact paths from execution.\n","    scientist_result = {\n","        \"code\": generated_code,\n","        \"processed_train_name\": processed_train_name, # Pass the train DataFrame name\n","        \"processed_test_name\": processed_test_name,   # Pass the test DataFrame name\n","    }\n","    \n","\n","    # --- Return state update ---\n","    return {\n","        \"scientist_output\": scientist_result,\n","        \"next_agent\": \"Supervisor\",\n","        \"current_task_description\": None\n","    }"]},{"cell_type":"code","execution_count":8,"id":"854d17ef","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.187907Z","iopub.status.busy":"2025-10-22T22:24:28.187524Z","iopub.status.idle":"2025-10-22T22:24:28.205921Z","shell.execute_reply":"2025-10-22T22:24:28.204943Z"},"papermill":{"duration":0.029302,"end_time":"2025-10-22T22:24:28.207596","exception":false,"start_time":"2025-10-22T22:24:28.178294","status":"completed"},"tags":[]},"outputs":[],"source":["def bot_to_human_interpreter(state: GraphState) -> Dict[str, Any]:\n","    \"\"\"\n","    Gathers code from Analyst and Scientist outputs, blends them into a single script,\n","    adds explanations within the code, and formats it for copy-pasting.\n","    Returns an update dict with the combined code message and ROUTES TO THE REVIEW NODE.\n","    Does NOT set final_answer_generated=True.\n","    \"\"\"\n","    print(\"\\n--- BOT TO HUMAN INTERPRETER (Code Compiler) ---\")\n","\n","    # --- Robustly retrieve and validate outputs from the state ---\n","    # Note: This version assumes it's receiving state after Analyst/Scientist\n","    analyst_output_raw = state.get('analyst_llm_output')  # Check LLM output first\n","    scientist_output_raw = state.get('scientist_llm_output')\n","    \n","    if not analyst_output_raw:\n","        analyst_output_raw = state.get('analyst_output')  # Fallback to original if LLM output not found\n","        \n","    if not scientist_output_raw:\n","        scientist_output_raw = state.get('scientist_output')\n","\n","    # Initialize variables to avoid UnboundLocalError\n","    analyst_output = {}\n","    scientist_output = {}\n","    \n","    # Ensure they are dictionaries before proceeding\n","    if analyst_output_raw:\n","        analyst_output = analyst_output_raw if isinstance(analyst_output_raw, dict) else {}\n","    if scientist_output_raw:\n","        scientist_output = scientist_output_raw if isinstance(scientist_output_raw, dict) else {}\n","    \n","    # Initialize a list to hold parts of the combined script\n","    combined_code_parts = []\n","    \n","    # Add an introductory comment/header to the script\n","    combined_code_parts.append(\"# Combined Data Science and Data Analyst IA's Workflow Script\\n\")\n","    combined_code_parts.append(\"# Generated by Analyst-IA's Agent Team\\n\\n\")\n","\n","    # Indicate if there were errors\n","    analyst_error = analyst_output.get('error')\n","    scientist_error = scientist_output.get('error')\n","    \n","    if analyst_error or scientist_error:\n","        combined_code_parts.append(\"# !!! NOTE: Errors occurred during the automated workflow stages. Review the error notes below.\\n\\n\")\n","\n","    combined_code_parts.append(\"# This script combines preprocessing and modeling steps.\\n\")\n","    combined_code_parts.append(\"# Ensure you have the necessary libraries installed (e.g., pandas, numpy, sklearn, lightgbm, xgboost, matplotlib).\\n\")\n","    combined_code_parts.append(\"# Make sure your initial 'train' and 'test' DataFrames (or equivalent data loading) exist before running.\\n\\n\")\n","\n","    # --- Section 1: Data Preprocessing (from Data Analyst) ---\n","    combined_code_parts.append(\"# --- 1. Data Preprocessing ---\\n\")\n","    combined_code_parts.append(\"# This section handles steps like identifying column types, imputation, scaling, and encoding.\\n\")\n","    combined_code_parts.append(\"# It prepares the raw data into processed DataFrames ready for modeling.\\n\\n\")\n","\n","    # Get the Analyst's code and error status (safe now due to checks above)\n","    analyst_code = analyst_output.get('code', '')\n","\n","    # Safely get expected output names, handling cases where output is missing or incomplete\n","    processed_train_name = analyst_output.get('processed_train_name', \"processed_train_name\")  # Use a reasonable default\n","    processed_test_name = analyst_output.get('processed_test_name', \"processed_test_name\")    # Use a reasonable default\n","\n","    if analyst_error:\n","        combined_code_parts.append(f\"# !!! Data Analyst Error during preprocessing execution or code generation.\\n\")\n","        combined_code_parts.append(f\"# !!! Error Details: {analyst_error}\\n\\n\")\n","    elif not analyst_code:\n","        combined_code_parts.append(\"# Data preprocessing code was not provided by the Analyst.\\n\\n\")\n","    else:\n","        # Append the analyst's code\n","        combined_code_parts.append(analyst_code.strip())\n","        combined_code_parts.append(\"\\n\\n\")  # Add spacing after the code block\n","        # Add comments explaining the *expected output variables*\n","        combined_code_parts.append(\"# Expected outputs from this section (as variables in your environment):\\n\")\n","        combined_code_parts.append(f\"# - {processed_train_name}: Processed training features (potentially including target)\\n\")\n","        combined_code_parts.append(f\"# - {processed_test_name}: Processed test features\\n\\n\")\n","\n","    # --- Section 2: Model Training and Evaluation (from Data Scientist) ---\n","    combined_code_parts.append(\"# --- 2. Model Training and Evaluation ---\\n\")\n","    combined_code_parts.append(\"# This section uses the processed data to train machine learning models (LightGBM, XGBoost) and evaluate them.\\n\")\n","    combined_code_parts.append(f\"# It assumes variables like '{processed_train_name}' and '{processed_test_name}' exist from the previous section.\\n\\n\")\n","\n","    # Get the Scientist's code and error status (safe now due to checks above)\n","    scientist_code = scientist_output.get('code', '')\n","\n","    if scientist_error:\n","        combined_code_parts.append(f\"# !!! Data Scientist Error during modeling execution or code generation.\\n\")\n","        combined_code_parts.append(f\"# !!! Error Details: {scientist_error}\\n\\n\")\n","        # Optionally include stdout even if there was an error, might contain clues\n","        if scientist_stdout:\n","            combined_code_parts.append(\"# Captured output (may contain error details):\\n\")\n","            commented_stdout = \"\\n\".join([f\"# {line}\" for line in scientist_stdout.strip().split('\\n')])\n","            combined_code_parts.append(commented_stdout + \"\\n\\n\")\n","    elif not scientist_code:\n","        combined_code_parts.append(\"# Model training code was not provided by the Scientist.\\n\\n\")\n","    else:\n","        # Append the scientist's code\n","        combined_code_parts.append(scientist_code.strip())\n","        combined_code_parts.append(\"\\n\\n\")  # Add spacing after the code block\n","        # Add comments explaining expected outputs/artifacts\n","        combined_code_parts.append(\"# Expected outputs/artifacts from this section:\\n\")\n","        combined_code_parts.append(\"# - Console output showing training progress and validation scores.\\n\")\n","        combined_code_parts.append(\"# - 'lgb_metric_plot.png', 'xgb_metric_plot.png': Plots showing model performance during training.\\n\")\n","        combined_code_parts.append(\"# - 'lgb_test_preds.csv', 'xgb_test_preds.csv': CSV files with predictions on the test set.\\n\\n\")\n","\n","    # --- Section 3: Summary Notes ---\n","    combined_code_parts.append(\"# --- 3. Summary Notes ---\\n\")\n","    if analyst_error or scientist_error:\n","        combined_code_parts.append(\"# NOTE: Review the error messages and generated code sections carefully.\\n\")\n","    else:\n","        combined_code_parts.append(\"# NOTE: The automated workflow appears to have completed successfully. Review the generated files and console output.\\n\")\n","\n","    \n","        # Special case: if we have raw strings instead of structured output\n","    if isinstance(analyst_output_raw, str) or isinstance(scientist_output_raw, str):\n","        if isinstance(analyst_output_raw, str):\n","            combined_code_parts.append(\"# --- Analyst Output ---\\n\")\n","            combined_code_parts.append(analyst_output_raw)\n","            combined_code_parts.append(\"\\n\\n\")\n","            \n","        if isinstance(scientist_output_raw, str):\n","            combined_code_parts.append(\"# --- Scientist Output ---\\n\")\n","            combined_code_parts.append(scientist_output_raw)\n","            combined_code_parts.append(\"\\n\\n\")\n","            \n","        final_python_script = \"\".join(combined_code_parts)\n","        \n","        human_message_content = \"Here is the complete Python script generated by the workflow:\\n\\n\"\n","        human_message_content += \"```python\\n\"\n","        human_message_content += final_python_script\n","        human_message_content += \"```\\n\"\n","        print(\"Interpreter compiled code and generated message for CALOR-IA Analyst review.\")\n","        \n","        return {\n","\n","            \"analyst_llm_output\": analyst_output_raw,\n","            \"scientist_llm_output\": scientist_output_raw,\n","            \"messages\": state.get(\"messages\", []) + [AIMessage(content=human_message_content)],\n","            \"interpreter_finished\":True,\n","            \"next_agent\": \"supervisor\",\n","        }\n","    else:\n","        if analyst_code != None:\n","            \n","            combined_code_parts.append(analyst_code)\n","            combined_code_parts.append(\"\\n# --- End of Analyst Script ---\")\n","            \n","        if scientist_code != None:\n","            \n","            combined_code_parts.append(scientist_code)\n","            combined_code_parts.append(\"\\n# --- End of Scientist Script ---\")\n","    \n","    combined_code_parts.append(\"\\n# --- End of Script ---\")\n","    combined_code_parts.append(\"\\n# Copy and paste the entire block above into your environment/notebook to run the workflow!\\n\")\n","\n","    # Combine all parts into a single string representing the full script\n","    final_python_script = \"\".join(combined_code_parts)\n","\n","    # Wrap the entire script in a single markdown code block for easy copy-pasting\n","    human_message_content = \"Here is the complete Python script generated by the workflow, combining preprocessing and modeling steps:\\n\\n\"\n","    human_message_content += \"```python\\n\"  # Start markdown code block\n","    human_message_content += final_python_script\n","    human_message_content += \"```\\n\"        # End markdown code block\n","\n","    print(\"Interpreter compiled code and generated message for CALOR-IA Analyst review.\")\n","\n","    # Prepare the state update dictionary to return\n","    return {\n","        \"analyst_output\": analyst_output,\n","        \"scientist_output\": scientist_output,\n","        \"analyst_llm_output\": analyst_output_raw,\n","        \"scientist_llm_output\": scientist_output_raw,\n","        \"messages\": state.get(\"messages\", []) + [AIMessage(content=human_message_content)],\n","        \"interpreter_finished\":True,\n","        \"next_agent\": \"supervisor\",  # Route to the review node\n","    }"]},{"cell_type":"markdown","id":"00e05568","metadata":{"papermill":{"duration":0.007512,"end_time":"2025-10-22T22:24:28.222828","exception":false,"start_time":"2025-10-22T22:24:28.215316","status":"completed"},"tags":[]},"source":["# \"What if ğŸ¯ğŸš€ we build an AI Agent ğŸ¤– who can compete for me in this Kaggle competition ğŸ†ğŸ“Š?\""]},{"cell_type":"code","execution_count":9,"id":"565939b2","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.239804Z","iopub.status.busy":"2025-10-22T22:24:28.239511Z","iopub.status.idle":"2025-10-22T22:24:28.350408Z","shell.execute_reply":"2025-10-22T22:24:28.348808Z"},"papermill":{"duration":0.122032,"end_time":"2025-10-22T22:24:28.352459","exception":false,"start_time":"2025-10-22T22:24:28.230427","status":"completed"},"tags":[]},"outputs":[],"source":["data_llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-2.5-flash-preview-04-17\",\n","    google_api_key=secret_value_0,\n","    temperature = 0.3,\n","    max_output_tokens = 9000,          \n",")\n","\n","\n","# --- Define the new CALOR_IA_DATA_ANALYST_NODE ---\n","def CALOR_IA_DATA_ANALYST_NODE(state: GraphState) -> Dict[str, Any]:\n","    \"\"\"\n","    Reviews the compiled Python script from the interpreter and generates a summary\n","    or concluding message for the user using an LLM.\n","    \"\"\"\n","    print(\"\\n--- CALOR-IA DATA ANALYST (Post-Compilation Review) ---\")\n","    \n","    messages = state.get(\"messages\", []) \n","    analyst_output = state.get('analyst_output')\n","    # Try to get compiled script directly from state first\n","    analyst_code = analyst_output.get(\"code\", \"\")\n","    final_answer_generated = state.get('final_answer_generated')\n","\n","    if final_answer_generated:\n","        final_code = state.get('code_final')\n","        prompt_messages = [\n","        CALOR_IA_DATA_ANALYST_SYSINT,\n","        HumanMessage(content=f'''\n","        Assure or provide techniques that may increase the quality of  train, test = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv'),:\\n\\n```python\\n{final_code}...\\n```\\n\\n improve the dataset columns = {train.columns}  \n","        perform data augmentation or make more similar train data to test data \n","        avoid tis kind of error:\n","        ValueError: could not convert string to float: male processing and apply feature engineering return full python code.\n","        ''')\n","        ]\n","        \n","        print(\"CALOR-IA Analyst preprocessing pipeline...\")\n","        try:\n","            print(\"CALOR-IA Analyst: LLM response.\")\n","            llm_response = data_llm.invoke(prompt_messages)\n","            print(\"CALOR-IA Analyst: LLM invocation successful.\")\n","            summary_message_content = str(llm_response.content)\n","            print(f\"CALOR-IA Analyst IA analyst: {summary_message_content}...\")\n","        except Exception as e:\n","            print(f\"--- CALOR-IA Analyst Runtime Error during LLM call ---\")\n","            print(f\"Error Type: {type(e).__name__}\")\n","            print(f\"Error Details: {e}\")\n","            summary_message_content = f\"An error occurred while generating the final summary: {e}\"\n","            if not state.get('error'):\n","                 state['error'] = summary_message_content\n","        \n","        # Append the new summary message to the state\n","        updated_messages = messages + [AIMessage(content=summary_message_content)]\n","        print(f\"CALOR-IA Analyst adding message to state: {summary_message_content}...\")\n","        loop_process = True\n","        print(\"CALOR-IA Analyst routing to Supervisor.\")\n","        return {\n","            \"analyst_llm_output\": summary_message_content,\n","            \"message\" : updated_messages, # Pass the script to other nodes\n","            \"loop_process\": loop_process,\n","            \"next_agent\": \"supervisor\"\n","        }\n","        \n","    # If no direct script found, try to extract from messages\n","    if not analyst_code:\n","        # Find the last message that might contain the compiled script\n","        last_message = messages[-1] if messages else None\n","        \n","        if last_message and isinstance(last_message, AIMessage):\n","            # Attempt to extract the markdown code block content\n","            match = re.search(r\"```python\\n(.*)```\", last_message.content, re.DOTALL)\n","            if match:\n","                compiled_script_content = match.group(1).strip()\n","                print(\"CALOR-IA Analyst found compiled script in message.\")\n","            else:\n","                print(\"CALOR-IA Analyst: Could not find script markdown block in last message.\")\n","                # Try to find messages with the script data in another format\n","                for msg in reversed(messages):\n","                    if isinstance(msg, AIMessage) and \"# Combined Data Science and Data Analyst\" in msg.content:\n","                        compiled_script_content = msg.content\n","                        print(\"CALOR-IA Analyst found script content in a message without markdown.\")\n","                        break\n","                \n","                if not compiled_script_content:\n","                    compiled_script_content = last_message.content.strip() if last_message.content else \"No content found in last message.\"\n","        else:\n","            print(\"CALOR-IA Analyst: Last message was not an AI message or state was empty.\")\n","            compiled_script_content = \"Error: Could not retrieve compiled script content.\"\n","\n","\n","    \n","    # Store the found script for other nodes to use\n","    state[\"analyst_code\"] = analyst_code\n","    \n","    # Prepare prompt for the LLM to summarize the script\n","    prompt_messages = [\n","        CALOR_IA_DATA_ANALYST_SYSINT,\n","        HumanMessage(content=f\"provide data audmentation  :\\n\\n```python\\n{analyst_code}...\\n```\\n\\n columns = {train.columns} provide executable python preprocess pipeline for kaggle.\")\n","    ]\n","\n","    print(\"CALOR-IA Analyst invoking LLM for summary...\")\n","    try:\n","        print(\"CALOR-IA Analyst: LLM response.\")\n","        llm_response = data_llm.invoke(prompt_messages)\n","        print(\"CALOR-IA Analyst: LLM invocation successful.\")\n","        summary_message_content = str(llm_response.content)\n","        print(f\"CALOR-IA Analyst IA analyst: {summary_message_content}...\")\n","    except Exception as e:\n","        print(f\"--- CALOR-IA Analyst Runtime Error during LLM call ---\")\n","        print(f\"Error Type: {type(e).__name__}\")\n","        print(f\"Error Details: {e}\")\n","        summary_message_content = f\"An error occurred while generating the final summary: {e}\"\n","        if not state.get('error'):\n","             state['error'] = summary_message_content\n","\n","    # Append the new summary message to the state\n","    updated_messages = messages + [AIMessage(content=summary_message_content)]\n","    print(f\"CALOR-IA Analyst adding message to state: {summary_message_content[:200]}...\")\n","\n","    print(\"CALOR-IA Analyst routing to Supervisor.\")\n","    return {\n","        \"analyst_llm_output\": summary_message_content,\n","        \"message\" : updated_messages, # Pass the script to other nodes\n","        \"next_agent\": \"Supervisor\"\n","    }"]},{"cell_type":"code","execution_count":10,"id":"aaafe962","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.37007Z","iopub.status.busy":"2025-10-22T22:24:28.369722Z","iopub.status.idle":"2025-10-22T22:24:28.386642Z","shell.execute_reply":"2025-10-22T22:24:28.385769Z"},"papermill":{"duration":0.027355,"end_time":"2025-10-22T22:24:28.388231","exception":false,"start_time":"2025-10-22T22:24:28.360876","status":"completed"},"tags":[]},"outputs":[],"source":["sci_llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-2.5-flash-preview-04-17\",\n","    google_api_key=secret_value_0,\n","    temperature = 0.3,\n","    max_output_tokens = 9000,          \n",")\n","\n","# --- Define the new CALOR_IA_DATA_ANALYST_NODE ---\n","def CALOR_IA_DATA_SCIENTIST_NODE(state: GraphState) -> Dict[str, Any]:\n","    \"\"\"\n","    Reviews the compiled Python script from the interpreter and generates a summary\n","    or concluding message for the user using an LLM.\n","    \"\"\"\n","    print(\"\\n--- CALOR_IA_DATA_SCIENTIST (Post-Compilation Review) ---\")\n","    \n","    scientist_output = state.get(\"scientist_output\")\n","    \n","    compiled_script_content = scientist_output.get(\"code\")\n","\n","\n","    loop_process = state.get('loop_process')\n","    final_answered_genarated = state.get('final_answered_generated')\n","    analyst_llm_output = state.get('analyst_llm_output')\n","\n","    \n","    if not loop_process and final_answered_genarated:\n","        prompt_message = [\n","            CALOR_IA_DATA_SCIENTIST_SYSINT,\n","            HumanMessage(content=f'Assure cross validation is perfecly applied,    Assure optuna is applied and assure it will take just the first 3 enhacement or use a eraly stop method  {analyst_llm_output} avoid this error ValueError: Input array must be 1 dimensional, assure pd.to_csv(submission.csv, infex=False) exist and python script is complete')\n","        ]\n","        try:\n","            print(\"CALOR-IA Scientist: LLM response.\")\n","            llm_response = sci_llm.invoke(prompt_messages)\n","            print(\"CALOR-IA Scientist: LLM invocation successful.\")\n","            summary_message_content = str(llm_response.content)\n","            print(f\"CALOR_IA_DATA_SCIENTIST LLM response content: {summary_message_content}...\")\n","        except Exception as e:\n","            print(f\"--- CALOR_IA_DATA_SCIENTIST Runtime Error during LLM call ---\")\n","            print(f\"Error Type: {type(e).__name__}\")\n","            print(f\"Error Details: {e}\")\n","            summary_message_content = f\"An error occurred while generating the final summary: {e}\"\n","            if not state.get('error'):\n","                 state['error'] = summary_message_content\n","\n","        updated_messages = state.get(\"messages\", []) + [AIMessage(content=summary_message_content)]\n","        print(f\"CALOR_IA_DATA_SCIENTIST adding message to state: {summary_message_content}...\")\n","\n","            \n","        print(\"CALOR-IA Scientist routing to Supervisor.\")\n","        return {\n","            \"scientist_llm_output\": summary_message_content,\n","            \"loop_process\": False,\n","            'loop': loop,\n","            \"message\": updated_messages,  # Pass the script forward\n","            \"next_agent\": \"Supervisor\"\n","        }\n","        \n","    # If not found directly, try to get from messages\n","    if not compiled_script_content:\n","        messages = state.get(\"messages\", [])\n","        last_message = messages[-1] if messages else None\n","\n","        if last_message and isinstance(last_message, AIMessage):\n","            # Attempt to extract the markdown code block content\n","            match = re.search(r\"```python\\n(.*)```\", last_message.content, re.DOTALL)\n","            if match:\n","                compiled_script_content = match.group(1).strip()\n","                print(\"CALOR-IA Scientist found compiled script.\")\n","            else:\n","                print(\"CALOR-IA Scientist: Could not find script markdown block in last message.\")\n","                # Try to find script in other messages\n","                for msg in reversed(messages):\n","                    if isinstance(msg, AIMessage) and \"# Combined Data Science and Data Analyst\" in msg.content:\n","                        compiled_script_content = msg.content\n","                        print(\"CALOR-IA Scientist found script content in a message without markdown.\")\n","                        break\n","                \n","                if not compiled_script_content:\n","                    compiled_script_content = last_message.content.strip() if last_message.content else \"No content found in last message.\"\n","        else:\n","            print(\"CALOR-IA Scientist: Last message was not an AI message or state was empty.\")\n","            compiled_script_content = \"Error: Could not retrieve compiled script content.\"\n","\n","    # Prepare prompt for the LLM to summarize the script\n","    prompt_messages = [\n","        CALOR_IA_DATA_SCIENTIST_SYSINT,\n","        HumanMessage(content=f\" Apply cross validation or related technique to reduce metric RMSLE,    Assure optuna is applied and assure it will take just the first 3 enhacement or use a eraly stop method   :\\n\\n```python\\n{compiled_script_content}...\\n```\\n\\n thiis code provided has RMSLE = 0.0583 you must beat it; Please provide executable python preprocess pipeline for kaggle.\")\n","    ]\n","\n","    print(\"CALOR_IA_DATA_SCIENTIST...\")\n","    try:\n","        print(\"CALOR-IA Scientist: LLM response.\")\n","        llm_response = sci_llm.invoke(prompt_messages)\n","        print(\"CALOR-IA Scientist: LLM invocation successful.\")\n","        summary_message_content = str(llm_response.content)\n","        print(f\"CALOR_IA_DATA_SCIENTIST LLM response content: {summary_message_content}...\")\n","    except Exception as e:\n","        print(f\"--- CALOR_IA_DATA_SCIENTIST Runtime Error during LLM call ---\")\n","        print(f\"Error Type: {type(e).__name__}\")\n","        print(f\"Error Details: {e}\")\n","        summary_message_content = f\"An error occurred while generating the final summary: {e}\"\n","        if not state.get('error'):\n","             state['error'] = summary_message_content\n","\n","    # Append the new summary message to the state\n","    updated_messages = state.get(\"messages\", []) + [AIMessage(content=summary_message_content)]\n","    print(f\"CALOR_IA_DATA_SCIENTIST adding message to state: {summary_message_content}...\")\n","\n","    print(\"CALOR-IA Scientist routing to Supervisor.\")\n","    return {\n","        \"scientist_llm_output\": summary_message_content,\n","        \"message\": updated_messages,  # Pass the script forward\n","        \"next_agent\": \"Supervisor\"\n","    }"]},{"cell_type":"code","execution_count":11,"id":"ee72d0bb","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.405463Z","iopub.status.busy":"2025-10-22T22:24:28.405167Z","iopub.status.idle":"2025-10-22T22:24:28.424506Z","shell.execute_reply":"2025-10-22T22:24:28.423448Z"},"papermill":{"duration":0.030077,"end_time":"2025-10-22T22:24:28.426292","exception":false,"start_time":"2025-10-22T22:24:28.396215","status":"completed"},"tags":[]},"outputs":[],"source":["code_llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-2.5-flash-preview-04-17\",\n","    google_api_key=secret_value_0,\n","    temperature = 0.1,\n","    max_output_tokens = 12000,          \n",")\n","\n","\n","def CODE_COMPILER_IA(state: GraphState) -> Dict[str, Any]:\n","    \"\"\"\n","    Combines the preprocessing script with the model training script and outputs the final result.\n","    \"\"\"\n","    analyst_output = state.get(\"analyst_llm_output\")\n","    scientist_output = state.get(\"scientist_llm_output\")\n","    \n","    print(\"\\n--- CODE_COMPILER_IA (Combining Scripts) ---\")\n","    \n","    loop_process = state.get('loop_process')\n","    final_answered_generated = state.get('final_answered_generated')\n","    loop = state.get('loop')        \n","\n","    if loop == 0:\n","        prompt_messages = [\n","        CALOR_IA_CODE_COMPILER_SYSINT,\n","        HumanMessage(content=(f\"\"\"\n","        train, test = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv'), pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv'), \n","        you have to set up python code ready to run on Kaggle:\n","        \n","        {analyst_output}.) if multiple pipelines chose just one, assure categorical and numerical data is separate before processing\n","        \n","        review ans set upin order to work with:\n","       \n","        {scientist_output} \n","        # assure \n","   # assure pd.to_csv(submission.csv, infex=False) exist and python script is complete\n","    \"\"\"  )\n","                    )]\n","                     \n","        try:\n","            print(\"CODE_COMPILER_IA: LLM response.\")\n","            llm_response = code_llm.invoke(prompt_messages)\n","            print(\"CODE_COMPILER_IA: LLM invocation successful.\")\n","            summary_message_content = str(llm_response.content)\n","            print(f\"CALOR_IA_DATA_SCIENTIST LLM response content: {summary_message_content}...\")\n","        except Exception as e:\n","            print(f\"--- CALOR_IA_DATA_SCIENTIST Runtime Error during LLM call ---\")\n","            print(f\"Error Type: {type(e).__name__}\")\n","            print(f\"Error Details: {e}\")\n","            summary_message_content = f\"An error occurred while generating the final summary: {e}\"\n","            \n","        if not state.get('error'):\n","            state['error'] = summary_message_content\n","            \n","        updated_messages = state.get(\"messages\", []) + [AIMessage(content=summary_message_content)]\n","        \n","        return {\n","                \"final_answered_generated\": summary_message_content,\n","                \"code_final\": updated_messages,\n","                \"loop_process\": True,\n","                'loop': loop,# Pass the script forward\n","                \"next_agent\": \"supervisor\"\n","                    }                     \n","        print(\"CODE_COMPILER_IA: Combined script generated successfully.\")\n","    if loop_process and final_answered_generated:\n","\n","        prompt_messages = [\n","        CALOR_IA_CODE_COMPILER_SYSINT,\n","        HumanMessage(content=(f\"\"\"\n","        \n","        you have to set up python code ready to run on Kaggle:\n","        apply all improvements processed on pipelines\n","        data_analyst pipeline\n","        assure id columns are always handle as id = 'id'\n","        Assure no errors on data types\n","        Assure optuna is applied and assure it will take just the first 3 enhacement or use a eraly stop method \n","        review\n","        \n","        {analyst_output}.)\n","        \n","        review\n","       \n","        {scientist_output}\n","        \n","   # assure models are enssamble on defined correctly\n","    # \n","    \"\"\"  )\n","                    )]\n","                     \n","        try:\n","            print(\"CODE_COMPILER_IA: LLM response.\")\n","            llm_response = code_llm.invoke(prompt_messages)\n","            print(\"CODE_COMPILER_IA: LLM invocation successful.\")\n","            summary_message_content = str(llm_response.content)\n","            print(f\"CALOR_IA_DATA_SCIENTIST LLM response content: {summary_message_content}...\")\n","        except Exception as e:\n","            print(f\"--- CALOR_IA_DATA_SCIENTIST Runtime Error during LLM call ---\")\n","            print(f\"Error Type: {type(e).__name__}\")\n","            print(f\"Error Details: {e}\")\n","            summary_message_content = f\"An error occurred while generating the final summary: {e}\"\n","            \n","        if not state.get('error'):\n","            state['error'] = summary_message_content\n","\n","        \n","        \n","        # Return the complete combined script\n","        return {\n","            \"messages\": messages + [AIMessage(content=summary_message_content)],\n","            \"code_final\": summary_message_content,\n","            \"loop_process\": False,\n","            'loop': loop,\n","            \"final_answer_generated\": True,\n","            \"next_agent\": \"Supervisor\"\n","            }\n","    # Get modeling script - try to find in scientist_output\n","\n","    \n","    messages = state.get(\"messages\", [])\n","    \n","    # If we couldn't find modeling script in state, search through messages\n","    if not scientist_output and not analyst_output:\n","        # Find preprocessing script first (from analyst)\n","        for msg in reversed(messages):\n","            if isinstance(msg, AIMessage) and \"```python\" in msg.content and (\"CALOR-IA Analyst: LLM response.\" or \"CALOR-IA Scientist: LLM response.\") in msg.content:\n","                match = re.search(r\"```python\\n(.*?)```\", msg.content, re.DOTALL)\n","                if match:\n","                    preprocessing_script = match.group(1).strip()\n","                    print(\"CODE_COMPILER_IA found preprocessing script in message.\")\n","                    break\n","    \n","    # Create the combined script\n","    prompt_messages = [\n","        CALOR_IA_CODE_COMPILER_SYSINT,\n","        HumanMessage(content=(f\"\"\"\n","        \n","        you have to set up python code ready to run on Kaggle:\n","        \n","        data_analyst pipeline\n","        #assure data analyst process\n","        {analyst_output}.)\n","        data)scientist pipeline\n","        {scientist_output}\n","    ] # assure other models not loss expected 5 diferent models: \n","    # assure random forest and cat bost where add to the pipepline also print to check on their progress of execution as before\n","    Becareful with output.shape before to blend the predictions and assure predict in the correct scaled dataset\n","    \n","\n","    \"\"\")\n","                    )]\n","    try:\n","        print(\"CALOR-IA Scientist: LLM response.\")\n","        llm_response = code_llm.invoke(prompt_messages)\n","        print(\"CALOR-IA Scientist: LLM invocation successful.\")\n","        summary_message_content = str(llm_response.content)\n","        print(f\"CALOR_IA_DATA_SCIENTIST LLM response content: {summary_message_content}...\")\n","    except Exception as e:\n","        print(f\"--- CALOR_IA_DATA_SCIENTIST Runtime Error during LLM call ---\")\n","        print(f\"Error Type: {type(e).__name__}\")\n","        print(f\"Error Details: {e}\")\n","        summary_message_content = f\"An error occurred while generating the final summary: {e}\"\n","        if not state.get('error'):\n","            state['error'] = summary_message_content\n","\n","   \n","                     \n","    print(\"CODE_COMPILER_IA: Combined script generated successfully.\")\n","\n","    # Return the complete combined script\n","    return {\n","        \"messages\": messages + [AIMessage(content=summary_message_content)],\n","        \"code_final\": summary_message_content,\n","        \"final_answer_generated\": True,\n","        \"next_agent\": \"Supervisor\"\n","    }"]},{"cell_type":"code","execution_count":12,"id":"653bba7a","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.443012Z","iopub.status.busy":"2025-10-22T22:24:28.44272Z","iopub.status.idle":"2025-10-22T22:24:28.487898Z","shell.execute_reply":"2025-10-22T22:24:28.487009Z"},"papermill":{"duration":0.05548,"end_time":"2025-10-22T22:24:28.489756","exception":false,"start_time":"2025-10-22T22:24:28.434276","status":"completed"},"tags":[]},"outputs":[],"source":["workflow = StateGraph(GraphState)\n","\n","# Add nodes\n","workflow.add_node(\"Supervisor\", supervisor_node)\n","workflow.add_node(\"DataAnalyst\", data_analyst_node)\n","workflow.add_node(\"DataAnalyst_AI\", CALOR_IA_DATA_ANALYST_NODE)\n","workflow.add_node(\"DataSciientist_AI\", CALOR_IA_DATA_SCIENTIST_NODE)\n","workflow.add_node(\"DataScientist\", data_scientist_node)\n","workflow.add_node(\"HumanInterpreter\", bot_to_human_interpreter) # Use this exact name\n","workflow.add_node(\"code_compiler_AI\", CODE_COMPILER_IA)\n","# Define entry point\n","workflow.set_entry_point(\"Supervisor\")\n","\n","# --- ADD THESE EDGES ---\n","# After Analyst runs, go back to Supervisor\n","workflow.add_edge(\"DataAnalyst\", \"Supervisor\")\n","\n","workflow.add_edge(\"DataScientist\", \"Supervisor\")\n","\n","workflow.add_edge(\"HumanInterpreter\", \"Supervisor\")\n","\n","workflow.add_edge(\"DataAnalyst_AI\", \"Supervisor\")\n","\n","workflow.add_edge(\"DataSciientist_AI\", \"Supervisor\")\n","\n","workflow.add_edge(\"code_compiler_AI\", \"Supervisor\")\n","\n","# --- END OF ADDED EDGES ---\n","\n","# Define conditional edges FROM SUPERVISOR ONLY\n","workflow.add_conditional_edges(\n","    \"Supervisor\",\n","    # Function to decide route based on supervisor's decision\n","    lambda state: state.get(\"next_agent\"),\n","    # Mapping decision to node name\n","    {\n","        \"DataAnalyst\": \"DataAnalyst\",\n","        \"DataScientist\": \"DataScientist\",\n","        \"HumanInterpreter\": \"HumanInterpreter\", # Ensure this matches add_node name\n","        \"DataAnalyst_AI\": \"DataAnalyst_AI\",\n","        \"DataScientist_AI\": \"DataSciientist_AI\",\n","        \"code_compiler_AI\":\"code_compiler_AI\",\n","        \"Supervisor\": \"Supervisor\", # Allow looping back if needed (e.g., waiting)\n","        \"__end__\": END # Map \"__end__\" string to the graph's end state\n","    }\n",")\n","\n","# Compile the graph\n","app = workflow.compile()"]},{"cell_type":"code","execution_count":13,"id":"65373500","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:28.507644Z","iopub.status.busy":"2025-10-22T22:24:28.507284Z","iopub.status.idle":"2025-10-22T22:24:40.525727Z","shell.execute_reply":"2025-10-22T22:24:40.524148Z"},"papermill":{"duration":12.029767,"end_time":"2025-10-22T22:24:40.527498","exception":false,"start_time":"2025-10-22T22:24:28.497731","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- STARTING WORKFLOW (Using DataFrame Variable Names) ---\n","\n","--- SUPERVISOR ---\n","Supervisor reviewing state. Last message type: HumanMessage\n","Supervisor received content:  Start working ğŸ’ª\n","\n","Original Analyst output available: False\n","Scientist output available: False\n","Interpreter finished signal: False\n","Final answer generated flag: False\n","loop =======> # None\n","Supervisor: Initial human message received. Assigning initial task to Original Data Analyst.\n","DEBUG: Supervisor RETURNING next_agent: 'DataAnalyst' (Type: <class 'str'>)\n","DEBUG: Supervisor RETURNING final_answer_generated: False\n","DEBUG: Supervisor RETURNING interpreter_finished: False\n","\n","--- Workflow Step 1 ---\n","Processing update from node: Supervisor\n","ğŸ§‘â€ğŸ’» User says (via state):  Start working ğŸ’ª\n","\n","ğŸ¤– Supervisor says: Okay, starting the data analysis workflow with the Data Analyst.\n","\n","==================================================\n","\n","\n","--- DATA ANALYST ---\n","Analyst expects input DataFrame variable: 'None'\n","Analyst expects input DataFrame variable: 'None'\n","Analyst will simulate creating output DataFrames: 'processed_None' and 'processed_None'\n","\n","--- Workflow Step 2 ---\n","Processing update from node: DataAnalyst\n","\n","==================================================\n","\n","\n","--- SUPERVISOR ---\n","Supervisor reviewing state. Last message type: AIMessage\n","Supervisor received content: Okay, starting the data analysis workflow with the Data Analyst.\n","Original Analyst output available: True\n","Scientist output available: False\n","Interpreter finished signal: False\n","Final answer generated flag: False\n","loop =======> # None\n","Supervisor: Original Data Analyst successfully generated code and data names. Routing to Data Scientist.\n","Supervisor propagating names from original analyst output to Scientist: train='processed_None', test='processed_None'\n","DEBUG: Supervisor RETURNING next_agent: 'DataScientist' (Type: <class 'str'>)\n","DEBUG: Supervisor RETURNING final_answer_generated: False\n","DEBUG: Supervisor RETURNING interpreter_finished: False\n","\n","--- Workflow Step 3 ---\n","Processing update from node: Supervisor\n","\n","==================================================\n","\n","\n","--- DATA SCIENTIST (Code Generation Only) ---\n","Scientist generating code for task: Build and evaluate LightGBM and XGBoost models using the processed data.\n","Will use processed train DataFrame variable: 'processed_None'\n","Will use processed test DataFrame variable : 'processed_None'\n","--- Code Generation Complete ---\n","\n","--- Workflow Step 4 ---\n","Processing update from node: DataScientist\n","\n","==================================================\n","\n","\n","--- SUPERVISOR ---\n","Supervisor reviewing state. Last message type: AIMessage\n","Supervisor received content: Okay, starting the data analysis workflow with the Data Analyst.\n","Original Analyst output available: True\n","Scientist output available: True\n","Interpreter finished signal: False\n","Final answer generated flag: False\n","loop =======> # None\n","Supervisor: Scientist code generation complete. Routing to HumanInterpreter for compilation.\n","Supervisor propagating names from state for next step: train='processed_None', test='processed_None'\n","DEBUG: Supervisor RETURNING next_agent: 'HumanInterpreter' (Type: <class 'str'>)\n","DEBUG: Supervisor RETURNING final_answer_generated: False\n","DEBUG: Supervisor RETURNING interpreter_finished: False\n","\n","--- Workflow Step 5 ---\n","Processing update from node: Supervisor\n","ğŸ¤– Supervisor says: Scientist code generation complete. Compiling final script.\n","\n","==================================================\n","\n","\n","--- BOT TO HUMAN INTERPRETER (Code Compiler) ---\n","Interpreter compiled code and generated message for CALOR-IA Analyst review.\n","\n","--- Workflow Step 6 ---\n","Processing update from node: HumanInterpreter\n","ğŸ¤– HumanInterpreter says: Here is the complete Python script generated by the workflow, combining preprocessing and modeling steps:\n","\n","```python\n","# Combined Data Science and Data Analyst IA's Workflow Script\n","# Generated by Analyst-IA's Agent Team\n","\n","# This script combines preprocessing and modeling steps.\n","# Ensure you have the necessary libraries installed (e.g., pandas, numpy, sklearn, lightgbm, xgboost, matplotlib).\n","# Make sure your initial 'train' and 'test' DataFrames (or equivalent data loading) exist before running.\n","\n","# --- 1. Data Preprocessing ---\n","# This section handles steps like identifying column types, imputation, scaling, and encoding.\n","# It prepares the raw data into processed DataFrames ready for modeling.\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, QuantileTransformer\n","from sklearn.impute import KNNImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestRegressor # This import might be needed in the generated code\n","\n","# --- Helper Functions (Copying the ones provided by user) ---\n","# Assuming these were defined or should be included in the generated code\n","\n","# Define helper functions here (copying from user's input)\n","# ... (your create_features, swish, root_mean_squared_error, build_swish_mlp, rmsle functions go here)\n","def create_features(df):\n","    '''\n","    Create advanced features from the input data.\n","    This function serves as a form of \"data augmentation\" for tabular data\n","    by deriving new, potentially more informative features from existing ones.\n","    '''\n","    df = df.copy()\n","    epsilon = 1e-6 # Small constant to avoid division by zero\n","\n","    # --- Basic Interaction & Ratio Features (>= 6 columns engineered) ---\n","\n","    # 1. Body Mass Index (BMI): Standard health metric, relates weight and height.\n","    df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2 + epsilon)\n","\n","    # 2. Duration * Heart_Rate: Represents total heartbeats during exercise, proxy for effort.\n","    df['Duration_HR'] = df['Duration'] * df['Heart_Rate']\n","\n","    # 3. Duration * Body_Temp: Represents total heat generated during exercise, proxy for intensity/effort.\n","    df['Duration_Temp'] = df['Duration'] * df['Body_Temp']\n","\n","    # 4. Weight / Height Ratio: Simpler ratio than BMI, might capture different linear relationships.\n","    df['Weight_Height_Ratio'] = df['Weight'] / (df['Height'] + epsilon)\n","\n","    # 5. Heart Rate per Minute: Already captured by Heart_Rate, but could consider rate relative to duration?\n","    #    Let's create Heart_Rate_per_Duration instead.\n","    df['Heart_Rate_per_Duration'] = df['Heart_Rate'] / (df['Duration'] + epsilon) # Rate of heartbeats per minute of exercise\n","\n","    # 6. Body Temperature per Minute: Change in temp per minute of exercise.\n","    df['Body_Temp_per_Duration'] = df['Body_Temp'] / (df['Duration'] + epsilon)\n","\n","    # --- More Complex Interaction Features ---\n","\n","    # 7. Exercise Intensity: Combines HR, Duration, and inversely related to Age (older might have lower max HR).\n","    df['Exercise_Intensity'] = (df['Heart_Rate'] * df['Duration']) / (df['Age'] + epsilon)\n","\n","    # 8. Metabolic Factor: Combines BMI, Duration, HR, and inversely related to Age. A complex interaction term.\n","    df['Metabolic_Factor'] = (df['BMI'] * df['Duration'] * df['Heart_Rate']) / (df['Age'] + epsilon)\n","\n","    # 9. Age * Duration Interaction: Older individuals exercising for longer might have different calorie burn patterns.\n","    df['Age_Duration_Interaction'] = df['Age'] * df['Duration']\n","\n","    # 10. Height * Weight Product: Simple interaction, might capture overall body size effect differently than BMI.\n","    df['Height_Weight_Product'] = df['Height'] * df['Weight']\n","\n","    # --- Polynomial Features (Example for Duration) ---\n","    # Captures non-linear relationship with Duration.\n","    df['Duration_Sq'] = df['Duration']**2\n","\n","    # --- Categorical Features (Discretization) ---\n","\n","    # 11. Age Group: Discretizing Age can help capture non-linear effects or group-specific patterns.\n","    df['Age_Group'] = pd.cut(df['Age'], bins=[0, 25, 35, 45, 55, 65, 100], labels=['<25', '25-35', '35-45', '45-55', '55-65', '65+'], right=False)\n","    df['Age_Group'] = df['Age_Group'].astype(object).fillna('Unknown')\n","\n","\n","    # 12. BMI Category: Standard health categories for BMI.\n","    df['BMI_Category'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, 35, 40, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese I', 'Obese II', 'Obese III'], right=False)\n","    df['BMI_Category'] = df['BMI_Category'].astype(object).fillna('Unknown')\n","\n","\n","    # 13. Heart Rate Zones (Simplified): Based on max HR (approx 220-Age) and exercise intensity.\n","    df['Max_HR_Estimate'] = 220 - df['Age']\n","    df['HR_Zone'] = pd.cut(df['Heart_Rate'] / (df['Max_HR_Estimate'] + epsilon),\n","                           bins=[0, 0.6, 0.7, 0.8, 0.9, 1.1], # Example zones based on % of max HR\n","                           labels=['<60%', '60-70%', '70-80%', '80-90%', '>90%'],\n","                           right=False)\n","    df['HR_Zone'] = df['HR_Zone'].astype(object).fillna('Unknown')\n","    df = df.drop('Max_HR_Estimate', axis=1) # Drop intermediate column\n","\n","\n","    # 14. Duration Category: Simple bins for exercise duration.\n","    df['Duration_Category'] = pd.cut(df['Duration'], bins=[0, 15, 30, 45, 60, 120, 300], labels=['<15min', '15-30min', '30-45min', '45-60min', '1-2hr', '>2hr'], right=False)\n","    df['Duration_Category'] = df['Duration_Category'].astype(object).fillna('Unknown')\n","\n","\n","    # Interaction between Sex and Duration/HeartRate/BodyTemp\n","    df['Sex_Male_Duration'] = df['Duration'] * (df['Sex'] == 'M')\n","    df['Sex_Female_Duration'] = df['Duration'] * (df['Sex'] == 'F')\n","    df['Sex_Male_HR'] = df['Heart_Rate'] * (df['Sex'] == 'M')\n","    df['Sex_Female_HR'] = df['Heart_Rate'] * (df['Sex'] == 'F')\n","\n","\n","    return df\n","\n","\n","# --- Load Data (Assuming 'train' and 'test' DataFrames are already loaded) ---\n","print(\"Using input DataFrames named 'None' and 'None'...\")\n","#Assuming 'y' and 'submission' also exist\n","If 'id' and 'Calories' need dropping, it should happen *before* feature engineering on the original DFs.\n","# Let's assume the initial DataFrames passed in via 'None' and 'None'\n","# are already prepped (ids dropped, target 'y' separated).\n","If not, add that step here:\n","y = None['Calories'] \n","# None = None.drop('Calories', axis=1) # Example drop\n","\n","# Example: If train_df/test_df *contain* id/Calories\n","# train_df_working = None.copy()\n","# test_df_working = None.copy()\n","# if 'id' in train_df_working.columns: train_ids = train_df_working['id'] ; train_df_working = train_df_working.drop('id', axis=1)\n","# if 'Calories' in train_df_working.columns: y = train_df_working['Calories'] ; train_df_working = train_df_working.drop('Calories', axis=1)\n","# if 'id' in test_df_working.columns: test_ids = test_df_working['id'] ; test_df_working = test_df_working.drop('id', axis=1)\n","# print(f\"Initial train shape: {train_df_working.shape}\")\n","# print(f\"Initial test shape: {test_df_working.shape}\")\n","\n","\n","# --- Feature Engineering ---\n","print(\"\n","Applying feature engineering...\")\n","# Apply feature engineering to the working copies (or original DFs if pre-cleaned)\n","# Using the assumed initial variable names from the state.\n","X_train_fe = create_features(None) \n","X_test_fe = create_features(None)\n","\n","print(f\"Train shape after feature engineering: {X_train_fe.shape}\")\n","print(f\"Test shape after feature engineering: {X_test_fe.shape}\")\n","\n","# Identify numerical and categorical features AFTER feature engineering\n","# Ensure handling of potential errors if a feature was expected but not created\n","all_features = list(X_train_fe.columns)\n","numerical_features = [col for col in all_features if X_train_fe[col].dtype in ['int64', 'float64']]\n","categorical_features = [col for col in all_features if X_train_fe[col].dtype == 'object'] # Using object dtype for pandas categorical/string\n","\n","print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n","print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n","\n","# --- Preprocessing Pipeline (KNN Imputation & Quantile Transformation + OneHot) ---\n","print(\"\n","Setting up preprocessing pipeline...\")\n","\n","# Check if numerical/categorical features exist before creating pipelines\n","transformers = []\n","if numerical_features:\n","    numerical_pipeline = Pipeline([\n","        ('imputer', KNNImputer(n_neighbors=5)),       # KNN imputation\n","        ('scaler', QuantileTransformer(output_distribution='normal')) # Robust scaling via QuantileTransformer\n","    ])\n","    transformers.append(('num', numerical_pipeline, numerical_features))\n","else:\n","    print(\"No numerical features found. Skipping numerical pipeline.\")\n","\n","if categorical_features:\n","    categorical_pipeline = Pipeline([\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)) # One-Hot Encoding\n","    ])\n","    transformers.append(('cat', categorical_pipeline, categorical_features))\n","else:\n","     print(\"No categorical features found. Skipping categorical pipeline.\")\n","\n","# Create ColumnTransformer only if there are transformers\n","if transformers:\n","    preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n","\n","    # Fit and Transform data\n","    print(\"Fitting and transforming data with preprocessor...\")\n","    # *** Assign results to the variable names specified by the analyst node ***\n","    processed_None = preprocessor.fit_transform(X_train_fe)\n","    processed_None = preprocessor.transform(X_test_fe)\n","\n","    print(f\"Processed train shape (variable 'processed_None'): {processed_None.shape}\")\n","    print(f\"Processed test shape (variable 'processed_None'): {processed_None.shape}\")\n","\n","else:\n","    print(\"No features identified for preprocessing. Processed data will be empty.\")\n","    # Handle case where no features are processed - might need to create empty arrays\n","    processed_None = np.empty((X_train_fe.shape[0], 0)) # Create empty numpy array with correct rows\n","    processed_None = np.empty((X_test_fe.shape[0], 0))\n","    print(f\"Processed train shape (variable 'processed_None'): {processed_None.shape}\")\n","    print(f\"Processed test shape (variable 'processed_None'): {processed_None.shape}\")\n","\n","\n","# The processed data is now available as numpy arrays named 'processed_None' and 'processed_None'\n","# The target variable 'y' is assumed to also be available from the initial loading step.\n","# This completes the Analyst's task of preparing the data for the Scientist.\n","\n","# Expected outputs from this section (as variables in your environment):\n","# - processed_None: Processed training features (potentially including target)\n","# - processed_None: Processed test features\n","\n","# --- 2. Model Training and Evaluation ---\n","# This section uses the processed data to train machine learning models (LightGBM, XGBoost) and evaluate them.\n","# It assumes variables like 'processed_None' and 'processed_None' exist from the previous section.\n","\n","# --- Imports for the generated modeling script ---\n","import pandas as pd # Might be needed if submission is loaded/modified\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_log_error\n","import lightgbm as lgb\n","import xgboost as xgb\n","from catboost import CatBoostRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# expected_shape should be evaluated within the generated code\n","expected_shape = (processed_None.shape[0] if 'processed_None' in locals() else 0)\n","\n","# Import TensorFlow components if available\n","try:\n","    import tensorflow as tf\n","    import tensorflow.keras.backend as K\n","    from tensorflow.keras.layers import Dense, Input\n","    from tensorflow.keras.models import Model\n","    from tensorflow.keras.optimizers import Adam\n","except ImportError:\n","    print(\"TensorFlow not available. TensorFlow model will be skipped.\")\n","    tf = None # Set tf to None to check availability later\n","\n","# --- Helper Functions (Copying the ones from Analyst's generated code) ---\n","# Ensure these match what the analyst generated, especially the RMSLE and TF functions\n","\n","def swish(x):\n","    # Need to handle both numpy (for direct use if needed) and tensorflow\n","    if tf and isinstance(x, tf.Tensor):\n","         return x * tf.keras.backend.sigmoid(x)\n","    else: # Assume numpy or similar\n","         # This numpy version is just for local testing outside TF context if needed\n","         # Actual TF model will use the TF version\n","         return x / (1 + np.exp(-x)) # Sigmoid approximation for numpy\n","\n","def root_mean_squared_error(y_true, y_pred):\n","    # Need to handle both numpy and tensorflow\n","    if tf and isinstance(y_true, tf.Tensor):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","    else: # Assume numpy\n","        return np.sqrt(np.mean(np.square(y_pred - y_true)))\n","\n","\n","def build_swish_mlp(input_shape):\n","    if tf is None:\n","        print(\"TensorFlow not available, cannot build swish MLP.\")\n","        return None\n","    inputs = Input(shape=input_shape)\n","    x = Dense(64, activation=swish)(inputs)\n","    x = Dense(128, activation=swish)(x)\n","    x = Dense(64, activation=swish)(x)\n","    x = Dense(1)(x)\n","    model = Model(inputs, x)\n","    # Use RootMeanSquaredError metric for TensorFlow\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","    return model\n","\n","# rmsle function from sklearn.metrics\n","def rmsle(y_true, y_pred):\n","    # Ensure predictions are non-negative for log calculation\n","    y_pred = np.maximum(0, y_pred)\n","    # Ensure inputs are numpy arrays and handle potential shape issues\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","    # Escape braces for f-string inside the generated code\n","    print(f\"{Debug RMSLE shapes - True: {y_true.shape}, Pred: {y_pred.shape}}\") # Corrected escaping\n","    if y_true.shape != y_pred.shape:\n","        print(f\"{Warning: Shape mismatch between true and predicted values ({y_true.shape} vs {y_pred.shape}). Cannot compute RMSLE.}\") # Corrected escaping\n","        return float('inf')\n","    # Add a small epsilon to avoid log(0)\n","    return np.sqrt(mean_squared_log_error(np.maximum(0, y_true) + 1e-9, y_pred + 1e-9))\n","\n","\n","# --- Data Splitting ---\n","print(\"\n","Splitting data into training and validation sets...\")\n","# Use the processed train data variable name from the analyst node\n","# Assuming the target 'y' is also available from the initial data loading/prep\n","# Escape the variables being checked and accessed so they evaluate in the generated code context\n","if 'processed_None' in locals() and 'y' in locals() and {processed_train_name}.shape[0] == y.shape[0]:\n","    X_train, X_val, y_train, y_val = train_test_split(processed_None, y, test_size=0.2, random_state=42)\n","    # Escape braces for f-strings inside the generated code\n","    print(f\"{Train shapes: {X_train.shape}, {y_train.shape}}\") # CORRECTED\n","    print(f\"{Validation shapes: {X_val.shape}, {y_val.shape}}\") # CORRECTED\n","    data_split_success = True\n","else:\n","    # Escape braces for f-strings inside the generated code\n","    print(f\"{Error: Processed train data ('processed_None') or target ('y') not found or shapes mismatch. Skipping modeling.}\")\n","    # Use { } for the outer f-string, and {variable} for variables inside the inner f-string\n","    print(f\"{'processed_None' in locals(): {'processed_None' in locals()}}\") # CORRECTED\n","    print(f\"{'y' in locals(): {'y' in locals()}}\") # CORRECTED\n","    # Escape variables/accessors being checked and accessed\n","    if 'processed_None' in locals(): print(f\"{Shape of 'processed_None': {locals()['processed_None'].shape}}\") # CORRECTED\n","    if 'y' in locals(): print(f\"{Shape of 'y': {y.shape}}\") # CORRECTED\n","    X_train, X_val, y_train, y_val = np.array([]).reshape(0,-1), np.array([]).reshape(0,-1), np.array([]), np.array([]) # Create empty arrays\n","    data_split_success = False\n","\n","\n","if data_split_success and X_train.shape[0] > 0:\n","    # --- Scaling for Neural Network ---\n","    # NN often performs better with features scaled to a 0-1 range and log-transformed target\n","    print(\"\n","Scaling features for Neural Network using MinMaxScaler...\")\n","    scaler_nn = MinMaxScaler()\n","    X_train_scaled = scaler_nn.fit_transform(X_train)\n","    X_val_scaled = scaler_nn.transform(X_val)\n","    # Scale the actual test set using the processed test data variable name\n","    # Escape variable being checked and accessed\n","    if 'processed_None' in locals():\n","       X_test_scaled = scaler_nn.transform({processed_test_name})\n","       test_scaling_success = True\n","    else:\n","       print(f\"{Error: Processed test data ('processed_None') not found for scaling.}\")\n","       X_test_scaled = np.array([]).reshape(0,-1)\n","       test_scaling_success = False\n","\n","\n","    print(\"Log transforming target for NN...\")\n","    y_train_log = np.log1p(y_train)\n","    y_val_log = np.log1p(y_val)\n","\n","    # --- Model Training ---\n","\n","    # Train LightGBM (using split processed data - numpy arrays)\n","    print(\"\n","Training LightGBM...\")\n","    lgb_model = lgb.LGBMRegressor(objective='regression', metric='rmse', n_estimators=2000, learning_rate=0.03, num_leaves=40, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1, lambda_l1=0.1, lambda_l2=0.1, seed=42, n_jobs=-1, verbose=-1)\n","    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n","    lgb_val_pred = lgb_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if 'processed_None' in locals():\n","       lgb_test_pred = lgb_model.predict({processed_test_name})\n","    else:\n","       # Escape variable accessor being accessed in the generated code\n","       lgb_test_pred = np.zeros({processed_test_name}.shape[0] if 'processed_None' in locals() else 0) # Predict zeros if test data missing\n","       print(f\"{Warning: 'processed_None' not available for LightGBM test prediction.}\")\n","    print(\"LightGBM training complete.\")\n","\n","    # Train XGBoost (using split processed data - numpy arrays)\n","    print(\"\n","Training XGBoost...\")\n","    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', n_estimators=2000, learning_rate=0.03, max_depth=7, subsample=0.8, colsample_bytree=0.8, lambda_l1=0.1, lambda_l2=0.1, seed=42, n_jobs=-1, tree_method='hist') # Use hist for faster training\n","    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n","    xgb_val_pred = xgb_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if 'processed_None' in locals():\n","       xgb_test_pred = xgb_model.predict({processed_test_name})\n","    else:\n","        # Escape variable accessor being accessed in the generated code\n","        xgb_test_pred = np.zeros({processed_test_name}.shape[0] if 'processed_None' in locals() else 0)\n","        print(f\"{Warning: 'processed_None' not available for XGBoost test prediction.}\")\n","    print(\"XGBoost training complete.\")\n","\n","    # Train RandomForestRegressor (using split processed data - numpy arrays)\n","    print(\"\n","Training RandomForestRegressor...\")\n","    rf_model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n","    rf_model.fit(X_train, y_train)\n","    rf_val_pred = rf_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if 'processed_None' in locals():\n","        rf_test_pred = rf_model.predict({processed_test_name})\n","    else:\n","        # Escape variable accessor being accessed in the generated code\n","        rf_test_pred = np.zeros({processed_test_name}.shape[0] if 'processed_None' in locals() else 0)\n","        print(f\"{Warning: 'processed_None' not available for RandomForest test prediction.}\")\n","    print(\"RandomForestRegressor training complete.\")\n","\n","    # Train CatBoostRegressor (using split processed data - numpy arrays)\n","    print(\"\n","Training CatBoostRegressor...\")\n","    cat_model = CatBoostRegressor(iterations=2000, learning_rate=0.03, depth=7, random_seed=42, verbose=0, early_stopping_rounds=100, l2_leaf_reg=3, loss_function='RMSE')\n","    cat_model.fit(X_train, y_train, eval_set=(X_val, y_val))\n","    cat_val_pred = cat_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if 'processed_None' in locals():\n","        cat_test_pred = cat_model.predict({processed_test_name})\n","    else:\n","         # Escape variable accessor being accessed in the generated code\n","         cat_test_pred = np.zeros({processed_test_name}.shape[0] if 'processed_None' in locals() else 0)\n","         print(f\"{Warning: 'processed_None' not available for CatBoost test prediction.}\")\n","    print(\"CatBoostRegressor training complete.\")\n","\n","\n","    # Build and train TensorFlow model (using scaled data)\n","    if tf and test_scaling_success: # Only attempt TF if TF is available and test data was scaled\n","        print(\"\n","Building and training TensorFlow model...\")\n","        if X_train_scaled.shape[0] == 0 or X_train_scaled.shape[1] == 0:\n","             print(\"{Warning: Scaled training data is empty. Skipping TensorFlow model training.}\") # Corrected escaping\n","             tf_test_pred = np.zeros(X_test_scaled.shape[0])\n","             tf_val_pred = np.zeros(X_val_scaled.shape[0])\n","        else:\n","            tf_model = build_swish_mlp(input_shape=(X_train_scaled.shape[1],))\n","            if tf_model: # Check if model was built successfully\n","                early_stopping_tf = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=15, restore_best_weights=True, mode='min', verbose=0)\n","\n","                print(\"{  Training TensorFlow model...}\") # Corrected escaping\n","                history = tf_model.fit(X_train_scaled, y_train_log,\n","                                       epochs=200,\n","                                       batch_size=64,\n","                                       verbose=0,\n","                                       validation_data=(X_val_scaled, y_val_log),\n","                                       callbacks=[early_stopping_tf])\n","                print(\"{  TensorFlow model training complete.}\") # Corrected escaping\n","\n","                print(\"{Making TensorFlow predictions...}\") # Corrected escaping\n","                tfy_test_pred_log = tf_model.predict(X_test_scaled).flatten()\n","                tf_test_pred = np.expm1(tfy_test_pred_log)\n","                # Clip to original target range (need 'y' min/max - assuming 'y' is available and is the original target)\n","                if 'y' in locals():\n","                    # Escape accessor y.min() and y.max()\n","                    tf_test_pred = np.clip(tf_test_pred, {y}.min(), {y}.max())\n","                else:\n","                     # Fallback clipping or warning if y is not available\n","                     tf_test_pred = np.maximum(0, tf_test_pred) # At least ensure non-negative\n","                     print(\"{Warning: Original target 'y' not available for TensorFlow prediction clipping.}\") # Corrected escaping\n","\n","\n","                tfy_val_pred_log = tf_model.predict(X_val_scaled).flatten()\n","                tf_val_pred = np.expm1(tfy_val_pred_log)\n","                if 'y' in locals():\n","                    # Escape accessor y.min() and y.max()\n","                    tf_val_pred = np.clip(tf_val_pred, {y}.min(), {y}.max())\n","                else:\n","                    tf_val_pred = np.maximum(0, tf_val_pred)\n","                    print(\"{Warning: Original target 'y' not available for TensorFlow validation prediction clipping.}\") # Corrected escaping\n","                print(\"{TensorFlow predictions made.}\") # Corrected escaping\n","            else: # TF model build failed\n","                tf_test_pred = np.zeros(X_test_scaled.shape[0])\n","                tf_val_pred = np.zeros(X_val_scaled.shape[0])\n","                print(\"{TensorFlow model could not be built. Skipping TF predictions.}\") # Corrected escaping\n","\n","    else: # TF not available or test data not scaled\n","        print(\"\n","Skipping TensorFlow model training and prediction.\")\n","        # Ensure prediction variables exist even if TF is skipped\n","        # Need the shape of the processed test data to create zeros array\n","        # Escape variable accessor being accessed in the generated code\n","        test_data_shape = ({processed_test_name}.shape[0] if 'processed_None' in locals() else 0)\n","        tf_test_pred = np.zeros(test_data_shape)\n","        tf_val_pred = np.zeros(X_val.shape[0]) # Use X_val shape for validation predictions\n","        print(\"{TensorFlow prediction variables initialized to zeros.}\") # Corrected escaping\n","\n","\n","    # --- Validation Scores ---\n","    print(f\"\n","--- Validation RMSLE Scores ---\")\n","    # Escape accessor y_val.shape\n","    if {y_val}.shape[0] > 0:\n","        # Check if prediction variables exist and have correct shape before computing scores\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'lgb_val_pred' in locals() and {lgb_val_pred}.shape == {y_val}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{LightGBM Validation RMSLE: {rmsle(y_val, lgb_val_pred):.5f}}\") # CORRECTED\n","        else:\n","           print(\"{LightGBM Validation RMSLE: N/A (predictions missing or shape mismatch)}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'xgb_val_pred' in locals() and {xgb_val_pred}.shape == {y_val}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{XGBoost Validation RMSLE: {rmsle(y_val, xgb_val_pred):.5f}}\") # CORRECTED\n","        else:\n","            print(\"{XGBoost Validation RMSLE: N/A (predictions missing or shape mismatch)}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'rf_val_pred' in locals() and {rf_val_pred}.shape == {y_val}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{RandomForest Validation RMSLE: {rmsle(y_val, rf_val_pred):.5f}}\") # CORRECTED\n","        else:\n","            print(\"{RandomForest Validation RMSLE: N/A (predictions missing or shape mismatch)}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'cat_val_pred' in locals() and {cat_val_pred}.shape == {y_val}.shape:\n","            # Escape braces for f-string inside the generated code\n","            print(f\"{CatBoost Validation RMSLE: {rmsle(y_val, cat_val_pred):.5f}}\") # CORRECTED\n","        else:\n","             print(\"{CatBoost Validation RMSLE: N/A (predictions missing or shape mismatch)}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'tf_val_pred' in locals() and {tf_val_pred}.shape == {y_val}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{TensorFlow Validation RMSLE: {rmsle(y_val, tf_val_pred):.5f}}\") # CORRECTED\n","        else:\n","            print(\"{TensorFlow Validation RMSLE: N/A (predictions missing or shape mismatch)}\")\n","\n","    else:\n","        print(\"{No validation data available to compute scores.}\")\n","    print(f\"------------------------------\")\n","\n","    # --- Ensemble Prediction ---\n","    print(\"\n","Creating ensemble prediction...\")\n","\n","    # Ensure all test prediction variables exist and have the same expected shape\n","    # The expected shape is the number of rows in the processed test data\n","\n","\n","    # List of models and their test predictions\n","    # Escape prediction variable accessors\n","    test_preds = {\n","        'lgb': lgb_test_pred if 'lgb_test_pred' in locals() and {lgb_test_pred}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'xgb': xgb_test_pred if 'xgb_test_pred' in locals() and {xgb_test_pred}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'rf':  rf_test_pred  if 'rf_test_pred'  in locals() and {rf_test_pred}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'cat': cat_test_pred if 'cat_test_pred' in locals() and {cat_test_pred}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'tf':  tf_test_pred  if 'tf_test_pred'  in locals() and {tf_test_pred}.shape[0] == expected_shape else np.zeros(expected_shape),\n","    }\n","    # Escape prediction variable accessors and y_val shape accessor\n","    val_preds = {\n","        'lgb': lgb_val_pred if 'lgb_val_pred' in locals() and {lgb_val_pred}.shape == {y_val}.shape else np.zeros({y_val}.shape[0]),\n","        'xgb': xgb_val_pred if 'xgb_val_pred' in locals() and {xgb_val_pred}.shape == {y_val}.shape else np.zeros({y_val}.shape[0]),\n","        'rf':  rf_val_pred  if 'rf_val_pred'  in locals() and {rf_val_pred}.shape == {y_val}.shape else np.zeros({y_val}.shape[0]),\n","        'cat': cat_val_pred if 'cat_val_pred' in locals() and {cat_val_pred}.shape == {y_val}.shape else np.zeros({y_val}.shape[0]),\n","        'tf':  tf_val_pred  if 'tf_val_pred'  in locals() and {tf_val_pred}.shape == {y_val}.shape else np.zeros({y_val}.shape[0]),\n","    }\n","\n","    # Calculate weights based on validation performance (using RMSLE)\n","    # Avoid division by zero or using infinite RMSLE from errors\n","    # Escape accessors for val_preds and y_val\n","    val_rmsles = {\n","        'lgb': rmsle({y_val}, {val_preds}['lgb']) if {y_val}.shape[0] > 0 and np.sum(np.abs({val_preds}['lgb'])) > 0 else float('inf'),\n","        'xgb': rmsle({y_val}, {val_preds}['xgb']) if {y_val}.shape[0] > 0 and np.sum(np.abs({val_preds}['xgb'])) > 0 else float('inf'),\n","        'rf':  rmsle({y_val}, {val_preds}['rf'])  if {y_val}.shape[0] > 0 and np.sum(np.abs({val_preds}['rf'])) > 0 else float('inf'),\n","        'cat': rmsle({y_val}, {val_preds}['cat']) if {y_val}.shape[0] > 0 and np.sum(np.abs({val_preds}['cat'])) > 0 else float('inf'),\n","        'tf':  rmsle({y_val}, {val_preds}['tf'])  if {y_val}.shape[0] > 0 and np.sum(np.abs({val_preds}['tf'])) > 0 and tf is not None else float('inf'), # Only include TF if available\n","    }\n","\n","    # Calculate inverse RMSLE (higher for better models), handle inf/zero\n","    # Escape dictionary view accessors\n","    inv_rmsles = {k: (1.0 / v if v != 0 and v != float('inf') else 0) for k, v in {val_rmsles}.items()}\n","\n","    # Normalize to get weights\n","    # Escape dictionary view accessors\n","    total_inv = sum({inv_rmsles}.values())\n","    if total_inv > 1e-9: # Avoid division by near zero\n","        # Escape dictionary view accessors\n","        weights = {k: v / total_inv for k, v in {inv_rmsles}.items()}\n","    else: # If all models failed or had infinite RMSLE, use equal weights or zero weights\n","        print(\"{Warning: Cannot calculate meaningful ensemble weights (all models failed or infinite RMSLE). Using equal weights as fallback.}\") # Corrected escaping\n","        # Escape dictionary view accessors\n","        valid_models = [m for m, inv in {inv_rmsles}.items() if inv > 0]\n","        if valid_models:\n","             equal_weight = 1.0 / len(valid_models)\n","             # Escape dictionary creation and looping\n","             weights = {m: equal_weight for m in valid_models}\n","             for m in models: # Ensure all keys are in weights dict\n","                 if m not in weights: weights[m] = 0\n","        else:\n","             print(\"{Warning: No models trained successfully. Ensemble prediction will be zero.}\") # Corrected escaping\n","             # Escape dictionary creation\n","             weights = {m: 0 for m in models}\n","\n","\n","    # Escape braces for f-string inside the generated code\n","    print(f\"{Model weights: {weights}}\") # CORRECTED\n","\n","    # Calculate weighted ensemble predictions\n","    ensemble_predictions = np.zeros(expected_shape)\n","    for model in models:\n","        # Escape weights and test_preds dictionary accessors\n","        ensemble_predictions += {weights}[model] * {test_preds}[model]\n","\n","\n","    # Ensure predictions are non-negative (calories can't be negative)\n","    ensemble_predictions = np.maximum(0, ensemble_predictions)\n","\n","    # --- Create Submission ---\n","    # Assume 'submission' DataFrame with an 'id' column is available from initial loading\n","    print(\"\n","Preparing submission file...\")\n","    # Escape submission DataFrame accessors (.columns, .shape[0]) and expected_shape\n","    if 'submission' in locals() and 'id' in {submission}.columns and {submission}.shape[0] == {expected_shape}:\n","        # Escape submission DataFrame item assignment and method call\n","        {submission}['Calories'] = ensemble_predictions\n","        {submission}.to_csv('ensemble_submission.csv', index=False)\n","        print(\"{Submission file created: ensemble_submission.csv}\") # Corrected escaping\n","    else:\n","        print(\"{Error: Submission DataFrame not found, invalid, or shape mismatch. Cannot create submission file.}\") # Corrected escaping\n","        print(\"{Details:}\") # Corrected escaping\n","        if 'submission' not in locals(): print(\"{'submission' variable not found in generated code locals().}\") # Corrected escaping\n","        # Escape submission DataFrame column check\n","        elif 'id' not in {submission}.columns: print(\"{'submission' DataFrame missing 'id' column.}\") # Corrected escaping\n","        # Escape submission DataFrame shape and expected_shape comparison within the *inner* f-string.\n","        # This needs to be escaped using { and } in the outer f-string.\n","        elif {submission}.shape[0] != {expected_shape}: print(f\"{Submission shape ({submission.shape[0]}) does not match test prediction shape ({expected_shape}).}\") # CORRECTED\n","    # --- CORRECTED LINES END HERE ---\n","\n","\n","    print(\"\n","Process complete!\") # This print does not contain a variable, so no inner f-string escaping needed\n","\n","else:\n","    # Escape variable accessor being accessed in the generated code\n","    print(\"\\nSkipping model training, validation, and submission due to data split failure or empty training data.\")\n","    # Ensure ensemble_predictions variable exists for potential return, even if zero\n","    ensemble_predictions = np.zeros({processed_test_name}.shape[0] if 'processed_None' in locals() else 0)\n","\n","# Expected outputs/artifacts from this section:\n","# - Console output showing training progress and validation scores.\n","# - 'lgb_metric_plot.png', 'xgb_metric_plot.png': Plots showing model performance during training.\n","# - 'lgb_test_preds.csv', 'xgb_test_preds.csv': CSV files with predictions on the test set.\n","\n","# --- 3. Summary Notes ---\n","# NOTE: The automated workflow appears to have completed successfully. Review the generated files and console output.\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, QuantileTransformer\n","from sklearn.impute import KNNImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestRegressor # This import might be needed in the generated code\n","\n","# --- Helper Functions (Copying the ones provided by user) ---\n","# Assuming these were defined or should be included in the generated code\n","\n","# Define helper functions here (copying from user's input)\n","# ... (your create_features, swish, root_mean_squared_error, build_swish_mlp, rmsle functions go here)\n","def create_features(df):\n","    '''\n","    Create advanced features from the input data.\n","    This function serves as a form of \"data augmentation\" for tabular data\n","    by deriving new, potentially more informative features from existing ones.\n","    '''\n","    df = df.copy()\n","    epsilon = 1e-6 # Small constant to avoid division by zero\n","\n","    # --- Basic Interaction & Ratio Features (>= 6 columns engineered) ---\n","\n","    # 1. Body Mass Index (BMI): Standard health metric, relates weight and height.\n","    df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2 + epsilon)\n","\n","    # 2. Duration * Heart_Rate: Represents total heartbeats during exercise, proxy for effort.\n","    df['Duration_HR'] = df['Duration'] * df['Heart_Rate']\n","\n","    # 3. Duration * Body_Temp: Represents total heat generated during exercise, proxy for intensity/effort.\n","    df['Duration_Temp'] = df['Duration'] * df['Body_Temp']\n","\n","    # 4. Weight / Height Ratio: Simpler ratio than BMI, might capture different linear relationships.\n","    df['Weight_Height_Ratio'] = df['Weight'] / (df['Height'] + epsilon)\n","\n","    # 5. Heart Rate per Minute: Already captured by Heart_Rate, but could consider rate relative to duration?\n","    #    Let's create Heart_Rate_per_Duration instead.\n","    df['Heart_Rate_per_Duration'] = df['Heart_Rate'] / (df['Duration'] + epsilon) # Rate of heartbeats per minute of exercise\n","\n","    # 6. Body Temperature per Minute: Change in temp per minute of exercise.\n","    df['Body_Temp_per_Duration'] = df['Body_Temp'] / (df['Duration'] + epsilon)\n","\n","    # --- More Complex Interaction Features ---\n","\n","    # 7. Exercise Intensity: Combines HR, Duration, and inversely related to Age (older might have lower max HR).\n","    df['Exercise_Intensity'] = (df['Heart_Rate'] * df['Duration']) / (df['Age'] + epsilon)\n","\n","    # 8. Metabolic Factor: Combines BMI, Duration, HR, and inversely related to Age. A complex interaction term.\n","    df['Metabolic_Factor'] = (df['BMI'] * df['Duration'] * df['Heart_Rate']) / (df['Age'] + epsilon)\n","\n","    # 9. Age * Duration Interaction: Older individuals exercising for longer might have different calorie burn patterns.\n","    df['Age_Duration_Interaction'] = df['Age'] * df['Duration']\n","\n","    # 10. Height * Weight Product: Simple interaction, might capture overall body size effect differently than BMI.\n","    df['Height_Weight_Product'] = df['Height'] * df['Weight']\n","\n","    # --- Polynomial Features (Example for Duration) ---\n","    # Captures non-linear relationship with Duration.\n","    df['Duration_Sq'] = df['Duration']**2\n","\n","    # --- Categorical Features (Discretization) ---\n","\n","    # 11. Age Group: Discretizing Age can help capture non-linear effects or group-specific patterns.\n","    df['Age_Group'] = pd.cut(df['Age'], bins=[0, 25, 35, 45, 55, 65, 100], labels=['<25', '25-35', '35-45', '45-55', '55-65', '65+'], right=False)\n","    df['Age_Group'] = df['Age_Group'].astype(object).fillna('Unknown')\n","\n","\n","    # 12. BMI Category: Standard health categories for BMI.\n","    df['BMI_Category'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, 35, 40, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese I', 'Obese II', 'Obese III'], right=False)\n","    df['BMI_Category'] = df['BMI_Category'].astype(object).fillna('Unknown')\n","\n","\n","    # 13. Heart Rate Zones (Simplified): Based on max HR (approx 220-Age) and exercise intensity.\n","    df['Max_HR_Estimate'] = 220 - df['Age']\n","    df['HR_Zone'] = pd.cut(df['Heart_Rate'] / (df['Max_HR_Estimate'] + epsilon),\n","                           bins=[0, 0.6, 0.7, 0.8, 0.9, 1.1], # Example zones based on % of max HR\n","                           labels=['<60%', '60-70%', '70-80%', '80-90%', '>90%'],\n","                           right=False)\n","    df['HR_Zone'] = df['HR_Zone'].astype(object).fillna('Unknown')\n","    df = df.drop('Max_HR_Estimate', axis=1) # Drop intermediate column\n","\n","\n","    # 14. Duration Category: Simple bins for exercise duration.\n","    df['Duration_Category'] = pd.cut(df['Duration'], bins=[0, 15, 30, 45, 60, 120, 300], labels=['<15min', '15-30min', '30-45min', '45-60min', '1-2hr', '>2hr'], right=False)\n","    df['Duration_Category'] = df['Duration_Category'].astype(object).fillna('Unknown')\n","\n","\n","    # Interaction between Sex and Duration/HeartRate/BodyTemp\n","    df['Sex_Male_Duration'] = df['Duration'] * (df['Sex'] == 'M')\n","    df['Sex_Female_Duration'] = df['Duration'] * (df['Sex'] == 'F')\n","    df['Sex_Male_HR'] = df['Heart_Rate'] * (df['Sex'] == 'M')\n","    df['Sex_Female_HR'] = df['Heart_Rate'] * (df['Sex'] == 'F')\n","\n","\n","    return df\n","\n","\n","# --- Load Data (Assuming 'train' and 'test' DataFrames are already loaded) ---\n","print(\"Using input DataFrames named 'None' and 'None'...\")\n","#Assuming 'y' and 'submission' also exist\n","If 'id' and 'Calories' need dropping, it should happen *before* feature engineering on the original DFs.\n","# Let's assume the initial DataFrames passed in via 'None' and 'None'\n","# are already prepped (ids dropped, target 'y' separated).\n","If not, add that step here:\n","y = None['Calories'] \n","# None = None.drop('Calories', axis=1) # Example drop\n","\n","# Example: If train_df/test_df *contain* id/Calories\n","# train_df_working = None.copy()\n","# test_df_working = None.copy()\n","# if 'id' in train_df_working.columns: train_ids = train_df_working['id'] ; train_df_working = train_df_working.drop('id', axis=1)\n","# if 'Calories' in train_df_working.columns: y = train_df_working['Calories'] ; train_df_working = train_df_working.drop('Calories', axis=1)\n","# if 'id' in test_df_working.columns: test_ids = test_df_working['id'] ; test_df_working = test_df_working.drop('id', axis=1)\n","# print(f\"Initial train shape: {train_df_working.shape}\")\n","# print(f\"Initial test shape: {test_df_working.shape}\")\n","\n","\n","# --- Feature Engineering ---\n","print(\"\n","Applying feature engineering...\")\n","# Apply feature engineering to the working copies (or original DFs if pre-cleaned)\n","# Using the assumed initial variable names from the state.\n","X_train_fe = create_features(None) \n","X_test_fe = create_features(None)\n","\n","print(f\"Train shape after feature engineering: {X_train_fe.shape}\")\n","print(f\"Test shape after feature engineering: {X_test_fe.shape}\")\n","\n","# Identify numerical and categorical features AFTER feature engineering\n","# Ensure handling of potential errors if a feature was expected but not created\n","all_features = list(X_train_fe.columns)\n","numerical_features = [col for col in all_features if X_train_fe[col].dtype in ['int64', 'float64']]\n","categorical_features = [col for col in all_features if X_train_fe[col].dtype == 'object'] # Using object dtype for pandas categorical/string\n","\n","print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n","print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n","\n","# --- Preprocessing Pipeline (KNN Imputation & Quantile Transformation + OneHot) ---\n","print(\"\n","Setting up preprocessing pipeline...\")\n","\n","# Check if numerical/categorical features exist before creating pipelines\n","transformers = []\n","if numerical_features:\n","    numerical_pipeline = Pipeline([\n","        ('imputer', KNNImputer(n_neighbors=5)),       # KNN imputation\n","        ('scaler', QuantileTransformer(output_distribution='normal')) # Robust scaling via QuantileTransformer\n","    ])\n","    transformers.append(('num', numerical_pipeline, numerical_features))\n","else:\n","    print(\"No numerical features found. Skipping numerical pipeline.\")\n","\n","if categorical_features:\n","    categorical_pipeline = Pipeline([\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)) # One-Hot Encoding\n","    ])\n","    transformers.append(('cat', categorical_pipeline, categorical_features))\n","else:\n","     print(\"No categorical features found. Skipping categorical pipeline.\")\n","\n","# Create ColumnTransformer only if there are transformers\n","if transformers:\n","    preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n","\n","    # Fit and Transform data\n","    print(\"Fitting and transforming data with preprocessor...\")\n","    # *** Assign results to the variable names specified by the analyst node ***\n","    processed_None = preprocessor.fit_transform(X_train_fe)\n","    processed_None = preprocessor.transform(X_test_fe)\n","\n","    print(f\"Processed train shape (variable 'processed_None'): {processed_None.shape}\")\n","    print(f\"Processed test shape (variable 'processed_None'): {processed_None.shape}\")\n","\n","else:\n","    print(\"No features identified for preprocessing. Processed data will be empty.\")\n","    # Handle case where no features are processed - might need to create empty arrays\n","    processed_None = np.empty((X_train_fe.shape[0], 0)) # Create empty numpy array with correct rows\n","    processed_None = np.empty((X_test_fe.shape[0], 0))\n","    print(f\"Processed train shape (variable 'processed_None'): {processed_None.shape}\")\n","    print(f\"Processed test shape (variable 'processed_None'): {processed_None.shape}\")\n","\n","\n","# The processed data is now available as numpy arrays named 'processed_None' and 'processed_None'\n","# The target variable 'y' is assumed to also be available from the initial loading step.\n","# This completes the Analyst's task of preparing the data for the Scientist.\n","\n","\n","# --- End of Analyst Script ---\n","# --- Imports for the generated modeling script ---\n","import pandas as pd # Might be needed if submission is loaded/modified\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_log_error\n","import lightgbm as lgb\n","import xgboost as xgb\n","from catboost import CatBoostRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# expected_shape should be evaluated within the generated code\n","expected_shape = (processed_None.shape[0] if 'processed_None' in locals() else 0)\n","\n","# Import TensorFlow components if available\n","try:\n","    import tensorflow as tf\n","    import tensorflow.keras.backend as K\n","    from tensorflow.keras.layers import Dense, Input\n","    from tensorflow.keras.models import Model\n","    from tensorflow.keras.optimizers import Adam\n","except ImportError:\n","    print(\"TensorFlow not available. TensorFlow model will be skipped.\")\n","    tf = None # Set tf to None to check availability later\n","\n","# --- Helper Functions (Copying the ones from Analyst's generated code) ---\n","# Ensure these match what the analyst generated, especially the RMSLE and TF functions\n","\n","def swish(x):\n","    # Need to handle both numpy (for direct use if needed) and tensorflow\n","    if tf and isinstance(x, tf.Tensor):\n","         return x * tf.keras.backend.sigmoid(x)\n","    else: # Assume numpy or similar\n","         # This numpy version is just for local testing outside TF context if needed\n","         # Actual TF model will use the TF version\n","         return x / (1 + np.exp(-x)) # Sigmoid approximation for numpy\n","\n","def root_mean_squared_error(y_true, y_pred):\n","    # Need to handle both numpy and tensorflow\n","    if tf and isinstance(y_true, tf.Tensor):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","    else: # Assume numpy\n","        return np.sqrt(np.mean(np.square(y_pred - y_true)))\n","\n","\n","def build_swish_mlp(input_shape):\n","    if tf is None:\n","        print(\"TensorFlow not available, cannot build swish MLP.\")\n","        return None\n","    inputs = Input(shape=input_shape)\n","    x = Dense(64, activation=swish)(inputs)\n","    x = Dense(128, activation=swish)(x)\n","    x = Dense(64, activation=swish)(x)\n","    x = Dense(1)(x)\n","    model = Model(inputs, x)\n","    # Use RootMeanSquaredError metric for TensorFlow\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","    return model\n","\n","# rmsle function from sklearn.metrics\n","def rmsle(y_true, y_pred):\n","    # Ensure predictions are non-negative for log calculation\n","    y_pred = np.maximum(0, y_pred)\n","    # Ensure inputs are numpy arrays and handle potential shape issues\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","    # Escape braces for f-string inside the generated code\n","    print(f\"{Debug RMSLE shapes - True: {y_true.shape}, Pred: {y_pred.shape}}\") # Corrected escaping\n","    if y_true.shape != y_pred.shape:\n","        print(f\"{Warning: Shape mismatch between true and predicted values ({y_true.shape} vs {y_pred.shape}). Cannot compute RMSLE.}\") # Corrected escaping\n","        return float('inf')\n","    # Add a small epsilon to avoid log(0)\n","    return np.sqrt(mean_squared_log_error(np.maximum(0, y_true) + 1e-9, y_pred + 1e-9))\n","\n","\n","# --- Data Splitting ---\n","print(\"\n","Splitting data into training and validation sets...\")\n","# Use the processed train data variable name from the analyst node\n","# Assuming the target 'y' is also available from the initial data loading/prep\n","# Escape the variables being checked and accessed so they evaluate in the generated code context\n","if 'processed_None' in locals() and 'y' in locals() and {processed_train_name}.shape[0] == y.shape[0]:\n","    X_train, X_val, y_train, y_val = train_test_split(processed_None, y, test_size=0.2, random_state=42)\n","    # Escape braces for f-strings inside the generated code\n","    print(f\"{Train shapes: {X_train.shape}, {y_train.shape}}\") # CORRECTED\n","    print(f\"{Validation shapes: {X_val.shape}, {y_val.shape}}\") # CORRECTED\n","    data_split_success = True\n","else:\n","    # Escape braces for f-strings inside the generated code\n","    print(f\"{Error: Processed train data ('processed_None') or target ('y') not found or shapes mismatch. Skipping modeling.}\")\n","    # Use { } for the outer f-string, and {variable} for variables inside the inner f-string\n","    print(f\"{'processed_None' in locals(): {'processed_None' in locals()}}\") # CORRECTED\n","    print(f\"{'y' in locals(): {'y' in locals()}}\") # CORRECTED\n","    # Escape variables/accessors being checked and accessed\n","    if 'processed_None' in locals(): print(f\"{Shape of 'processed_None': {locals()['processed_None'].shape}}\") # CORRECTED\n","    if 'y' in locals(): print(f\"{Shape of 'y': {y.shape}}\") # CORRECTED\n","    X_train, X_val, y_train, y_val = np.array([]).reshape(0,-1), np.array([]).reshape(0,-1), np.array([]), np.array([]) # Create empty arrays\n","    data_split_success = False\n","\n","\n","if data_split_success and X_train.shape[0] > 0:\n","    # --- Scaling for Neural Network ---\n","    # NN often performs better with features scaled to a 0-1 range and log-transformed target\n","    print(\"\n","Scaling features for Neural Network using MinMaxScaler...\")\n","    scaler_nn = MinMaxScaler()\n","    X_train_scaled = scaler_nn.fit_transform(X_train)\n","    X_val_scaled = scaler_nn.transform(X_val)\n","    # Scale the actual test set using the processed test data variable name\n","    # Escape variable being checked and accessed\n","    if 'processed_None' in locals():\n","       X_test_scaled = scaler_nn.transform({processed_test_name})\n","       test_scaling_success = True\n","    else:\n","       print(f\"{Error: Processed test data ('processed_None') not found for scaling.}\")\n","       X_test_scaled = np.array([]).reshape(0,-1)\n","       test_scaling_success = False\n","\n","\n","    print(\"Log transforming target for NN...\")\n","    y_train_log = np.log1p(y_train)\n","    y_val_log = np.log1p(y_val)\n","\n","    # --- Model Training ---\n","\n","    # Train LightGBM (using split processed data - numpy arrays)\n","    print(\"\n","Training LightGBM...\")\n","    lgb_model = lgb.LGBMRegressor(objective='regression', metric='rmse', n_estimators=2000, learning_rate=0.03, num_leaves=40, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1, lambda_l1=0.1, lambda_l2=0.1, seed=42, n_jobs=-1, verbose=-1)\n","    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n","    lgb_val_pred = lgb_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if 'processed_None' in locals():\n","       lgb_test_pred = lgb_model.predict({processed_test_name})\n","    else:\n","       # Escape variable accessor being accessed in the generated code\n","       lgb_test_pred = np.zeros({processed_test_name}.shape[0] if 'processed_None' in locals() else 0) # Predict zeros if test data missing\n","       print(f\"{Warning: 'processed_None' not available for LightGBM test prediction.}\")\n","    print(\"LightGBM training complete.\")\n","\n","    # Train XGBoost (using split processed data - numpy arrays)\n","    print(\"\n","Training XGBoost...\")\n","    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', n_estimators=2000, learning_rate=0.03, max_depth=7, subsample=0.8, colsample_bytree=0.8, lambda_l1=0.1, lambda_l2=0.1, seed=42, n_jobs=-1, tree_method='hist') # Use hist for faster training\n","    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n","    xgb_val_pred = xgb_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if 'processed_None' in locals():\n","       xgb_test_pred = xgb_model.predict({processed_test_name})\n","    else:\n","        # Escape variable accessor being accessed in the generated code\n","        xgb_test_pred = np.zeros({processed_test_name}.shape[0] if 'processed_None' in locals() else 0)\n","        print(f\"{Warning: 'processed_None' not available for XGBoost test prediction.}\")\n","    print(\"XGBoost training complete.\")\n","\n","    # Train RandomForestRegressor (using split processed data - numpy arrays)\n","    print(\"\n","Training RandomForestRegressor...\")\n","    rf_model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n","    rf_model.fit(X_train, y_train)\n","    rf_val_pred = rf_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if 'processed_None' in locals():\n","        rf_test_pred = rf_model.predict({processed_test_name})\n","    else:\n","        # Escape variable accessor being accessed in the generated code\n","        rf_test_pred = np.zeros({processed_test_name}.shape[0] if 'processed_None' in locals() else 0)\n","        print(f\"{Warning: 'processed_None' not available for RandomForest test prediction.}\")\n","    print(\"RandomForestRegressor training complete.\")\n","\n","    # Train CatBoostRegressor (using split processed data - numpy arrays)\n","    print(\"\n","Training CatBoostRegressor...\")\n","    cat_model = CatBoostRegressor(iterations=2000, learning_rate=0.03, depth=7, random_seed=42, verbose=0, early_stopping_rounds=100, l2_leaf_reg=3, loss_function='RMSE')\n","    cat_model.fit(X_train, y_train, eval_set=(X_val, y_val))\n","    cat_val_pred = cat_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    # Escape variable being checked and accessed\n","    if 'processed_None' in locals():\n","        cat_test_pred = cat_model.predict({processed_test_name})\n","    else:\n","         # Escape variable accessor being accessed in the generated code\n","         cat_test_pred = np.zeros({processed_test_name}.shape[0] if 'processed_None' in locals() else 0)\n","         print(f\"{Warning: 'processed_None' not available for CatBoost test prediction.}\")\n","    print(\"CatBoostRegressor training complete.\")\n","\n","\n","    # Build and train TensorFlow model (using scaled data)\n","    if tf and test_scaling_success: # Only attempt TF if TF is available and test data was scaled\n","        print(\"\n","Building and training TensorFlow model...\")\n","        if X_train_scaled.shape[0] == 0 or X_train_scaled.shape[1] == 0:\n","             print(\"{Warning: Scaled training data is empty. Skipping TensorFlow model training.}\") # Corrected escaping\n","             tf_test_pred = np.zeros(X_test_scaled.shape[0])\n","             tf_val_pred = np.zeros(X_val_scaled.shape[0])\n","        else:\n","            tf_model = build_swish_mlp(input_shape=(X_train_scaled.shape[1],))\n","            if tf_model: # Check if model was built successfully\n","                early_stopping_tf = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=15, restore_best_weights=True, mode='min', verbose=0)\n","\n","                print(\"{  Training TensorFlow model...}\") # Corrected escaping\n","                history = tf_model.fit(X_train_scaled, y_train_log,\n","                                       epochs=200,\n","                                       batch_size=64,\n","                                       verbose=0,\n","                                       validation_data=(X_val_scaled, y_val_log),\n","                                       callbacks=[early_stopping_tf])\n","                print(\"{  TensorFlow model training complete.}\") # Corrected escaping\n","\n","                print(\"{Making TensorFlow predictions...}\") # Corrected escaping\n","                tfy_test_pred_log = tf_model.predict(X_test_scaled).flatten()\n","                tf_test_pred = np.expm1(tfy_test_pred_log)\n","                # Clip to original target range (need 'y' min/max - assuming 'y' is available and is the original target)\n","                if 'y' in locals():\n","                    # Escape accessor y.min() and y.max()\n","                    tf_test_pred = np.clip(tf_test_pred, {y}.min(), {y}.max())\n","                else:\n","                     # Fallback clipping or warning if y is not available\n","                     tf_test_pred = np.maximum(0, tf_test_pred) # At least ensure non-negative\n","                     print(\"{Warning: Original target 'y' not available for TensorFlow prediction clipping.}\") # Corrected escaping\n","\n","\n","                tfy_val_pred_log = tf_model.predict(X_val_scaled).flatten()\n","                tf_val_pred = np.expm1(tfy_val_pred_log)\n","                if 'y' in locals():\n","                    # Escape accessor y.min() and y.max()\n","                    tf_val_pred = np.clip(tf_val_pred, {y}.min(), {y}.max())\n","                else:\n","                    tf_val_pred = np.maximum(0, tf_val_pred)\n","                    print(\"{Warning: Original target 'y' not available for TensorFlow validation prediction clipping.}\") # Corrected escaping\n","                print(\"{TensorFlow predictions made.}\") # Corrected escaping\n","            else: # TF model build failed\n","                tf_test_pred = np.zeros(X_test_scaled.shape[0])\n","                tf_val_pred = np.zeros(X_val_scaled.shape[0])\n","                print(\"{TensorFlow model could not be built. Skipping TF predictions.}\") # Corrected escaping\n","\n","    else: # TF not available or test data not scaled\n","        print(\"\n","Skipping TensorFlow model training and prediction.\")\n","        # Ensure prediction variables exist even if TF is skipped\n","        # Need the shape of the processed test data to create zeros array\n","        # Escape variable accessor being accessed in the generated code\n","        test_data_shape = ({processed_test_name}.shape[0] if 'processed_None' in locals() else 0)\n","        tf_test_pred = np.zeros(test_data_shape)\n","        tf_val_pred = np.zeros(X_val.shape[0]) # Use X_val shape for validation predictions\n","        print(\"{TensorFlow prediction variables initialized to zeros.}\") # Corrected escaping\n","\n","\n","    # --- Validation Scores ---\n","    print(f\"\n","--- Validation RMSLE Scores ---\")\n","    # Escape accessor y_val.shape\n","    if {y_val}.shape[0] > 0:\n","        # Check if prediction variables exist and have correct shape before computing scores\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'lgb_val_pred' in locals() and {lgb_val_pred}.shape == {y_val}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{LightGBM Validation RMSLE: {rmsle(y_val, lgb_val_pred):.5f}}\") # CORRECTED\n","        else:\n","           print(\"{LightGBM Validation RMSLE: N/A (predictions missing or shape mismatch)}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'xgb_val_pred' in locals() and {xgb_val_pred}.shape == {y_val}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{XGBoost Validation RMSLE: {rmsle(y_val, xgb_val_pred):.5f}}\") # CORRECTED\n","        else:\n","            print(\"{XGBoost Validation RMSLE: N/A (predictions missing or shape mismatch)}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'rf_val_pred' in locals() and {rf_val_pred}.shape == {y_val}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{RandomForest Validation RMSLE: {rmsle(y_val, rf_val_pred):.5f}}\") # CORRECTED\n","        else:\n","            print(\"{RandomForest Validation RMSLE: N/A (predictions missing or shape mismatch)}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'cat_val_pred' in locals() and {cat_val_pred}.shape == {y_val}.shape:\n","            # Escape braces for f-string inside the generated code\n","            print(f\"{CatBoost Validation RMSLE: {rmsle(y_val, cat_val_pred):.5f}}\") # CORRECTED\n","        else:\n","             print(\"{CatBoost Validation RMSLE: N/A (predictions missing or shape mismatch)}\")\n","\n","        # Escape accessors prediction.shape and y_val.shape\n","        if 'tf_val_pred' in locals() and {tf_val_pred}.shape == {y_val}.shape:\n","           # Escape braces for f-string inside the generated code\n","           print(f\"{TensorFlow Validation RMSLE: {rmsle(y_val, tf_val_pred):.5f}}\") # CORRECTED\n","        else:\n","            print(\"{TensorFlow Validation RMSLE: N/A (predictions missing or shape mismatch)}\")\n","\n","    else:\n","        print(\"{No validation data available to compute scores.}\")\n","    print(f\"------------------------------\")\n","\n","    # --- Ensemble Prediction ---\n","    print(\"\n","Creating ensemble prediction...\")\n","\n","    # Ensure all test prediction variables exist and have the same expected shape\n","    # The expected shape is the number of rows in the processed test data\n","\n","\n","    # List of models and their test predictions\n","    # Escape prediction variable accessors\n","    test_preds = {\n","        'lgb': lgb_test_pred if 'lgb_test_pred' in locals() and {lgb_test_pred}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'xgb': xgb_test_pred if 'xgb_test_pred' in locals() and {xgb_test_pred}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'rf':  rf_test_pred  if 'rf_test_pred'  in locals() and {rf_test_pred}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'cat': cat_test_pred if 'cat_test_pred' in locals() and {cat_test_pred}.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'tf':  tf_test_pred  if 'tf_test_pred'  in locals() and {tf_test_pred}.shape[0] == expected_shape else np.zeros(expected_shape),\n","    }\n","    # Escape prediction variable accessors and y_val shape accessor\n","    val_preds = {\n","        'lgb': lgb_val_pred if 'lgb_val_pred' in locals() and {lgb_val_pred}.shape == {y_val}.shape else np.zeros({y_val}.shape[0]),\n","        'xgb': xgb_val_pred if 'xgb_val_pred' in locals() and {xgb_val_pred}.shape == {y_val}.shape else np.zeros({y_val}.shape[0]),\n","        'rf':  rf_val_pred  if 'rf_val_pred'  in locals() and {rf_val_pred}.shape == {y_val}.shape else np.zeros({y_val}.shape[0]),\n","        'cat': cat_val_pred if 'cat_val_pred' in locals() and {cat_val_pred}.shape == {y_val}.shape else np.zeros({y_val}.shape[0]),\n","        'tf':  tf_val_pred  if 'tf_val_pred'  in locals() and {tf_val_pred}.shape == {y_val}.shape else np.zeros({y_val}.shape[0]),\n","    }\n","\n","    # Calculate weights based on validation performance (using RMSLE)\n","    # Avoid division by zero or using infinite RMSLE from errors\n","    # Escape accessors for val_preds and y_val\n","    val_rmsles = {\n","        'lgb': rmsle({y_val}, {val_preds}['lgb']) if {y_val}.shape[0] > 0 and np.sum(np.abs({val_preds}['lgb'])) > 0 else float('inf'),\n","        'xgb': rmsle({y_val}, {val_preds}['xgb']) if {y_val}.shape[0] > 0 and np.sum(np.abs({val_preds}['xgb'])) > 0 else float('inf'),\n","        'rf':  rmsle({y_val}, {val_preds}['rf'])  if {y_val}.shape[0] > 0 and np.sum(np.abs({val_preds}['rf'])) > 0 else float('inf'),\n","        'cat': rmsle({y_val}, {val_preds}['cat']) if {y_val}.shape[0] > 0 and np.sum(np.abs({val_preds}['cat'])) > 0 else float('inf'),\n","        'tf':  rmsle({y_val}, {val_preds}['tf'])  if {y_val}.shape[0] > 0 and np.sum(np.abs({val_preds}['tf'])) > 0 and tf is not None else float('inf'), # Only include TF if available\n","    }\n","\n","    # Calculate inverse RMSLE (higher for better models), handle inf/zero\n","    # Escape dictionary view accessors\n","    inv_rmsles = {k: (1.0 / v if v != 0 and v != float('inf') else 0) for k, v in {val_rmsles}.items()}\n","\n","    # Normalize to get weights\n","    # Escape dictionary view accessors\n","    total_inv = sum({inv_rmsles}.values())\n","    if total_inv > 1e-9: # Avoid division by near zero\n","        # Escape dictionary view accessors\n","        weights = {k: v / total_inv for k, v in {inv_rmsles}.items()}\n","    else: # If all models failed or had infinite RMSLE, use equal weights or zero weights\n","        print(\"{Warning: Cannot calculate meaningful ensemble weights (all models failed or infinite RMSLE). Using equal weights as fallback.}\") # Corrected escaping\n","        # Escape dictionary view accessors\n","        valid_models = [m for m, inv in {inv_rmsles}.items() if inv > 0]\n","        if valid_models:\n","             equal_weight = 1.0 / len(valid_models)\n","             # Escape dictionary creation and looping\n","             weights = {m: equal_weight for m in valid_models}\n","             for m in models: # Ensure all keys are in weights dict\n","                 if m not in weights: weights[m] = 0\n","        else:\n","             print(\"{Warning: No models trained successfully. Ensemble prediction will be zero.}\") # Corrected escaping\n","             # Escape dictionary creation\n","             weights = {m: 0 for m in models}\n","\n","\n","    # Escape braces for f-string inside the generated code\n","    print(f\"{Model weights: {weights}}\") # CORRECTED\n","\n","    # Calculate weighted ensemble predictions\n","    ensemble_predictions = np.zeros(expected_shape)\n","    for model in models:\n","        # Escape weights and test_preds dictionary accessors\n","        ensemble_predictions += {weights}[model] * {test_preds}[model]\n","\n","\n","    # Ensure predictions are non-negative (calories can't be negative)\n","    ensemble_predictions = np.maximum(0, ensemble_predictions)\n","\n","    # --- Create Submission ---\n","    # Assume 'submission' DataFrame with an 'id' column is available from initial loading\n","    print(\"\n","Preparing submission file...\")\n","    # Escape submission DataFrame accessors (.columns, .shape[0]) and expected_shape\n","    if 'submission' in locals() and 'id' in {submission}.columns and {submission}.shape[0] == {expected_shape}:\n","        # Escape submission DataFrame item assignment and method call\n","        {submission}['Calories'] = ensemble_predictions\n","        {submission}.to_csv('ensemble_submission.csv', index=False)\n","        print(\"{Submission file created: ensemble_submission.csv}\") # Corrected escaping\n","    else:\n","        print(\"{Error: Submission DataFrame not found, invalid, or shape mismatch. Cannot create submission file.}\") # Corrected escaping\n","        print(\"{Details:}\") # Corrected escaping\n","        if 'submission' not in locals(): print(\"{'submission' variable not found in generated code locals().}\") # Corrected escaping\n","        # Escape submission DataFrame column check\n","        elif 'id' not in {submission}.columns: print(\"{'submission' DataFrame missing 'id' column.}\") # Corrected escaping\n","        # Escape submission DataFrame shape and expected_shape comparison within the *inner* f-string.\n","        # This needs to be escaped using { and } in the outer f-string.\n","        elif {submission}.shape[0] != {expected_shape}: print(f\"{Submission shape ({submission.shape[0]}) does not match test prediction shape ({expected_shape}).}\") # CORRECTED\n","    # --- CORRECTED LINES END HERE ---\n","\n","\n","    print(\"\n","Process complete!\") # This print does not contain a variable, so no inner f-string escaping needed\n","\n","else:\n","    # Escape variable accessor being accessed in the generated code\n","    print(\"\\nSkipping model training, validation, and submission due to data split failure or empty training data.\")\n","    # Ensure ensemble_predictions variable exists for potential return, even if zero\n","    ensemble_predictions = np.zeros({processed_test_name}.shape[0] if 'processed_None' in locals() else 0)\n","\n","    \n","# --- End of Scientist Script ---\n","# --- End of Script ---\n","# Copy and paste the entire block above into your environment/notebook to run the workflow!\n","```\n","\n","\n","==================================================\n","\n","\n","--- SUPERVISOR ---\n","Supervisor reviewing state. Last message type: AIMessage\n","Supervisor received content: Here is the complete Python script generated by the workflow, combining preprocessing and modeling steps:\n","\n","```python\n","# Combined Data Science and Data Analyst IA's Workflow Script\n","# Generated by Analys...\n","Original Analyst output available: True\n","Scientist output available: True\n","Interpreter finished signal: True\n","Final answer generated flag: False\n","loop =======> # None\n","Supervisor: Scientist AI review complete. Final answer should be ready.\n","DEBUG: Supervisor RETURNING next_agent: 'code_compiler_AI' (Type: <class 'str'>)\n","DEBUG: Supervisor RETURNING final_answer_generated: True\n","DEBUG: Supervisor RETURNING interpreter_finished: True\n","\n","--- Workflow Step 7 ---\n","Processing update from node: Supervisor\n","ğŸ¤– Supervisor says: Workflow complete. Final script and reviews have been generated.\n","\n","==================================================\n","\n","\n","--- CODE_COMPILER_IA (Combining Scripts) ---\n","CALOR-IA Scientist: LLM response.\n","--- CALOR_IA_DATA_SCIENTIST Runtime Error during LLM call ---\n","Error Type: NotFound\n","Error Details: 404 models/gemini-2.5-flash-preview-04-17 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n","CODE_COMPILER_IA: Combined script generated successfully.\n","\n","--- Workflow Step 8 ---\n","Processing update from node: code_compiler_AI\n","ğŸ¤– code_compiler_AI says: An error occurred while generating the final summary: 404 models/gemini-2.5-flash-preview-04-17 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n","\n","==================================================\n","\n","\n","--- SUPERVISOR ---\n","Supervisor reviewing state. Last message type: AIMessage\n","Supervisor received content: An error occurred while generating the final summary: 404 models/gemini-2.5-flash-preview-04-17 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the...\n","Original Analyst output available: True\n","Scientist output available: True\n","Interpreter finished signal: True\n","Final answer generated flag: True\n","loop =======> # None\n","Supervisor: Final answer signal received. Ending workflow.\n","DEBUG: Supervisor RETURNING next_agent: 'DataAnalyst_AI' (Type: <class 'str'>)\n","DEBUG: Supervisor RETURNING final_answer_generated: False\n","DEBUG: Supervisor RETURNING interpreter_finished: True\n","\n","--- Workflow Step 9 ---\n","Processing update from node: Supervisor\n","ğŸ¤– Supervisor says: Workflow complete. lets loop over trough all specialist one more time.\n","\n","==================================================\n","\n","\n","--- CALOR-IA DATA ANALYST (Post-Compilation Review) ---\n","CALOR-IA Analyst invoking LLM for summary...\n","CALOR-IA Analyst: LLM response.\n","--- CALOR-IA Analyst Runtime Error during LLM call ---\n","Error Type: NotFound\n","Error Details: 404 models/gemini-2.5-flash-preview-04-17 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n","CALOR-IA Analyst adding message to state: An error occurred while generating the final summary: 404 models/gemini-2.5-flash-preview-04-17 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the...\n","CALOR-IA Analyst routing to Supervisor.\n","\n","--- Workflow Step 10 ---\n","Processing update from node: DataAnalyst_AI\n","\n","==================================================\n","\n","\n","--- SUPERVISOR ---\n","Supervisor reviewing state. Last message type: AIMessage\n","Supervisor received content: Workflow complete. lets loop over trough all specialist one more time.\n","Original Analyst output available: True\n","Scientist output available: True\n","Interpreter finished signal: True\n","Final answer generated flag: False\n","loop =======> # 0\n","Supervisor: Scientist AI review complete. Final answer should be ready.\n","DEBUG: Supervisor RETURNING next_agent: 'code_compiler_AI' (Type: <class 'str'>)\n","DEBUG: Supervisor RETURNING final_answer_generated: True\n","DEBUG: Supervisor RETURNING interpreter_finished: True\n","\n","--- Workflow Step 11 ---\n","Processing update from node: Supervisor\n","\n","==================================================\n","\n","\n","--- CODE_COMPILER_IA (Combining Scripts) ---\n","CODE_COMPILER_IA: LLM response.\n","--- CALOR_IA_DATA_SCIENTIST Runtime Error during LLM call ---\n","Error Type: NotFound\n","Error Details: 404 models/gemini-2.5-flash-preview-04-17 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n","\n","--- Workflow Step 12 ---\n","Processing update from node: code_compiler_AI\n","\n","==================================================\n","\n","\n","--- SUPERVISOR ---\n","Supervisor reviewing state. Last message type: AIMessage\n","Supervisor received content: Workflow complete. Final script and reviews have been generated.\n","Original Analyst output available: True\n","Scientist output available: True\n","Interpreter finished signal: True\n","Final answer generated flag: True\n","loop =======> # 0\n","\n","--- Workflow Step 13 ---\n","Processing update from node: Supervisor\n","\n","==================================================\n","\n","\n","--- Workflow Finished ---\n"]}],"source":["import json # Make sure json is imported if you haven't already\n","\n","# Assuming imports, node definitions, graph setup, and compilation are done above\n","\n","# --- Run the Graph ---\n","\n","# Initial state setup (unchanged)\n","initial_input_message = f\"\"\" Start working ğŸ’ª\n","\"\"\"\n","initial_state = {\n","    \"messages\": [HumanMessage(content=initial_input_message)],\n","    \"final_answer_generated\": False,\n","    \"current_task_description\": \"Preprocess and analyze training and test data.\", \n","}\n","\n","print(\"\\n--- STARTING WORKFLOW (Using DataFrame Variable Names) ---\")\n","\n","# Keep track of seen message contents to avoid reprinting supervisor messages repeatedly\n","seen_message_contents = set()\n","\n","\n","for step, event in enumerate(app.stream(initial_state, {\"recursion_limit\":40})):\n","    print(f\"\\n--- Workflow Step {step + 1} ---\")\n","    for node_name, update in event.items():\n","        print(f\"Processing update from node: {node_name}\")\n","\n","        # Check if the update is valid and process messages\n","        if update is None:\n","            print(\"Node returned None, no state update.\")\n","            continue # Skip processing if the node returned None\n","\n","        # Process messages added in this update\n","        if 'messages' in update:\n","            new_messages_in_update = [\n","                msg for msg in update['messages']\n","                if isinstance(msg, (AIMessage, HumanMessage)) and msg.content not in seen_message_contents\n","            ]\n","            for msg in new_messages_in_update:\n","                if isinstance(msg, AIMessage):\n","                    print(f\"ğŸ¤– {node_name} says: {msg.content}\")\n","                elif isinstance(msg, HumanMessage):\n","                    print(f\"ğŸ§‘â€ğŸ’» User says (via state): {msg.content}\") # User messages might reappear if state is passed\n","                seen_message_contents.add(msg.content)\n","\n","\n","        # The HumanInterpreter output is already formatted with the code block\n","        # We don't need special processing here if it's added to messages\n","        # The loop above processing 'messages' will print it when the HumanInterpreter runs\n","        # If you wanted to handle HumanInterpreter output *differently* here, you could add:\n","        # if node_name == 'HumanInterpreter':\n","        #     # Access the state after the update to get the new message\n","        #     final_message = update.get('messages', [])[-1] if update.get('messages') else None\n","        #     if final_message and isinstance(final_message, AIMessage):\n","        #          print(\"\\n--- FINAL REPORT ---\")\n","        #          print(final_message.content) # Print the full markdown content\n","        #          print(\"--- END FINAL REPORT ---\")\n","\n","\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","# After the loop finishes, the graph execution has completed (__end__ reached or limit hit)\n","print(\"\\n--- Workflow Finished ---\")\n","\n"]},{"cell_type":"markdown","id":"e19dabef","metadata":{"papermill":{"duration":0.010056,"end_time":"2025-10-22T22:24:40.548666","exception":false,"start_time":"2025-10-22T22:24:40.53861","status":"completed"},"tags":[]},"source":["# **Ready to Run!**\n","\n","**This code is designed to be copied directly into a cell in your Kaggle notebook. Just make sure you have already loaded your initial train.csv and test.csv files into DataFrames named train and test respectively, before this code block.**\n","\n","**The script will then:**\n","\n","**ğŸš€ Preprocess train and test using the defined pipelines and KNN Imputer, saving the results as processed_train_name and processed_test_name.**\n","**ğŸ§  This version is ready to copy and paste so my goal was made and I am very produ of me if you read this message please leave a comment**\n","**ğŸ“Š Now I  have to find how to make them make me win, or maybe you can do it for me**\n","**ğŸ’¾ you can make a node to execute the code the we do not have to copy and paste the code, or you can play with LLM for each agent**\n","**What's Next?**\n","\n","****\n","\n","**This workflow shows just how powerful collaborative AI agents can be in jumpstarting your machine learning projects. Go ahead, copy the code, run it, and see the magic happen! âœ¨ Let me know what you build next in the comments!**"]},{"cell_type":"code","execution_count":14,"id":"b78f7f92","metadata":{"execution":{"iopub.execute_input":"2025-10-22T22:24:40.572664Z","iopub.status.busy":"2025-10-22T22:24:40.572313Z","iopub.status.idle":"2025-10-22T22:57:06.271462Z","shell.execute_reply":"2025-10-22T22:57:06.270091Z"},"papermill":{"duration":1945.713881,"end_time":"2025-10-22T22:57:06.273433","exception":false,"start_time":"2025-10-22T22:24:40.559552","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-10-22 22:24:50.812293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761171891.096518      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761171891.174848      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Loading data...\n","Data loaded successfully.\n","Separating target variable and dropping 'id' column...\n","Initial train shape (features only): (750000, 7)\n","Initial test shape (features only): (250000, 7)\n","Target variable 'y_train_original' shape: (750000,)\n","\n","Applying feature engineering...\n","Train shape after feature engineering: (750000, 26)\n","Test shape after feature engineering: (250000, 26)\n","\n","Numerical features (21): ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'BMI', 'Duration_HR', 'Duration_Temp', 'Weight_Height_Ratio', 'Heart_Rate_per_Duration', 'Body_Temp_per_Duration', 'Exercise_Intensity', 'Metabolic_Factor', 'Age_Duration_Interaction', 'Height_Weight_Product', 'Duration_Sq', 'Sex_Male_Duration', 'Sex_Female_Duration', 'Sex_Male_HR', 'Sex_Female_HR']\n","Categorical features (5): ['Sex', 'Age_Group', 'BMI_Category', 'HR_Zone', 'Duration_Category']\n","\n","Setting up preprocessing pipeline...\n","Added numerical pipeline for 21 features.\n","Added categorical pipeline for 5 features.\n","Fitting preprocessor on engineered training data...\n","Transforming engineered training data...\n","Transforming engineered test data...\n","\n","Processed train shape: (750000, 38)\n","Processed test shape: (250000, 38)\n","\n","Data preprocessing complete.\n","\n","Splitting data into training and validation sets...\n","Train shapes: (600000, 38), (600000,)\n","Validation shapes: (150000, 38), (150000,)\n","\n","Scaling features for Neural Network using MinMaxScaler...\n","Log transforming target for NN...\n","\n","Training LightGBM...\n","LightGBM training complete.\n","\n","Training XGBoost...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [22:26:30] WARNING: /workspace/src/learner.cc:742: \n","Parameters: { \"lambda_l1\", \"lambda_l2\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["XGBoost training complete.\n","\n","Training RandomForestRegressor...\n","RandomForestRegressor training complete.\n","\n","Training CatBoostRegressor...\n","CatBoostRegressor training complete.\n","\n","Building and training TensorFlow model...\n","  Training TensorFlow model...\n"]},{"name":"stderr","output_type":"stream","text":["2025-10-22 22:41:38.194616: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"]},{"name":"stdout","output_type":"stream","text":["  TensorFlow model training complete.\n","Making TensorFlow predictions...\n","\u001b[1m7813/7813\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step\n","\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step\n","TensorFlow predictions made.\n","\n","--- Validation RMSLE Scores ---\n","LightGBM Validation RMSLE: 0.06225\n","XGBoost Validation RMSLE: 0.06157\n","RandomForest Validation RMSLE: 0.06260\n","CatBoost Validation RMSLE: 0.06164\n","TensorFlow Validation RMSLE: 0.06123\n","------------------------------\n","\n","Creating ensemble prediction...\n","Model weights: {'lgb': 0.19871160601395138, 'xgb': 0.2009303091010189, 'rf': 0.19762246556470436, 'cat': 0.2006908373798366, 'tf': 0.20204478194048886}\n","\n","Preparing submission file...\n","Submission file created: submission.csv\n","\n","Process complete!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, QuantileTransformer\n","from sklearn.impute import KNNImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_log_error\n","import lightgbm as lgb\n","import xgboost as xgb\n","from catboost import CatBoostRegressor\n","\n","# Import TensorFlow components if available\n","try:\n","    import tensorflow as tf\n","    import tensorflow.keras.backend as K\n","    from tensorflow.keras.layers import Dense, Input\n","    from tensorflow.keras.models import Model\n","    from tensorflow.keras.optimizers import Adam\n","except ImportError:\n","    print(\"TensorFlow not available. TensorFlow model will be skipped.\")\n","    tf = None # Set tf to None to check availability later\n","\n","# --- Helper Functions ---\n","def create_features(df):\n","    '''\n","    Create advanced features from the input data.\n","    This function serves as a form of \"data augmentation\" for tabular data\n","    by deriving new, potentially more informative features from existing ones.\n","    '''\n","    df = df.copy()\n","    epsilon = 1e-6 # Small constant to avoid division by zero\n","\n","    # --- Basic Interaction & Ratio Features (>= 6 columns engineered) ---\n","\n","    # 1. Body Mass Index (BMI): Standard health metric, relates weight and height.\n","    df['BMI'] = df['Weight'] / ((df['Height'] / 100) ** 2 + epsilon)\n","\n","    # 2. Duration * Heart_Rate: Represents total heartbeats during exercise, proxy for effort.\n","    df['Duration_HR'] = df['Duration'] * df['Heart_Rate']\n","\n","    # 3. Duration * Body_Temp: Represents total heat generated during exercise, proxy for intensity/effort.\n","    df['Duration_Temp'] = df['Duration'] * df['Body_Temp']\n","\n","    # 4. Weight / Height Ratio: Simpler ratio than BMI, might capture different linear relationships.\n","    df['Weight_Height_Ratio'] = df['Weight'] / (df['Height'] + epsilon)\n","\n","    # 5. Heart Rate per Minute: Already captured by Heart_Rate, but could consider rate relative to duration?\n","    #    Let's create Heart_Rate_per_Duration instead.\n","    df['Heart_Rate_per_Duration'] = df['Heart_Rate'] / (df['Duration'] + epsilon) # Rate of heartbeats per minute of exercise\n","\n","    # 6. Body Temperature per Minute: Change in temp per minute of exercise.\n","    df['Body_Temp_per_Duration'] = df['Body_Temp'] / (df['Duration'] + epsilon)\n","\n","    # --- More Complex Interaction Features ---\n","\n","    # 7. Exercise Intensity: Combines HR, Duration, and inversely related to Age (older might have lower max HR).\n","    df['Exercise_Intensity'] = (df['Heart_Rate'] * df['Duration']) / (df['Age'] + epsilon)\n","\n","    # 8. Metabolic Factor: Combines BMI, Duration, HR, and inversely related to Age. A complex interaction term.\n","    df['Metabolic_Factor'] = (df['BMI'] * df['Duration'] * df['Heart_Rate']) / (df['Age'] + epsilon)\n","\n","    # 9. Age * Duration Interaction: Older individuals exercising for longer might have different calorie burn patterns.\n","    df['Age_Duration_Interaction'] = df['Age'] * df['Duration']\n","\n","    # 10. Height * Weight Product: Simple interaction, might capture overall body size effect differently than BMI.\n","    df['Height_Weight_Product'] = df['Height'] * df['Weight']\n","\n","    # --- Polynomial Features (Example for Duration) ---\n","    # Captures non-linear relationship with Duration.\n","    df['Duration_Sq'] = df['Duration']**2\n","\n","    # --- Categorical Features (Discretization) ---\n","\n","    # 11. Age Group: Discretizing Age can help capture non-linear effects or group-specific patterns.\n","    df['Age_Group'] = pd.cut(df['Age'], bins=[0, 25, 35, 45, 55, 65, 100], labels=['<25', '25-35', '35-45', '45-55', '55-65', '65+'], right=False)\n","    df['Age_Group'] = df['Age_Group'].astype(object).fillna('Unknown')\n","\n","\n","    # 12. BMI Category: Standard health categories for BMI.\n","    df['BMI_Category'] = pd.cut(df['BMI'], bins=[0, 18.5, 25, 30, 35, 40, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese I', 'Obese II', 'Obese III'], right=False)\n","    df['BMI_Category'] = df['BMI_Category'].astype(object).fillna('Unknown')\n","\n","\n","    # 13. Heart Rate Zones (Simplified): Based on max HR (approx 220-Age) and exercise intensity.\n","    df['Max_HR_Estimate'] = 220 - df['Age']\n","    df['HR_Zone'] = pd.cut(df['Heart_Rate'] / (df['Max_HR_Estimate'] + epsilon),\n","                           bins=[0, 0.6, 0.7, 0.8, 0.9, 1.1], # Example zones based on % of max HR\n","                           labels=['<60%', '60-70%', '70-80%', '80-90%', '>90%'],\n","                           right=False)\n","    df['HR_Zone'] = df['HR_Zone'].astype(object).fillna('Unknown')\n","    df = df.drop('Max_HR_Estimate', axis=1) # Drop intermediate column\n","\n","\n","    # 14. Duration Category: Simple bins for exercise duration.\n","    df['Duration_Category'] = pd.cut(df['Duration'], bins=[0, 15, 30, 45, 60, 120, 300], labels=['<15min', '15-30min', '30-45min', '45-60min', '1-2hr', '>2hr'], right=False)\n","    df['Duration_Category'] = df['Duration_Category'].astype(object).fillna('Unknown')\n","\n","\n","    # Interaction between Sex and Duration/HeartRate/BodyTemp\n","    df['Sex_Male_Duration'] = df['Duration'] * (df['Sex'] == 'M')\n","    df['Sex_Female_Duration'] = df['Duration'] * (df['Sex'] == 'F')\n","    df['Sex_Male_HR'] = df['Heart_Rate'] * (df['Sex'] == 'M')\n","    df['Sex_Female_HR'] = df['Heart_Rate'] * (df['Sex'] == 'F')\n","\n","\n","    return df\n","\n","# TensorFlow specific helper functions\n","if tf:\n","    def swish(x):\n","        return x * tf.keras.backend.sigmoid(x)\n","\n","    def root_mean_squared_error(y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.cast(y_pred, tf.float32)\n","        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n","\n","    def build_swish_mlp(input_shape):\n","        inputs = Input(shape=input_shape)\n","        x = Dense(64, activation=swish)(inputs)\n","        x = Dense(128, activation=swish)(x)\n","        x = Dense(64, activation=swish)(x)\n","        x = Dense(1)(x)\n","        model = Model(inputs, x)\n","        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","        return model\n","\n","# rmsle function from sklearn.metrics\n","def rmsle(y_true, y_pred):\n","    # Ensure predictions are non-negative for log calculation\n","    y_pred = np.maximum(0, y_pred)\n","    # Ensure inputs are numpy arrays and handle potential shape issues\n","    y_true = np.asarray(y_true)\n","    y_pred = np.asarray(y_pred)\n","    # print(f\"Debug RMSLE shapes - True: {y_true.shape}, Pred: {y_pred.shape}\") # Debug print\n","    if y_true.shape != y_pred.shape:\n","        print(f\"Warning: Shape mismatch between true and predicted values ({y_true.shape} vs {y_pred.shape}). Cannot compute RMSLE.\")\n","        return float('inf')\n","    # Add a small epsilon to avoid log(0)\n","    return np.sqrt(mean_squared_log_error(np.maximum(0, y_true) + 1e-9, y_pred + 1e-9))\n","\n","\n","# --- Load Data ---\n","print(\"Loading data...\")\n","try:\n","    train = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv')\n","    test = pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\n","    submission = pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')\n","    print(\"Data loaded successfully.\")\n","except FileNotFoundError:\n","    print(\"Ensure train.csv, test.csv, and sample_submission.csv are in the specified path.\")\n","    # Exit or handle error appropriately in a real script\n","    # For this example, we'll create dummy data if files aren't found\n","    print(\"Creating dummy data for demonstration.\")\n","    data = {\n","        'id': range(100),\n","        'Sex': np.random.choice(['M', 'F'], 100),\n","        'Age': np.random.randint(15, 80, 100),\n","        'Height': np.random.uniform(150, 190, 100),\n","        'Weight': np.random.uniform(50, 100, 100),\n","        'Duration': np.random.uniform(10, 60, 100),\n","        'Heart_Rate': np.random.uniform(80, 180, 100),\n","        'Body_Temp': np.random.uniform(37, 40, 100),\n","        'Calories': np.random.uniform(50, 500, 100)\n","    }\n","    train = pd.DataFrame(data)\n","    test = pd.DataFrame(data).drop('Calories', axis=1).head(50) # Smaller test set\n","    submission = pd.DataFrame({'id': test['id'], 'Calories': 0})\n","\n","\n","# --- Separate Target and Drop ID ---\n","print(\"Separating target variable and dropping 'id' column...\")\n","y_train_original = train['Calories']\n","train_ids = train['id'] # Keep train ids if needed later, though not for training\n","test_ids = test['id'] # Keep test ids for submission\n","\n","train_df_features = train.drop(['id', 'Calories'], axis=1)\n","test_df_features = test.drop('id', axis=1)\n","\n","print(f\"Initial train shape (features only): {train_df_features.shape}\")\n","print(f\"Initial test shape (features only): {test_df_features.shape}\")\n","print(f\"Target variable 'y_train_original' shape: {y_train_original.shape}\")\n","\n","\n","# --- Feature Engineering (Data Augmentation for Tabular Data) ---\n","print(\"\\nApplying feature engineering...\")\n","X_train_fe = create_features(train_df_features)\n","X_test_fe = create_features(test_df_features)\n","\n","print(f\"Train shape after feature engineering: {X_train_fe.shape}\")\n","print(f\"Test shape after feature engineering: {X_test_fe.shape}\")\n","\n","# Identify numerical and categorical features AFTER feature engineering\n","# Ensure handling of potential errors if a feature was expected but not created\n","numerical_features = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n","categorical_features = X_train_fe.select_dtypes(include='object').columns.tolist() # Using object dtype for pandas categorical/string\n","\n","print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n","print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n","\n","# --- Preprocessing Pipeline (KNN Imputation & Quantile Transformation + OneHot) ---\n","print(\"\\nSetting up preprocessing pipeline...\")\n","\n","# Check if numerical/categorical features exist before creating pipelines\n","transformers = []\n","if numerical_features:\n","    numerical_pipeline = Pipeline([\n","        ('imputer', KNNImputer(n_neighbors=5)),       # KNN imputation\n","        ('scaler', QuantileTransformer(output_distribution='normal', n_quantiles=100)) # Robust scaling via QuantileTransformer\n","    ])\n","    transformers.append(('num', numerical_pipeline, numerical_features))\n","    print(f\"Added numerical pipeline for {len(numerical_features)} features.\")\n","else:\n","    print(\"No numerical features found. Skipping numerical pipeline.\")\n","\n","if categorical_features:\n","    categorical_pipeline = Pipeline([\n","        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)) # One-Hot Encoding\n","    ])\n","    transformers.append(('cat', categorical_pipeline, categorical_features))\n","    print(f\"Added categorical pipeline for {len(categorical_features)} features.\")\n","else:\n","     print(\"No categorical features found. Skipping categorical pipeline.\")\n","\n","# Create ColumnTransformer only if there are transformers\n","if transformers:\n","    preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n","\n","    # Fit and Transform data\n","    print(\"Fitting preprocessor on engineered training data...\")\n","    preprocessor.fit(X_train_fe)\n","\n","    print(\"Transforming engineered training data...\")\n","    X_train_processed = preprocessor.transform(X_train_fe)\n","\n","    print(\"Transforming engineered test data...\")\n","    X_test_processed = preprocessor.transform(X_test_fe)\n","\n","    print(f\"\\nProcessed train shape: {X_train_processed.shape}\")\n","    print(f\"Processed test shape: {X_test_processed.shape}\")\n","\n","    # Get feature names after preprocessing (optional, mainly for inspection)\n","    try:\n","         processed_feature_names = preprocessor.get_feature_names_out()\n","    except Exception as e:\n","         print(f\"Warning: Could not get feature names after preprocessing: {e}\")\n","         processed_feature_names = [f'feature_{i}' for i in range(X_train_processed.shape[1])]\n","\n","    # Convert processed numpy arrays back to DataFrame for easier inspection/use if needed\n","    # X_train_processed_df = pd.DataFrame(X_train_processed, columns=processed_feature_names)\n","    # X_test_processed_df = pd.DataFrame(X_test_processed, columns=processed_feature_names)\n","\n","    # print(\"\\nProcessed data (first 5 rows of train):\")\n","    # print(X_train_processed_df.head())\n","\n","else:\n","    print(\"\\nNo features identified for preprocessing. Processed data will be empty.\")\n","    # Handle case where no features are processed - create empty arrays/DataFrames\n","    X_train_processed = np.empty((X_train_fe.shape[0], 0))\n","    X_test_processed = np.empty((X_test_fe.shape[0], 0))\n","    # X_train_processed_df = pd.DataFrame(X_train_processed)\n","    # X_test_processed_df = pd.DataFrame(X_test_processed)\n","    processed_feature_names = []\n","    print(f\"Processed train shape: {X_train_processed.shape}\")\n","    print(f\"Processed test shape: {X_test_processed.shape}\")\n","\n","\n","print(\"\\nData preprocessing complete.\")\n","\n","# --- Data Splitting ---\n","print(\"\\nSplitting data into training and validation sets...\")\n","# Use the processed train data variable name from the analyst node\n","# Assuming the target 'y_train_original' is available from the initial data loading/prep\n","if X_train_processed.shape[0] > 0 and X_train_processed.shape[0] == y_train_original.shape[0]:\n","    X_train, X_val, y_train, y_val = train_test_split(X_train_processed, y_train_original, test_size=0.2, random_state=42)\n","    print(f\"Train shapes: {X_train.shape}, {y_train.shape}\")\n","    print(f\"Validation shapes: {X_val.shape}, {y_val.shape}\")\n","    data_split_success = True\n","else:\n","    print(f\"Error: Processed train data ('X_train_processed') or target ('y_train_original') not found or shapes mismatch. Skipping modeling.\")\n","    print(f\"'X_train_processed' shape: {X_train_processed.shape}\")\n","    print(f\"'y_train_original' shape: {y_train_original.shape}\")\n","    X_train, X_val, y_train, y_val = np.array([]).reshape(0,-1), np.array([]).reshape(0,-1), np.array([]), np.array([]) # Create empty arrays\n","    data_split_success = False\n","\n","\n","if data_split_success and X_train.shape[0] > 0:\n","    # --- Scaling for Neural Network ---\n","    # NN often performs better with features scaled to a 0-1 range and log-transformed target\n","    print(\"\\nScaling features for Neural Network using MinMaxScaler...\")\n","    scaler_nn = MinMaxScaler()\n","    X_train_scaled = scaler_nn.fit_transform(X_train)\n","    X_val_scaled = scaler_nn.transform(X_val)\n","    # Scale the actual test set using the processed test data variable name\n","    if X_test_processed.shape[0] > 0:\n","       X_test_scaled = scaler_nn.transform(X_test_processed)\n","       test_scaling_success = True\n","    else:\n","       print(f\"Error: Processed test data ('X_test_processed') not found for scaling.\")\n","       X_test_scaled = np.array([]).reshape(0,-1)\n","       test_scaling_success = False\n","\n","\n","    print(\"Log transforming target for NN...\")\n","    y_train_log = np.log1p(y_train)\n","    y_val_log = np.log1p(y_val)\n","\n","    # --- Model Training ---\n","    models = ['lgb', 'xgb', 'rf', 'cat', 'tf'] # Define models list for ensemble loop\n","\n","    # Train LightGBM (using split processed data - numpy arrays)\n","    print(\"\\nTraining LightGBM...\")\n","    lgb_model = lgb.LGBMRegressor(objective='regression', metric='rmse', n_estimators=2000, learning_rate=0.03, num_leaves=40, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=1, lambda_l1=0.1, lambda_l2=0.1, seed=42, n_jobs=-1, verbose=-1)\n","    lgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n","    lgb_val_pred = lgb_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    if X_test_processed.shape[0] > 0:\n","       lgb_test_pred = lgb_model.predict(X_test_processed)\n","    else:\n","       lgb_test_pred = np.zeros(X_test_processed.shape[0]) # Predict zeros if test data missing\n","       print(f\"Warning: 'X_test_processed' not available for LightGBM test prediction.\")\n","    print(\"LightGBM training complete.\")\n","\n","    # Train XGBoost (using split processed data - numpy arrays)\n","    print(\"\\nTraining XGBoost...\")\n","    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', n_estimators=2000, learning_rate=0.03, max_depth=7, subsample=0.8, colsample_bytree=0.8, lambda_l1=0.1, lambda_l2=0.1, seed=42, n_jobs=-1, tree_method='hist') # Use hist for faster training\n","    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n","    xgb_val_pred = xgb_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    if X_test_processed.shape[0] > 0:\n","       xgb_test_pred = xgb_model.predict(X_test_processed)\n","    else:\n","        xgb_test_pred = np.zeros(X_test_processed.shape[0])\n","        print(f\"Warning: 'X_test_processed' not available for XGBoost test prediction.\")\n","    print(\"XGBoost training complete.\")\n","\n","    # Train RandomForestRegressor (using split processed data - numpy arrays)\n","    print(\"\\nTraining RandomForestRegressor...\")\n","    rf_model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n","    rf_model.fit(X_train, y_train)\n","    rf_val_pred = rf_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    if X_test_processed.shape[0] > 0:\n","        rf_test_pred = rf_model.predict(X_test_processed)\n","    else:\n","        rf_test_pred = np.zeros(X_test_processed.shape[0])\n","        print(f\"Warning: 'X_test_processed' not available for RandomForest test prediction.\")\n","    print(\"RandomForestRegressor training complete.\")\n","\n","    # Train CatBoostRegressor (using split processed data - numpy arrays)\n","    print(\"\\nTraining CatBoostRegressor...\")\n","    cat_model = CatBoostRegressor(iterations=2000, learning_rate=0.03, depth=7, random_seed=42, verbose=0, early_stopping_rounds=100, l2_leaf_reg=3, loss_function='RMSE')\n","    cat_model.fit(X_train, y_train, eval_set=(X_val, y_val))\n","    cat_val_pred = cat_model.predict(X_val)\n","    # Predict on the processed test data using the variable name\n","    if X_test_processed.shape[0] > 0:\n","        cat_test_pred = cat_model.predict(X_test_processed)\n","    else:\n","         cat_test_pred = np.zeros(X_test_processed.shape[0])\n","         print(f\"Warning: 'X_test_processed' not available for CatBoost test prediction.\")\n","    print(\"CatBoostRegressor training complete.\")\n","\n","\n","    # Build and train TensorFlow model (using scaled data)\n","    if tf and test_scaling_success and X_train_scaled.shape[0] > 0 and X_train_scaled.shape[1] > 0: # Only attempt TF if TF is available, test data was scaled, and train data is not empty\n","        print(\"\\nBuilding and training TensorFlow model...\")\n","        tf_model = build_swish_mlp(input_shape=(X_train_scaled.shape[1],))\n","        if tf_model: # Check if model was built successfully\n","            early_stopping_tf = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=15, restore_best_weights=True, mode='min', verbose=0)\n","\n","            print(\"  Training TensorFlow model...\")\n","            history = tf_model.fit(X_train_scaled, y_train_log,\n","                                   epochs=200,\n","                                   batch_size=64,\n","                                   verbose=0,\n","                                   validation_data=(X_val_scaled, y_val_log),\n","                                   callbacks=[early_stopping_tf])\n","            print(\"  TensorFlow model training complete.\")\n","\n","            print(\"Making TensorFlow predictions...\")\n","            tfy_test_pred_log = tf_model.predict(X_test_scaled).flatten()\n","            tf_test_pred = np.expm1(tfy_test_pred_log)\n","            # Clip to original target range (need 'y_train_original' min/max)\n","            tf_test_pred = np.clip(tf_test_pred, y_train_original.min(), y_train_original.max())\n","\n","\n","            tfy_val_pred_log = tf_model.predict(X_val_scaled).flatten()\n","            tf_val_pred = np.expm1(tfy_val_pred_log)\n","            tf_val_pred = np.clip(tf_val_pred, y_train_original.min(), y_train_original.max())\n","            print(\"TensorFlow predictions made.\")\n","        else: # TF model build failed\n","            tf_test_pred = np.zeros(X_test_scaled.shape[0])\n","            tf_val_pred = np.zeros(X_val_scaled.shape[0])\n","            print(\"TensorFlow model could not be built. Skipping TF predictions.\")\n","\n","    else: # TF not available or data issues\n","        print(\"\\nSkipping TensorFlow model training and prediction.\")\n","        # Ensure prediction variables exist even if TF is skipped\n","        test_data_shape = X_test_processed.shape[0] if X_test_processed.shape[0] > 0 else test.shape[0] # Fallback shape if processed is empty\n","        tf_test_pred = np.zeros(test_data_shape)\n","        tf_val_pred = np.zeros(X_val.shape[0]) # Use X_val shape for validation predictions\n","        print(\"TensorFlow prediction variables initialized to zeros.\")\n","\n","\n","    # --- Validation Scores ---\n","    print(f\"\\n--- Validation RMSLE Scores ---\")\n","    if y_val.shape[0] > 0:\n","        # Check if prediction variables exist and have correct shape before computing scores\n","        if 'lgb_val_pred' in locals() and lgb_val_pred.shape == y_val.shape:\n","           print(f\"LightGBM Validation RMSLE: {rmsle(y_val, lgb_val_pred):.5f}\")\n","        else:\n","           print(\"LightGBM Validation RMSLE: N/A (predictions missing or shape mismatch)\")\n","\n","        if 'xgb_val_pred' in locals() and xgb_val_pred.shape == y_val.shape:\n","           print(f\"XGBoost Validation RMSLE: {rmsle(y_val, xgb_val_pred):.5f}\")\n","        else:\n","            print(\"XGBoost Validation RMSLE: N/A (predictions missing or shape mismatch)\")\n","\n","        if 'rf_val_pred' in locals() and rf_val_pred.shape == y_val.shape:\n","           print(f\"RandomForest Validation RMSLE: {rmsle(y_val, rf_val_pred):.5f}\")\n","        else:\n","            print(\"RandomForest Validation RMSLE: N/A (predictions missing or shape mismatch)\")\n","\n","        if 'cat_val_pred' in locals() and cat_val_pred.shape == y_val.shape:\n","            print(f\"CatBoost Validation RMSLE: {rmsle(y_val, cat_val_pred):.5f}\")\n","        else:\n","             print(\"CatBoost Validation RMSLE: N/A (predictions missing or shape mismatch)\")\n","\n","        if 'tf_val_pred' in locals() and tf_val_pred.shape == y_val.shape:\n","           print(f\"TensorFlow Validation RMSLE: {rmsle(y_val, tf_val_pred):.5f}\")\n","        else:\n","            print(\"TensorFlow Validation RMSLE: N/A (predictions missing or shape mismatch)\")\n","\n","    else:\n","        print(\"No validation data available to compute scores.\")\n","    print(f\"------------------------------\")\n","\n","    # --- Ensemble Prediction ---\n","    print(\"\\nCreating ensemble prediction...\")\n","\n","    # Ensure all test prediction variables exist and have the same expected shape\n","    # The expected shape is the number of rows in the processed test data\n","    expected_shape = X_test_processed.shape[0] if X_test_processed.shape[0] > 0 else test.shape[0] # Fallback shape\n","\n","    # List of models and their test predictions\n","    test_preds = {\n","        'lgb': lgb_test_pred if 'lgb_test_pred' in locals() and lgb_test_pred.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'xgb': xgb_test_pred if 'xgb_test_pred' in locals() and xgb_test_pred.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'rf':  rf_test_pred  if 'rf_test_pred'  in locals() and rf_test_pred.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'cat': cat_test_pred if 'cat_test_pred' in locals() and cat_test_pred.shape[0] == expected_shape else np.zeros(expected_shape),\n","        'tf':  tf_test_pred  if 'tf_test_pred'  in locals() and tf_test_pred.shape[0] == expected_shape else np.zeros(expected_shape),\n","    }\n","    val_preds = {\n","        'lgb': lgb_val_pred if 'lgb_val_pred' in locals() and lgb_val_pred.shape == y_val.shape else np.zeros(y_val.shape[0]),\n","        'xgb': xgb_val_pred if 'xgb_val_pred' in locals() and xgb_val_pred.shape == y_val.shape else np.zeros(y_val.shape[0]),\n","        'rf':  rf_val_pred  if 'rf_val_pred'  in locals() and rf_val_pred.shape == y_val.shape else np.zeros(y_val.shape[0]),\n","        'cat': cat_val_pred if 'cat_val_pred' in locals() and cat_val_pred.shape == y_val.shape else np.zeros(y_val.shape[0]),\n","        'tf':  tf_val_pred  if 'tf_val_pred'  in locals() and tf_val_pred.shape == y_val.shape else np.zeros(y_val.shape[0]),\n","    }\n","\n","    # Calculate weights based on validation performance (using RMSLE)\n","    # Avoid division by zero or using infinite RMSLE from errors\n","    val_rmsles = {\n","        'lgb': rmsle(y_val, val_preds['lgb']) if y_val.shape[0] > 0 and np.sum(np.abs(val_preds['lgb'])) > 0 else float('inf'),\n","        'xgb': rmsle(y_val, val_preds['xgb']) if y_val.shape[0] > 0 and np.sum(np.abs(val_preds['xgb'])) > 0 else float('inf'),\n","        'rf':  rmsle(y_val, val_preds['rf'])  if y_val.shape[0] > 0 and np.sum(np.abs(val_preds['rf'])) > 0 else float('inf'),\n","        'cat': rmsle(y_val, val_preds['cat']) if y_val.shape[0] > 0 and np.sum(np.abs(val_preds['cat'])) > 0 else float('inf'),\n","        'tf':  rmsle(y_val, val_preds['tf'])  if y_val.shape[0] > 0 and np.sum(np.abs(val_preds['tf'])) > 0 and tf is not None else float('inf'), # Only include TF if available\n","    }\n","\n","    # Calculate inverse RMSLE (higher for better models), handle inf/zero\n","    inv_rmsles = {k: (1.0 / v if v != 0 and v != float('inf') else 0) for k, v in val_rmsles.items()}\n","\n","    # Normalize to get weights\n","    total_inv = sum(inv_rmsles.values())\n","    if total_inv > 1e-9: # Avoid division by near zero\n","        weights = {k: v / total_inv for k, v in inv_rmsles.items()}\n","    else: # If all models failed or had infinite RMSLE, use equal weights or zero weights\n","        print(\"Warning: Cannot calculate meaningful ensemble weights (all models failed or infinite RMSLE). Using equal weights as fallback.\")\n","        valid_models = [m for m, inv in inv_rmsles.items() if inv > 0]\n","        if valid_models:\n","             equal_weight = 1.0 / len(valid_models)\n","             weights = {m: equal_weight for m in models} # Initialize all weights to 0 first\n","             for m in valid_models: # Assign equal weight to valid models\n","                 weights[m] = equal_weight\n","        else:\n","             print(\"Warning: No models trained successfully. Ensemble prediction will be zero.\")\n","             weights = {m: 0 for m in models}\n","\n","\n","    print(f\"Model weights: {weights}\")\n","\n","    # Calculate weighted ensemble predictions\n","    ensemble_predictions = np.zeros(expected_shape)\n","    for model in models:\n","        ensemble_predictions += weights[model] * test_preds[model]\n","\n","\n","    # Ensure predictions are non-negative (calories can't be negative)\n","    ensemble_predictions = np.maximum(0, ensemble_predictions)\n","\n","    # --- Create Submission ---\n","    # Assume 'submission' DataFrame with an 'id' column is available from initial loading\n","    print(\"\\nPreparing submission file...\")\n","    if 'submission' in locals() and 'id' in submission.columns and submission.shape[0] == expected_shape:\n","        submission['Calories'] = ensemble_predictions\n","        submission.to_csv('submission.csv', index=False)\n","        print(\"Submission file created: submission.csv\")\n","    else:\n","        print(\"Error: Submission DataFrame not found, invalid, or shape mismatch. Cannot create submission file.\")\n","        print(\"Details:\")\n","        if 'submission' not in locals(): print(\"'submission' variable not found.\")\n","        elif 'id' not in submission.columns: print(\"'submission' DataFrame missing 'id' column.\")\n","        elif submission.shape[0] != expected_shape: print(f\"Submission shape ({submission.shape[0]}) does not match test prediction shape ({expected_shape}).\")\n","\n","\n","    print(\"\\nProcess complete!\")\n","\n","else:\n","    print(\"\\nSkipping model training, validation, and submission due to data split failure or empty training data.\")\n","    # Ensure ensemble_predictions variable exists for potential return, even if zero\n","    expected_shape = X_test_processed.shape[0] if X_test_processed.shape[0] > 0 else test.shape[0] # Fallback shape\n","    ensemble_predictions = np.zeros(expected_shape)\n","    # Create a dummy submission file if possible\n","    if 'submission' in locals() and 'id' in submission.columns and submission.shape[0] == expected_shape:\n","         submission['Calories'] = ensemble_predictions\n","         submission.to_csv('submission.csv', index=False)\n","         print(\"Created dummy submission.csv with zero predictions due to processing/modeling failure.\")\n","    else:\n","         print(\"Could not create submission file due to missing data or shape mismatch.\")"]},{"cell_type":"code","execution_count":null,"id":"8d746f22","metadata":{"papermill":{"duration":0.03172,"end_time":"2025-10-22T22:57:06.335759","exception":false,"start_time":"2025-10-22T22:57:06.304039","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":11893428,"sourceId":91716,"sourceType":"competition"}],"dockerImageVersionId":31012,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"papermill":{"default_parameters":{},"duration":2027.529004,"end_time":"2025-10-22T22:57:10.497599","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-22T22:23:22.968595","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}
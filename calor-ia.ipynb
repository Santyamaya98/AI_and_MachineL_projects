{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Remove conflicting packages from the Kaggle base environment.Â¶\n!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:30:20.564428Z","iopub.execute_input":"2025-05-11T23:30:20.564799Z","iopub.status.idle":"2025-05-11T23:30:52.866152Z","shell.execute_reply.started":"2025-05-11T23:30:20.564760Z","shell.execute_reply":"2025-05-11T23:30:52.864867Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport operator\nimport re # Import regex for parsing\nimport json\nimport io\nimport sys\n\n\n\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage # Import AIMessage\n# from langchain_openai import ChatOpenAI # Replace with your desired LLM provider\nfrom langgraph.graph import StateGraph, END\n# Kaggle and Google AI\nfrom kaggle_secrets import UserSecretsClient\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom google import genai\nfrom google.genai import types\nfrom google.api_core import retry\n\n# IPython Display\nfrom IPython.display import Markdown, Image, display\n\n# PDF Processing\nimport pypdf\n\n# ChromaDB\n#import chromadb\n#from chromadb import Documents, EmbeddingFunction, Embeddings\n\n\nfrom typing import TypedDict, Annotated, Optional, Literal, List, Dict, Any\nfrom typing_extensions import TypedDict # \nfrom langchain_core.messages import BaseMessage \nfrom contextlib import redirect_stdout\n\n# Langchain and Langgraph\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage # Ensure these are imported\n\n\n# Pretty Print\nfrom pprint import pprint\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GOOGLE_API_KEY\")\ntrain, test, submission = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:31:22.535918Z","iopub.execute_input":"2025-05-11T23:31:22.536266Z","iopub.status.idle":"2025-05-11T23:31:28.103545Z","shell.execute_reply.started":"2025-05-11T23:31:22.536241Z","shell.execute_reply":"2025-05-11T23:31:28.102544Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n  warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **ğŸ¯ğŸš€We will defined langgraph who will coordinate our data analyst and data scienctist nodesğŸ¯ğŸš€**\n# **Lets build a ğŸ¤– herarchical ğŸ¤– architecture**\n# 1. supervisor\n# 2. data scientist\n# 3. data analyst\n# 4. outputs to Human interpreter ","metadata":{}},{"cell_type":"code","source":"# Using simple dicts for messages initially for clarity, but BaseMessage[] is better practice# Example: messages: Annotated[list[BaseMessage], add_messages]\n# Using simple dicts for messages initially for clarity, but BaseMessage[] is better practice# Example: messages: Annotated[list[BaseMessage], add_messages]\n# --- State Definition ---\nclass GraphState(TypedDict):\n    \"\"\"\n    ğŸ—‚ï¸ Central state definition for the CALOR-IA multi-agent workflow.\n    Reflects using DataFrame variable names instead of file paths.\n    \"\"\"\n    messages: Annotated[List[BaseMessage], operator.add]\n    supervisor_tasks: Optional[List[str]]\n    current_task_description: Optional[str]\n    analyst_output: Optional[Dict[str, Any]]\n    analyst_llm_output: Optional[Dict[str, Any]]\n    scientist_llm_output: Optional[Dict[str, Any]]\n    scientist_output: Optional[Dict[str, Any]]\n    code_final: Optional[Dict[str, Any]]\n    processed_train_name: Optional[str]     \n    processed_test_name: Optional[str]\n    error: Optional[str]\n    next_agent: Optional[Literal[\"DataAnalyst\",  \"DataScientist\", \"DataAnalystIA\",\"DataScientistIA\", \"Supervisor\", \"code_compiler_AI\",\"HumanInterpreter\", \"__end__\"]] \n    interpreter_finished : bool\n    final_answer_generated: bool # Added for clarity\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:31:28.105047Z","iopub.execute_input":"2025-05-11T23:31:28.105509Z","iopub.status.idle":"2025-05-11T23:31:28.112836Z","shell.execute_reply.started":"2025-05-11T23:31:28.105481Z","shell.execute_reply":"2025-05-11T23:31:28.111707Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nCALOR_IA_DATA_ANALYST_SYSINT = (\n    \"system\",\n    \"\"\"\n    ğŸ¤– You are CALOR-IA Data Analyst Bot.\n\n    ğŸ¯ Mission\n    1ï¸âƒ£ Execute the data analysis task provided in the `current_task_description` from the Supervisor.\n    2ï¸âƒ£ Write clean, runnable Python ğŸ code for data loading, cleaning, exploration, and visualization (using pandas, matplotlib, seaborn, etc.).\n    3ï¸âƒ£ Analyze data (potentially loaded from `intermediate_data_path` if provided) and extract valuable insights.\n    4ï¸âƒ£ Prepare data for the Data Scientist if requested (e.g., creating features, splitting data) and potentially save it, updating `intermediate_data_path`.\n    5ï¸âƒ£ Place your results (code, summary of findings, paths to saved plots or data) into the `analyst_output` dictionary in the graph state.\n    6ï¸âƒ£ Collaborate with CALOR_IA_DATA_SCIENTIST_SYSINT via the Supervisor by providing necessary data artifacts and insights.\n\n    ğŸ“ Style & Rules\n    - Focus solely on the task in `current_task_description`.\n    - Separate numerical and categorical data for better handling \n    - Output results clearly structured within the `analyst_output` dictionary (e.g., `{\"code\": \"...\", \"summary\": \"...\", \"plot_path\": \"/path/to/plot.png\", \"data_preview\": \"...\"}`).\n    - Generate runnable Python code within markdown code blocks (```python ... ```).\n    - Explain your code and findings concisely.\n    - If you save data or plots, mention the path clearly in your output summary.\n    - Use matplotlib/seaborn for plots.\n    - Stick to data analysis/preparation; defer modeling to the Scientist.\n    - Return the task to supervisor in json format for better understanding and the python code used to develop the task\n    - Return Just the python code\n    \n    Letâ€™s crunch some data! âœ¨\n    \"\"\"\n)\n\nCALOR_IA_DATA_SCIENTIST_SYSINT = (\n    \"system\",\n    \"\"\"\n    ğŸ¤– You are CALOR-IA Data Scientist Bot.\n\n    ğŸ¯ Mission\n    1ï¸âƒ£ Execute the machine learning task provided in the `current_task_description` from the Supervisor.\n    2ï¸âƒ£ Use data provided (potentially loaded from `intermediate_data_path` prepared by the Analyst) to build, train, and evaluate ML models (using scikit-learn, TensorFlow, PyTorch, etc.).\n    3ï¸âƒ£ Perform feature engineering if required and not already done by the Analyst.\n    4ï¸âƒ£ Generate predictions on test data as requested.\n    5ï¸âƒ£ Place your results (model summary, performance metrics, paths to saved models or predictions) into the `scientist_output` dictionary in the graph state.\n    6ï¸âƒ£ Collaborate with CALOR_IA_DATA_ANALYST_SYSINT via the Supervisor by requesting specific data views or providing model insights.\n\n    ğŸ“ Style & Rules\n    - Focus solely on the task in `current_task_description`.\n    - Output results clearly structured within the `scientist_output` dictionary (e.g., `{\"model_description\": \"...\", \"metrics\": {\"accuracy\": 0.95, ...}, \"predictions_path\": \"/path/to/preds.csv\", \"code\": \"...\"}`).\n    - Generate runnable Python code for model definition, training, and prediction within markdown code blocks (```python ... ```).\n    - Explain your model choices, training procedures, and evaluation metrics step-by-step.\n    - If you save models or predictions, mention the path clearly in your output summary.\n    - Aim for models that generalize well.\n    - Stick to machine learning tasks; defer data prep/exploration to the Analyst unless specified.\n    - Return the task to supervisor in json format for better understanding and the python code used to develop the task\n    - All models must be scored in RMSLE=n1â€‹i=1âˆ‘nâ€‹(log(1+y^â€‹iâ€‹)âˆ’log(1+yiâ€‹))2\n    - You will run the models on test data\n    Letâ€™s build some high-performing models! âœ¨\n    \"\"\"\n)\n\nCALOR_IA_CODE_COMPILER_SYSINT = (\n    \"system\",\n    f\"\"\"\n    ğŸ¤– This function implements the CALOR-IA Code Compiler Bot, which extracts and fixes code fragments from the workflow output. The bot:\n\n    ğŸ¯ Mission:\n    use following inputs :\n    train, test, submission = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv'),pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')\n    with the purpose of \n    1ï¸âƒ£. Searches for code blocks enclosed in triple backticks\n    2ï¸âƒ£. Falls back to looking for common code patterns if no blocks are found\n    3ï¸âƒ£. Generates a complete, fixed script with:\n       - Proper data loading\n       - Correct variable references\n       - Complete model training code\n       - Prediction generation\n       - Feature importance visualization\n       - Output file generation\n    \n    ğŸ“ To use this bot in your workflow:\n    \n    1. Import the function\n    2. Pass it the supervisor output text\n    3. Use the returned string as your complete, ready-to-run Python script\n\nThe script includes all necessary error handling and produces a standardized output format.\n\"\"\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:31:29.462203Z","iopub.execute_input":"2025-05-11T23:31:29.462536Z","iopub.status.idle":"2025-05-11T23:31:29.470474Z","shell.execute_reply.started":"2025-05-11T23:31:29.462515Z","shell.execute_reply":"2025-05-11T23:31:29.469483Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def supervisor_node(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Central decision-making node. Routes tasks to appropriate agents\n    or ends the workflow based on the current state.\n    Now routes to DataAnalyst_AI AFTER HumanInterpreter.\n    \"\"\"\n    print(\"\\n--- SUPERVISOR ---\")\n    messages = state.get(\"messages\", [])\n    last_message = messages[-1] if messages else None\n    print(f\"Supervisor reviewing state. Last message type: {type(last_message).__name__ if last_message else 'None'}\")\n    if hasattr(last_message, 'content'):\n         content_display = str(last_message.content)\n         print(f\"Supervisor received content: {content_display[:200]}{'...' if len(content_display) > 200 else ''}\")\n\n    # --- Retrieve outputs from potential sources ---\n    analyst_orig_output = state.get('analyst_output')\n    scientist_output_data = state.get('scientist_output')\n    analyst_llm_output = state.get('analyst_llm_output')\n    scientist_llm_output = state.get('scientist_llm_output')\n    code = state.get('code_final')\n    interpreter_finished = state.get('interpreter_finished', False)\n    final_answer_generated = state.get('final_answer_generated', False)\n    initial_train_name = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv')\n    initial_test_name = pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\n    tasks = state.get('supervisor_tasks', [])\n    error_flag = state.get('error')\n\n    print(f\"Original Analyst output available: {analyst_orig_output is not None}\")\n    print(f\"Scientist output available: {scientist_output_data is not None}\")\n    print(f\"Interpreter finished signal: {interpreter_finished}\")\n    print(f\"Final answer generated flag: {final_answer_generated}\")\n    \n    next_agent = None\n    task = None\n    new_message_content = None\n    updated_state_for_return = {}\n\n    # --- Decision Logic (Ordered by Priority) ---\n    \n    # 1. Check for final answer signal - highest priority\n    if final_answer_generated:\n        print(\"Supervisor: Final answer signal received. Ending workflow.\")\n        print(code)\n        next_agent = \"__end__\"\n        # No new message needed\n    \n    # 2. Check if Scientist AI has completed review\n    elif scientist_llm_output is not None and not final_answer_generated:\n        print(\"Supervisor: Scientist AI review complete. Final answer should be ready.\")\n        # Force end the workflow since we've gone through all steps\n        next_agent = \"code_compiler_AI\"\n        new_message_content = \"Workflow complete. Final script and reviews have been generated.\"\n        updated_state_for_return['final_answer_generated'] = True  # Force set this flag\n    \n    # 3. Check if Analyst AI has finished its review - Next step is Scientist AI\n    elif analyst_llm_output is not None and scientist_llm_output is None:\n        print(\"Supervisor: Analyst AI review complete. Routing to Scientist AI for review.\")\n        next_agent = \"DataScientist_AI\"\n        new_message_content = \"Analyst AI review complete. Routing compiled script to Data Scientist AI for review.\"\n    \n    # 4. Check if Interpreter has finished - Start AI review chain\n    elif interpreter_finished and analyst_llm_output is None:\n        # Reset the flag to prevent re-triggering\n        updated_state_for_return['interpreter_finished'] = False\n        print(\"Supervisor: Interpreter finished compiling script. Routing to Data Analyst AI for review.\")\n        next_agent = \"DataAnalyst_AI\"\n        new_message_content = \"Interpreter step complete. Routing compiled script for AI review.\"\n    \n    # 5. Check for error condition\n    elif error_flag is not None:\n        print(f\"Supervisor: Error detected in state: {error_flag}. Routing to Human Interpreter for review.\")\n        next_agent = \"HumanInterpreter\"\n    \n    # 6. Check for Scientist output data - Route to Interpreter\n    elif scientist_output_data is not None and not interpreter_finished:\n        print(\"Supervisor: Scientist code generation complete. Routing to HumanInterpreter for compilation.\")\n        next_agent = \"HumanInterpreter\"\n        new_message_content = \"Scientist code generation complete. Compiling final script.\"\n        updated_state_for_return[\"processed_train_name\"] = state.get(\"processed_train_name\")\n        updated_state_for_return[\"processed_test_name\"] = state.get(\"processed_test_name\")\n        print(f\"Supervisor propagating names from state for next step: train='{updated_state_for_return.get('processed_train_name')}', test='{updated_state_for_return.get('processed_test_name')}'\")\n    \n    # 7. Check for original DataAnalyst output - Route to DataScientist\n    elif analyst_orig_output is not None and scientist_output_data is None:\n        print(\"Supervisor: Original Data Analyst successfully generated code and data names. Routing to Data Scientist.\")\n        updated_state_for_return[\"processed_train_name\"] = analyst_orig_output.get('processed_train_name')\n        updated_state_for_return[\"processed_test_name\"] = analyst_orig_output.get('processed_test_name')\n        print(f\"Supervisor propagating names from original analyst output to Scientist: train='{updated_state_for_return.get('processed_train_name')}', test='{updated_state_for_return.get('processed_test_name')}'\")\n        task = \"Build and evaluate LightGBM and XGBoost models using the processed data.\"\n        next_agent = \"DataScientist\"\n    \n    # 8. Initial state handling\n    elif 'initial_input_message' in locals() or 'initial_input_message' in globals():\n        print(\"Supervisor: Initial human message received. Assigning initial task to Original Data Analyst.\")\n        next_agent = \"DataAnalyst\"\n        new_message_content = \"Okay, starting the data analysis workflow with the Data Analyst.\"\n        task = \"Perform initial EDA and preprocessing on train/test data.\"\n    \n    # 9. Unexpected state - End workflow\n    else:\n        print(\"Supervisor: Unexpected state - no initial message or prior agent output. Ending.\")\n        error_flag = \"Workflow ended in an unexpected state.\"\n        next_agent = \"__end__\"\n        new_message_content = \"Workflow terminated due to an unexpected state.\"\n\n    # --- Prepare Return Dictionary ---\n    updated_messages = list(messages)\n    if new_message_content is not None:\n        if not updated_messages or (hasattr(updated_messages[-1], 'content') and updated_messages[-1].content != new_message_content):\n            updated_messages.append(AIMessage(content=str(new_message_content)))\n\n    return_dict = {\n        \"messages\": updated_messages,\n        \"supervisor_tasks\": tasks,\n        \"current_task_description\": task,\n        \"next_agent\": next_agent,\n        \"error\": error_flag,\n        \"final_answer_generated\": updated_state_for_return.get('final_answer_generated', final_answer_generated),\n        \"interpreter_finished\": updated_state_for_return.get('interpreter_finished', interpreter_finished),\n        \"initial_train_name\": initial_train_name,\n        \"initial_test_name\": initial_test_name,\n        **updated_state_for_return\n    }\n\n    print(f\"DEBUG: Supervisor RETURNING next_agent: '{return_dict['next_agent']}' (Type: {type(return_dict['next_agent'])})\")\n    print(f\"DEBUG: Supervisor RETURNING final_answer_generated: {return_dict.get('final_answer_generated')}\")\n    print(f\"DEBUG: Supervisor RETURNING interpreter_finished: {return_dict.get('interpreter_finished')}\")\n\n    return return_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:36:10.010131Z","iopub.execute_input":"2025-05-11T23:36:10.010821Z","iopub.status.idle":"2025-05-11T23:36:10.026141Z","shell.execute_reply.started":"2025-05-11T23:36:10.010791Z","shell.execute_reply":"2025-05-11T23:36:10.025177Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def data_analyst_node(state: GraphState) -> Dict[str, Any]:\n    \"\"\"Data Analyst Node - Updated for DataFrame variable names.\"\"\"\n    print(\"\\n--- DATA ANALYST ---\")\n    task = state.get('current_task_description')\n    # Assume initial DFs are named 'train_df' and 'test_df' for clarity\n    # Adjust these names if your actual initial variables are different (e.g., 'train', 'test')\n    initial_train_name = state.get('initial_train_name')# <-- Adjust if your initial var is different\n    initial_test_name =  state.get('initial_test_name') # <-- Adjust if your initial var is different\n\n    if not task:\n        error_output = {\"error\": \"Data Analyst received no task.\", \"next_agent\": \"Supervisor\"}\n        print(\"Returning error output:\", error_output)\n        return error_output\n\n    # Simulate the names of the DataFrames that the processing code will create\n    processed_train_name = f\"processed_{initial_train_name}\" \n    processed_test_name = f\"processed_{initial_test_name}\"   \n\n    print(f\"Analyst expects input DataFrame variable: '{initial_train_name}'\")\n    print(f\"Analyst expects input DataFrame variable: '{initial_test_name}'\")\n    print(f\"Analyst will simulate creating output DataFrames: '{processed_train_name}' and '{processed_test_name}'\")\n\n    # --- Simulate Code (Make sure variable names match below) ---\n    simulated_code = f\"\"\"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\n# Ensure correct variable names are used in the code\ntrain = {initial_train_name}\ntest = {initial_test_name}  # Use the initial test name here\n\n# === Separate target variable from train ===\ntarget = train['Calories']\ntrain = train.drop(columns='Calories')\n\n# 1) Identify numerical vs. categorical columns\nnum_cols = train.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(\"Numerical columns:\", num_cols)\nprint(\"Categorical columns:\", cat_cols)\n\n# 2) Build preprocessing pipelines\nnum_pipeline = Pipeline([\n    ('scaler', MinMaxScaler())\n])\ncat_pipeline = Pipeline([\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', num_pipeline, num_cols),\n    ('cat', cat_pipeline, cat_cols)\n])\n\n# 3) KNN imputer for any remaining NaNs\nimputer = KNNImputer()\n\n# 4) Fit on train, transform both train & test\ntrain_array = preprocessor.fit_transform(train)\ntrain_array = imputer.fit_transform(train_array)\n\ntest_array = preprocessor.transform(test)  # Use 'test' variable name here\ntest_array = imputer.transform(test_array)\n\n# 5) Recover feature names so we can build nice DataFrames\nnum_feats = preprocessor.named_transformers_['num'] \\\n               .named_steps['scaler'].feature_names_in_\ncat_feats = preprocessor.named_transformers_['cat'] \\\n               .named_steps['onehot'].get_feature_names_out(cat_cols)\nall_feats = list(num_feats) + list(cat_feats)\n\n# Create the processed DataFrames using the intended variable names\n{processed_train_name} = pd.DataFrame(train_array, columns=all_feats)\n{processed_train_name}['Calories'] = target \n{processed_test_name}  = pd.DataFrame(test_array,  columns=all_feats)\n\nprint(f\"Processed train shape: {{{processed_train_name}.shape}}\")  # Print using the variable name\nprint(f\"Processed test shape:  {{{processed_test_name}.shape}}\")   # Print using the variable name\n    \"\"\"\n\n    return{\n        \"analyst_output\":{\n            \"code\": simulated_code,\n            # Keep nested for completeness of analyst output details\n            \"processed_train_name\": processed_train_name,\n            \"processed_test_name\": processed_test_name,\n        },\n         \"current_task_description\": None, # Task is complete for Analyst\n    }\n\ndef process_update(update: Dict[str, Any]) -> None:\n    if not isinstance(update, dict):\n        print(\"Update is not a dictionary.\")\n        return\n\n    if 'messages' in update:\n        messages = update['messages']\n        print(\"Messages from update:\")\n        for message in messages:\n            print(message)\n\n    if 'code' in update:\n        print(\"Code snippet provided in update:\")\n        print(update['code'])\n\n    if 'summary' in update:\n        print(\"Summary from update:\")\n        print(update['summary'])\n\n    if 'error' in update:\n        print(\"Error encountered:\")\n        print(update['error'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:36:11.222992Z","iopub.execute_input":"2025-05-11T23:36:11.223324Z","iopub.status.idle":"2025-05-11T23:36:11.233381Z","shell.execute_reply.started":"2025-05-11T23:36:11.223277Z","shell.execute_reply":"2025-05-11T23:36:11.232391Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import os\nfrom typing import Dict, Any\n\n# Assuming GraphState class is defined elsewhere\n# Assuming necessary library imports (like pandas, numpy etc.) are at the top level of your script\n\ndef data_scientist_node(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Data Scientist Node - Generates modeling code (LightGBM, XGBoost)\n    using processed data names provided by the DataAnalyst node via the state.\n    This version DOES NOT execute the generated code.\n    \"\"\"\n    print(\"\\n--- DATA SCIENTIST (Code Generation Only) ---\")\n    task = state.get('current_task_description')\n    if not task:\n        print(\"Scientist: No task received.\")\n        # Return an error structure within the expected output key\n        return {\n            \"scientist_output\": {\"error\": \"Data Scientist received no task.\"},\n            \"next_agent\": \"Supervisor\"\n            }\n\n    # --- Get processed DataFrame names from the analyst's output in state ---\n    # The analyst node puts these names in 'analyst_output' dictionary\n    analyst_result = state.get('analyst_output')\n    if not analyst_result or not isinstance(analyst_result, dict):\n        error_msg = \"Scientist Error: Analyst output not found or invalid in state.\"\n        print(error_msg)\n        return {\n            \"scientist_output\": {\"error\": error_msg},\n             \"next_agent\": \"Supervisor\"\n             }\n\n    # Get the variable names *as they will be created* by the analyst's code\n    processed_train_name = analyst_result.get('processed_train_name')\n    processed_test_name  = analyst_result.get('processed_test_name')\n    # Assume the analyst's code also creates a variable named 'target'\n    # This name is hardcoded based on the analyst's provided simulated code structur\n\n    if not processed_train_name or not processed_test_name:\n        error_msg = \"Scientist Error: Processed DataFrame names not found within analyst output in state.\"\n        print(error_msg)\n        return {\n            \"scientist_output\": {\"error\": error_msg},\n             \"next_agent\": \"Supervisor\"\n             }\n\n    print(f\"Scientist generating code for task: {task}\")\n    print(f\"Will use processed train DataFrame variable: '{processed_train_name}'\")\n    print(f\"Will use processed test DataFrame variable : '{processed_test_name}'\")\n\n\n    # --- Generate Code String ---\n    # Use single braces {} ONLY for variables substituted NOW ({processed_train_name}, etc.)\n    # Use double braces {{}} for f-strings intended for LATER execution within the generated code.\n    generated_code =f'''\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb\nimport warnings\nimport os # Import os to create directories if needed\n\n# --- Suppress Warnings ---\nwarnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nprint(\"\\\\n--- Data Scientist Code Execution Started ---\")\n\n# --- Configuration ---\n# These names are substituted when the string is created in the scientist node\nPROCESSED_TRAIN_NAME = '{processed_train_name}'\nPROCESSED_TEST_NAME  = '{processed_test_name}' # The variable name holding the target series (created by analyst code)\n\nOUTPUT_DIR = 'model_outputs'\n\ntrain = PROCESSED_TRAIN_NAME\ntest = PROCESSED_TEST_NAME\n# --- Setup ---\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Ensuring output directory exists: {{OUTPUT_DIR}}\") # Inner f-string: Use {{}}\n\n# --- Data Loading ---\nprint(f\"Loading data: train='{{PROCESSED_TRAIN_NAME}}', test='{{PROCESSED_TEST_NAME}}', target_var='{{TARGET_VARIABLE_NAME}}'\")\ntry:\n    # Attempt to load variables from the global scope where exec runs\n    # These variables are assumed to have been created by the Data Analyst's code\n    train = globals()[PROCESSED_TRAIN_NAME]        # Processed features for training\n    test = globals()[PROCESSED_TEST_NAME]         # Processed features for testing\n    target   = globals()[TARGET_VARIABLE_NAME]        # Target variable (Series)\n\n    print(f\"Loaded train shape: {{train.shape}}\")\n    print(f\"Loaded test shape: {{test.shape}}\")\n    print(f\"Loaded target shape: {{target.shape}}\")\n\n    # Basic sanity check: ensure number of rows match between features and target\n    if train.shape[0] != target.shape[0]:\n         print(f\"[ERROR] Row count mismatch between training features ({{train.shape[0]}}) and target ({{target.shape[0]}}).\")\n         exit(1)\n\nexcept KeyError as e:\n    print(f\"[ERROR] Required variable '{{e.args[0]}}' not found in the execution environment.\")\n    print(\"Please ensure the Data Analyst code ran successfully and created these variables with the expected names.\")\n    exit(1) # Exit the executed script with an error code\nexcept Exception as e:\n    print(f\"[ERROR] An unexpected error occurred during data loading: {{e}}\")\n    exit(1) # Exit the executed script with an error code\n\n\n# --- Feature/Target Split ---\n# X is the processed features DataFrame\n# y is the target Series\nX = train\ny = train['Calories']\n\nprint(f\"Feature shape (X): {{X.shape}}\")\nprint(f\"Target shape (y): {{y.shape}}\")\n\n# --- Align Test Columns ---\ntrain_cols_expected = X.columns # Get columns in order from X\nX_test = test.copy() # Start with a copy of the test set\n\n# Add missing columns with NaN\nmissing_cols_in_test = set(train_cols_expected) - set(X_test.columns)\nfor col in missing_cols_in_test:\n    print(f\"[Warning] Column '{{col}}' missing in test set. Adding with 0 (assuming one-hot encoded, 0 is safer than NaN).\") # Changed NaN to 0, common for OHE\n    X_test[col] = 0\n\n# Select and reorder columns to match training data exactly\nextra_cols_in_test = set(X_test.columns) - set(train_cols_expected)\nif extra_cols_in_test:\n    print(f\"[Warning] Extra columns in test set that are not in train: {{extra_cols_in_test}}. Dropping them.\")\n    X_test = X_test.drop(columns=list(extra_cols_in_test))\n\nX_test = X_test[train_cols_expected] # Ensure order matches train\n\nprint(f\"Feature shape (X): {{X.shape}}\")\nprint(f\"Test Feature shape (X_test): {{X_test.shape}}\")\nprint(f\"Matching columns: {{list(X.columns) == list(X_test.columns)}}\") # Verify column matching\n\n# --- Train/Validation Split ---\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nprint(f\"Train split shape: {{X_tr.shape}}, Validation split shape: {{X_val.shape}}\")\n\n# --- Metrics ---\ndef rmsle(y_true, y_pred):\n    y_pred_safe = np.maximum(0, y_pred) # Ensure non-negative predictions\n    epsilon = 1e-9 # Add epsilon to avoid log(0)\n    return np.sqrt(mean_squared_log_error(y_true + epsilon, y_pred_safe + epsilon))\n\ndef rmse(y_true, y_pred):\n     return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# --- Model Flags ---\nlgbm_success = False\nxgb_success = False\ncan_run_xgb = True # Assume XGBoost can run initially\nlgb_error = None\nxgb_error = None\ngbm = None # Initialize model variables\nbst = None\n\n# --- LightGBM ---\nprint(\"\\\\n>>> Training LightGBM...\")\ntry:\n    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n    lgb_val   = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n    lgb_params = {{ # Dictionary literal, braces are fine\n        'objective':'regression_l1', 'metric':'rmsle', 'verbosity':-1,\n        'n_estimators': 1000, 'learning_rate':0.05, 'feature_fraction': 0.8,\n        'bagging_fraction': 0.8, 'bagging_freq': 1, 'seed': 42, 'n_jobs': -1,\n        'boosting_type': 'gbdt',\n    }}\n    lgb_evals = {{}} # Dictionary literal, braces are fine\n    callbacks = [\n        lgb.log_evaluation(period=100, show_stdv=False),\n        lgb.early_stopping(stopping_rounds=50, verbose=False)\n    ]\n\n    gbm = lgb.train(\n        params=lgb_params, train_set=lgb_train, valid_sets=[lgb_train, lgb_val],\n        valid_names=['train','val'], callbacks=callbacks, evals_result=lgb_evals\n    )\n    print(\"LightGBM training complete.\")\n    lgbm_success = True # Mark success\n\n    # Plotting\n    metrics_to_plot = [lgb_params['metric']] if isinstance(lgb_params['metric'], str) else lgb_params['metric']\n    # Filter to metrics actually present in evals_result keys (handle multiple metrics)\n    valid_metrics_to_plot = [m for m in metrics_to_plot if m in lgb_evals.get('train', {{}})]\n\n    if valid_metrics_to_plot:\n        metric_key = valid_metrics_to_plot[0] # Plot the first metric found\n        plt.figure(figsize=(8, 5))\n        plt.plot(lgb_evals['train'][metric_key], label=f'train {{metric_key}}')\n        plt.plot(lgb_evals['val'][metric_key],   label=f'val {{metric_key}}')\n        plt.title(f'LightGBM {{metric_key.upper()}}')\n        plt.legend()\n        plt.xlabel('Boosting Rounds')\n        plt.ylabel(metric_key.upper())\n        plt.grid(True)\n        lgb_plot_path = os.path.join(OUTPUT_DIR, 'lgb_metric_plot.png')\n        plt.savefig(lgb_plot_path)\n        plt.close()\n        print(f\"Saved LightGBM plot to {{lgb_plot_path}}\")\n    else:\n        print(f\"[Warning] Metrics {{metrics_to_plot}} not found in LightGBM evals results keys {{list(lgb_evals.get('train', {{}}).keys())}} for plotting.\")\n    # Predictions & Validation\n    best_iter = gbm.best_iteration if gbm.best_iteration else lgb_params.get('n_estimators', 1000) # Use best iter or total\n    y_val_pred_lgb  = gbm.predict(X_val, num_iteration=best_iter)\n    y_test_pred_lgb = gbm.predict(X_test, num_iteration=best_iter)\n    y_test_pred_lgb_safe = np.maximum(0, y_test_pred_lgb) # Ensure non-negative\n    val_rmsle_lgb = rmsle(y_val, y_val_pred_lgb)\n    print(f\"LightGBM Validation RMSLE = {{val_rmsle_lgb:.5f}} (using best iter: {{best_iter}})\")\n    lgb_pred_path = os.path.join(OUTPUT_DIR, 'lgb_test_preds.csv')\n    pd.DataFrame({{'prediction': y_test_pred_lgb_safe}}).to_csv(lgb_pred_path, index=False) # Dict literal OK\n    print(f\"Saved LightGBM predictions to {{lgb_pred_path}}\")\n\nexcept Exception as e:\n    lgb_error = str(e)\n    print(f\"[ERROR] During LightGBM training or prediction: {{e}}\")\n\n# --- XGBoost ---\nprint(\"\\\\n>>> Training XGBoost...\")\ntry:\n    # Check if DMatrix creation is possible\n    # XGBoost requires labels for training/validation sets\n    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n    dval   = xgb.DMatrix(X_val, label=y_val)\n    dtest  = xgb.DMatrix(X_test) # Test set does not need label\n\nexcept Exception as e:\n    xgb_error = f\"DMatrix creation failed: {{e}}\"\n    print(f\"[ERROR] Creating XGBoost DMatrix: {{e}}\")\n    can_run_xgb = False # Cannot run XGBoost if DMatrix fails\n\nif can_run_xgb:\n    try:\n        xgb_params = {{ # Dictionary literal, braces are fine\n            'objective':'reg:squarederror', 'eval_metric':'rmse', 'eta':0.05,\n            'max_depth': 6, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42\n        }}\n        xgb_evals = {{}} # Dictionary literal, braces are fine\n        callbacks_xgb = [\n            xgb.callback.EvaluationMonitor(period=100),\n            xgb.callback.EarlyStopping(rounds=50, verbose=False)\n        ]\n\n        bst = xgb.train(\n            params=xgb_params, dtrain=dtrain, num_boost_round=1000,\n            evals=[(dtrain,'train'), (dval,'val')],\n            callbacks=callbacks_xgb,\n            evals_result=xgb_evals\n        )\n        print(\"XGBoost training complete.\")\n        xgb_success = True # Mark success\n\n        # Plotting\n        metrics_to_plot_xgb = [xgb_params['eval_metric']] if isinstance(xgb_params['eval_metric'], str) else xgb_params['eval_metric']\n         # Filter to metrics actually present\n        valid_metrics_to_plot_xgb = [m for m in metrics_to_plot_xgb if m in xgb_evals.get('train', {{}})]\n\n\n        if valid_metrics_to_plot_xgb:\n            metric_key_xgb = valid_metrics_to_plot_xgb[0] # Plot the first metric found\n            plt.figure(figsize=(8, 5))\n            plt.plot(xgb_evals['train'][metric_key_xgb], label=f'train {{metric_key_xgb}}')\n            plt.plot(xgb_evals['val'][metric_key_xgb],   label=f'val {{metric_key_xgb}}')\n            plt.title(f'XGBoost {{metric_key_xgb.upper()}}')\n            plt.legend()\n            plt.xlabel('Boosting Rounds')\n            plt.ylabel(metric_key_xgb.upper())\n            plt.grid(True)\n            xgb_plot_path = os.path.join(OUTPUT_DIR, 'xgb_metric_plot.png')\n            plt.savefig(xgb_plot_path)\n            plt.close()\n            print(f\"Saved XGBoost plot to {{xgb_plot_path}}\")\n        else:\n             print(f\"[Warning] Metrics {{metrics_to_plot_xgb}} not found in XGBoost evals results keys {{list(xgb_evals.get('train', {{}}).keys())}} for plotting.\")\n\n\n        # Predictions & Validation\n        best_iter_xgb = bst.best_iteration\n        y_val_pred_xgb  = bst.predict(dval, iteration_range=(0, best_iter_xgb)) # Use best iteration range\n        y_test_pred_xgb = bst.predict(dtest, iteration_range=(0, best_iter_xgb))\n        y_test_pred_xgb_safe = np.maximum(0, y_test_pred_xgb) # Ensure non-negative\n        val_rmsle_xgb = rmsle(y_val, y_val_pred_xgb) # Calculate RMSLE on validation set\n        print(f\"XGBoost Validation RMSLE = {{val_rmsle_xgb:.5f}} (using best iter: {{best_iter_xgb}})\") # Report RMSLE\n        xgb_pred_path = os.path.join(OUTPUT_DIR, 'xgb_test_preds.csv')\n        pd.DataFrame({{'prediction': y_test_pred_xgb_safe}}).to_csv(xgb_pred_path, index=False) # Dict literal OK\n        print(f\"Saved XGBoost predictions to {{xgb_pred_path}}\")\n\n    except Exception as e:\n        xgb_error = str(e)\n        print(f\"[ERROR] During XGBoost training or prediction: {{e}}\")\nelse:\n    print(\"Skipping XGBoost training due to DMatrix creation error.\")\n\n# --- Final Summary ---\nprint(\"\\\\n--- Model Training Summary ---\")\nif lgbm_success:\n    print(f\"LightGBM: Successfully trained. Validation RMSLE: {{val_rmsle_lgb:.5f}}\")\n    print(f\"LightGBM predictions saved to {{os.path.join(OUTPUT_DIR, 'lgb_test_preds.csv')}}\")\nelse:\nif lgb_error:\n     print(f\"LightGBM Error: {{lgb_error}}\")\n\nif xgb_success:\n    print(f\"XGBoost: Successfully trained. Validation RMSLE: {{val_rmsle_xgb:.5f}}\")\n    print(f\"XGBoost predictions saved to {{os.path.join(OUTPUT_DIR, 'xgb_test_preds.csv')}}\")\nelif can_run_xgb: # Only report failure if it was attempted\n    print(f\"XGBoost: Training failed.\")\n\nif xgb_error and can_run_xgb: # Only report specific error if attempt was made\n     print(f\"XGBoost Error: {{xgb_error}}\")\nelif not can_run_xgb and xgb_error: # Report DMatrix error if that was the cause\n    print(f\"XGBoost was skipped due to DMatrix creation failure: {{xgb_error}}\")\n\n\nprint(\"\\\\n--- Data Scientist Code Execution Finished ---\")\n''' \n    # --- Code Generation Complete ---\n    print(\"--- Code Generation Complete ---\")\n\n    # --- Prepare Results (Code Only) ---\n    # Since we are not executing, we only return the generated code.\n    # No stdout, error capture, or artifact paths from execution.\n    scientist_result = {\n        \"code\": generated_code,\n        \"processed_train_name\": processed_train_name, # Pass the train DataFrame name\n        \"processed_test_name\": processed_test_name,   # Pass the test DataFrame name\n    }\n    \n\n    # --- Return state update ---\n    return {\n        \"scientist_output\": scientist_result,\n        \"next_agent\": \"Supervisor\",\n        \"current_task_description\": None\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:36:11.959355Z","iopub.execute_input":"2025-05-11T23:36:11.959666Z","iopub.status.idle":"2025-05-11T23:36:11.975465Z","shell.execute_reply.started":"2025-05-11T23:36:11.959642Z","shell.execute_reply":"2025-05-11T23:36:11.974360Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def bot_to_human_interpreter(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Gathers code from Analyst and Scientist outputs, blends them into a single script,\n    adds explanations within the code, and formats it for copy-pasting.\n    Returns an update dict with the combined code message and ROUTES TO THE REVIEW NODE.\n    Does NOT set final_answer_generated=True.\n    \"\"\"\n    print(\"\\n--- BOT TO HUMAN INTERPRETER (Code Compiler) ---\")\n\n    # --- Robustly retrieve and validate outputs from the state ---\n    # Note: This version assumes it's receiving state after Analyst/Scientist\n    analyst_output_raw = state.get('analyst_llm_output')  # Check LLM output first\n    scientist_output_raw = state.get('scientist_llm_output')\n    \n    if not analyst_output_raw:\n        analyst_output_raw = state.get('analyst_output')  # Fallback to original if LLM output not found\n        \n    if not scientist_output_raw:\n        scientist_output_raw = state.get('scientist_output')\n\n    # Initialize variables to avoid UnboundLocalError\n    analyst_output = {}\n    scientist_output = {}\n    \n    # Ensure they are dictionaries before proceeding\n    if analyst_output_raw:\n        analyst_output = analyst_output_raw if isinstance(analyst_output_raw, dict) else {}\n    if scientist_output_raw:\n        scientist_output = scientist_output_raw if isinstance(scientist_output_raw, dict) else {}\n    \n    # Initialize a list to hold parts of the combined script\n    combined_code_parts = []\n    \n    # Add an introductory comment/header to the script\n    combined_code_parts.append(\"# Combined Data Science and Data Analyst IA's Workflow Script\\n\")\n    combined_code_parts.append(\"# Generated by Analyst-IA's Agent Team\\n\\n\")\n\n    # Indicate if there were errors\n    analyst_error = analyst_output.get('error')\n    scientist_error = scientist_output.get('error')\n    \n    if analyst_error or scientist_error:\n        combined_code_parts.append(\"# !!! NOTE: Errors occurred during the automated workflow stages. Review the error notes below.\\n\\n\")\n\n    combined_code_parts.append(\"# This script combines preprocessing and modeling steps.\\n\")\n    combined_code_parts.append(\"# Ensure you have the necessary libraries installed (e.g., pandas, numpy, sklearn, lightgbm, xgboost, matplotlib).\\n\")\n    combined_code_parts.append(\"# Make sure your initial 'train' and 'test' DataFrames (or equivalent data loading) exist before running.\\n\\n\")\n\n    # --- Section 1: Data Preprocessing (from Data Analyst) ---\n    combined_code_parts.append(\"# --- 1. Data Preprocessing ---\\n\")\n    combined_code_parts.append(\"# This section handles steps like identifying column types, imputation, scaling, and encoding.\\n\")\n    combined_code_parts.append(\"# It prepares the raw data into processed DataFrames ready for modeling.\\n\\n\")\n\n    # Get the Analyst's code and error status (safe now due to checks above)\n    analyst_code = analyst_output.get('code', '')\n\n    # Safely get expected output names, handling cases where output is missing or incomplete\n    processed_train_name = analyst_output.get('processed_train_name', \"processed_train_name\")  # Use a reasonable default\n    processed_test_name = analyst_output.get('processed_test_name', \"processed_test_name\")    # Use a reasonable default\n\n    if analyst_error:\n        combined_code_parts.append(f\"# !!! Data Analyst Error during preprocessing execution or code generation.\\n\")\n        combined_code_parts.append(f\"# !!! Error Details: {analyst_error}\\n\\n\")\n    elif not analyst_code:\n        combined_code_parts.append(\"# Data preprocessing code was not provided by the Analyst.\\n\\n\")\n    else:\n        # Append the analyst's code\n        combined_code_parts.append(analyst_code.strip())\n        combined_code_parts.append(\"\\n\\n\")  # Add spacing after the code block\n        # Add comments explaining the *expected output variables*\n        combined_code_parts.append(\"# Expected outputs from this section (as variables in your environment):\\n\")\n        combined_code_parts.append(f\"# - {processed_train_name}: Processed training features (potentially including target)\\n\")\n        combined_code_parts.append(f\"# - {processed_test_name}: Processed test features\\n\\n\")\n\n    # --- Section 2: Model Training and Evaluation (from Data Scientist) ---\n    combined_code_parts.append(\"# --- 2. Model Training and Evaluation ---\\n\")\n    combined_code_parts.append(\"# This section uses the processed data to train machine learning models (LightGBM, XGBoost) and evaluate them.\\n\")\n    combined_code_parts.append(f\"# It assumes variables like '{processed_train_name}' and '{processed_test_name}' exist from the previous section.\\n\\n\")\n\n    # Get the Scientist's code and error status (safe now due to checks above)\n    scientist_code = scientist_output.get('code', '')\n\n    if scientist_error:\n        combined_code_parts.append(f\"# !!! Data Scientist Error during modeling execution or code generation.\\n\")\n        combined_code_parts.append(f\"# !!! Error Details: {scientist_error}\\n\\n\")\n        # Optionally include stdout even if there was an error, might contain clues\n        if scientist_stdout:\n            combined_code_parts.append(\"# Captured output (may contain error details):\\n\")\n            commented_stdout = \"\\n\".join([f\"# {line}\" for line in scientist_stdout.strip().split('\\n')])\n            combined_code_parts.append(commented_stdout + \"\\n\\n\")\n    elif not scientist_code:\n        combined_code_parts.append(\"# Model training code was not provided by the Scientist.\\n\\n\")\n    else:\n        # Append the scientist's code\n        combined_code_parts.append(scientist_code.strip())\n        combined_code_parts.append(\"\\n\\n\")  # Add spacing after the code block\n        # Add comments explaining expected outputs/artifacts\n        combined_code_parts.append(\"# Expected outputs/artifacts from this section:\\n\")\n        combined_code_parts.append(\"# - Console output showing training progress and validation scores.\\n\")\n        combined_code_parts.append(\"# - 'lgb_metric_plot.png', 'xgb_metric_plot.png': Plots showing model performance during training.\\n\")\n        combined_code_parts.append(\"# - 'lgb_test_preds.csv', 'xgb_test_preds.csv': CSV files with predictions on the test set.\\n\\n\")\n\n    # --- Section 3: Summary Notes ---\n    combined_code_parts.append(\"# --- 3. Summary Notes ---\\n\")\n    if analyst_error or scientist_error:\n        combined_code_parts.append(\"# NOTE: Review the error messages and generated code sections carefully.\\n\")\n    else:\n        combined_code_parts.append(\"# NOTE: The automated workflow appears to have completed successfully. Review the generated files and console output.\\n\")\n\n    \n        # Special case: if we have raw strings instead of structured output\n    if isinstance(analyst_output_raw, str) or isinstance(scientist_output_raw, str):\n        if isinstance(analyst_output_raw, str):\n            combined_code_parts.append(\"# --- Analyst Output ---\\n\")\n            combined_code_parts.append(analyst_output_raw)\n            combined_code_parts.append(\"\\n\\n\")\n            \n        if isinstance(scientist_output_raw, str):\n            combined_code_parts.append(\"# --- Scientist Output ---\\n\")\n            combined_code_parts.append(scientist_output_raw)\n            combined_code_parts.append(\"\\n\\n\")\n            \n        final_python_script = \"\".join(combined_code_parts)\n        \n        human_message_content = \"Here is the complete Python script generated by the workflow:\\n\\n\"\n        human_message_content += \"```python\\n\"\n        human_message_content += final_python_script\n        human_message_content += \"```\\n\"\n        print(\"Interpreter compiled code and generated message for CALOR-IA Analyst review.\")\n        \n        return {\n\n            \"analyst_llm_output\": analyst_output_raw,\n            \"scientist_llm_output\": scientist_output_raw,\n            \"messages\": state.get(\"messages\", []) + [AIMessage(content=human_message_content)],\n            \"interpreter_finished\":True,\n            \"next_agent\": \"supervisor\",\n        }\n    else:\n        if analyst_code != None:\n            \n            combined_code_parts.append(analyst_code)\n            combined_code_parts.append(\"\\n# --- End of Analyst Script ---\")\n            \n        if scientist_code != None:\n            \n            combined_code_parts.append(scientist_code)\n            combined_code_parts.append(\"\\n# --- End of Scientist Script ---\")\n    \n    combined_code_parts.append(\"\\n# --- End of Script ---\")\n    combined_code_parts.append(\"\\n# Copy and paste the entire block above into your environment/notebook to run the workflow!\\n\")\n\n    # Combine all parts into a single string representing the full script\n    final_python_script = \"\".join(combined_code_parts)\n\n    # Wrap the entire script in a single markdown code block for easy copy-pasting\n    human_message_content = \"Here is the complete Python script generated by the workflow, combining preprocessing and modeling steps:\\n\\n\"\n    human_message_content += \"```python\\n\"  # Start markdown code block\n    human_message_content += final_python_script\n    human_message_content += \"```\\n\"        # End markdown code block\n\n    print(\"Interpreter compiled code and generated message for CALOR-IA Analyst review.\")\n\n    # Prepare the state update dictionary to return\n    return {\n        \"analyst_output\": analyst_output,\n        \"scientist_output\": scientist_output,\n        \"analyst_llm_output\": analyst_output_raw,\n        \"scientist_llm_output\": scientist_output_raw,\n        \"messages\": state.get(\"messages\", []) + [AIMessage(content=human_message_content)],\n        \"interpreter_finished\":True,\n        \"next_agent\": \"supervisor\",  # Route to the review node\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:36:14.772828Z","iopub.execute_input":"2025-05-11T23:36:14.773143Z","iopub.status.idle":"2025-05-11T23:36:14.795882Z","shell.execute_reply.started":"2025-05-11T23:36:14.773118Z","shell.execute_reply":"2025-05-11T23:36:14.794735Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"# \"What if ğŸ¯ğŸš€ we build an AI Agent ğŸ¤– who can compete for me in this Kaggle competition ğŸ†ğŸ“Š?\"","metadata":{}},{"cell_type":"code","source":"llm = ChatGoogleGenerativeAI(\n    model=\"gemini-1.5-flash-latest\",\n    google_api_key=secret_value_0,\n    temperature = 0.3,\n    max_output_tokens = 5000,          \n)\n\n\n# --- Define the new CALOR_IA_DATA_ANALYST_NODE ---\ndef CALOR_IA_DATA_ANALYST_NODE(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Reviews the compiled Python script from the interpreter and generates a summary\n    or concluding message for the user using an LLM.\n    \"\"\"\n    print(\"\\n--- CALOR-IA DATA ANALYST (Post-Compilation Review) ---\")\n    \n    messages = state.get(\"messages\", []) \n    analyst_output = state.get('analyst_output')\n    # Try to get compiled script directly from state first\n    analyst_code = analyst_output.get(\"code\", \"\")\n    \n    # If no direct script found, try to extract from messages\n    if not analyst_code:\n        # Find the last message that might contain the compiled script\n        last_message = messages[-1] if messages else None\n        \n        if last_message and isinstance(last_message, AIMessage):\n            # Attempt to extract the markdown code block content\n            match = re.search(r\"```python\\n(.*)```\", last_message.content, re.DOTALL)\n            if match:\n                compiled_script_content = match.group(1).strip()\n                print(\"CALOR-IA Analyst found compiled script in message.\")\n            else:\n                print(\"CALOR-IA Analyst: Could not find script markdown block in last message.\")\n                # Try to find messages with the script data in another format\n                for msg in reversed(messages):\n                    if isinstance(msg, AIMessage) and \"# Combined Data Science and Data Analyst\" in msg.content:\n                        compiled_script_content = msg.content\n                        print(\"CALOR-IA Analyst found script content in a message without markdown.\")\n                        break\n                \n                if not compiled_script_content:\n                    compiled_script_content = last_message.content.strip() if last_message.content else \"No content found in last message.\"\n        else:\n            print(\"CALOR-IA Analyst: Last message was not an AI message or state was empty.\")\n            compiled_script_content = \"Error: Could not retrieve compiled script content.\"\n\n\n    \n    # Store the found script for other nodes to use\n    state[\"analyst_code\"] = analyst_code\n    \n    # Prepare prompt for the LLM to summarize the script\n    prompt_messages = [\n        CALOR_IA_DATA_ANALYST_SYSINT,\n        HumanMessage(content=f\"Review the following generated Python script and provide the best data processing posible :\\n\\n```python\\n{analyst_code}...\\n```\\n\\nPlease provide executable python preprocess pipeline for kaggle.\")\n    ]\n\n    print(\"CALOR-IA Analyst invoking LLM for summary...\")\n    try:\n        print(\"CALOR-IA Analyst: LLM response.\")\n        llm_response = llm.invoke(prompt_messages)\n        print(\"CALOR-IA Analyst: LLM invocation successful.\")\n        summary_message_content = str(llm_response.content)\n        print(f\"CALOR-IA Analyst IA analyst: {summary_message_content}...\")\n    except Exception as e:\n        print(f\"--- CALOR-IA Analyst Runtime Error during LLM call ---\")\n        print(f\"Error Type: {type(e).__name__}\")\n        print(f\"Error Details: {e}\")\n        summary_message_content = f\"An error occurred while generating the final summary: {e}\"\n        if not state.get('error'):\n             state['error'] = summary_message_content\n\n    # Append the new summary message to the state\n    updated_messages = messages + [AIMessage(content=summary_message_content)]\n    print(f\"CALOR-IA Analyst adding message to state: {summary_message_content[:200]}...\")\n\n    print(\"CALOR-IA Analyst routing to Supervisor.\")\n    return {\n        \"analyst_llm_output\": summary_message_content,\n        \"message\" : updated_messages, # Pass the script to other nodes\n        \"next_agent\": \"Supervisor\"\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:47:45.928143Z","iopub.execute_input":"2025-05-11T23:47:45.928586Z","iopub.status.idle":"2025-05-11T23:47:45.943743Z","shell.execute_reply.started":"2025-05-11T23:47:45.928559Z","shell.execute_reply":"2025-05-11T23:47:45.942581Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# --- Define the new CALOR_IA_DATA_ANALYST_NODE ---\ndef CALOR_IA_DATA_SCIENTIST_NODE(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Reviews the compiled Python script from the interpreter and generates a summary\n    or concluding message for the user using an LLM.\n    \"\"\"\n    print(\"\\n--- CALOR_IA_DATA_SCIENTIST (Post-Compilation Review) ---\")\n    \n    scientist_output = state.get(\"scientist_output\")\n    \n    compiled_script_content = scientist_output.get(\"code\")\n    # If not found directly, try to get from messages\n    if not compiled_script_content:\n        messages = state.get(\"messages\", [])\n        last_message = messages[-1] if messages else None\n\n        if last_message and isinstance(last_message, AIMessage):\n            # Attempt to extract the markdown code block content\n            match = re.search(r\"```python\\n(.*)```\", last_message.content, re.DOTALL)\n            if match:\n                compiled_script_content = match.group(1).strip()\n                print(\"CALOR-IA Scientist found compiled script.\")\n            else:\n                print(\"CALOR-IA Scientist: Could not find script markdown block in last message.\")\n                # Try to find script in other messages\n                for msg in reversed(messages):\n                    if isinstance(msg, AIMessage) and \"# Combined Data Science and Data Analyst\" in msg.content:\n                        compiled_script_content = msg.content\n                        print(\"CALOR-IA Scientist found script content in a message without markdown.\")\n                        break\n                \n                if not compiled_script_content:\n                    compiled_script_content = last_message.content.strip() if last_message.content else \"No content found in last message.\"\n        else:\n            print(\"CALOR-IA Scientist: Last message was not an AI message or state was empty.\")\n            compiled_script_content = \"Error: Could not retrieve compiled script content.\"\n\n    # Prepare prompt for the LLM to summarize the script\n    prompt_messages = [\n        CALOR_IA_DATA_SCIENTIST_SYSINT,\n        HumanMessage(content=f\"Review the following generated Python script and provide the same models and two new models:\\n\\n```python\\n{compiled_script_content}...\\n```\\n\\n provide two version of combine models \\n\\n Please provide your python code ready to run on Kaggle.\")\n    ]\n\n    print(\"CALOR_IA_DATA_SCIENTIST...\")\n    try:\n        print(\"CALOR-IA Scientist: LLM response.\")\n        llm_response = llm.invoke(prompt_messages)\n        print(\"CALOR-IA Scientist: LLM invocation successful.\")\n        summary_message_content = str(llm_response.content)\n        print(f\"CALOR_IA_DATA_SCIENTIST LLM response content: {summary_message_content}...\")\n    except Exception as e:\n        print(f\"--- CALOR_IA_DATA_SCIENTIST Runtime Error during LLM call ---\")\n        print(f\"Error Type: {type(e).__name__}\")\n        print(f\"Error Details: {e}\")\n        summary_message_content = f\"An error occurred while generating the final summary: {e}\"\n        if not state.get('error'):\n             state['error'] = summary_message_content\n\n    # Append the new summary message to the state\n    updated_messages = state.get(\"messages\", []) + [AIMessage(content=summary_message_content)]\n    print(f\"CALOR_IA_DATA_SCIENTIST adding message to state: {summary_message_content}...\")\n\n    print(\"CALOR-IA Scientist routing to Supervisor.\")\n    return {\n        \"scientist_llm_output\": summary_message_content,\n        \"message\": updated_messages,  # Pass the script forward\n        \"next_agent\": \"Supervisor\"\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:49:54.059522Z","iopub.execute_input":"2025-05-11T23:49:54.059885Z","iopub.status.idle":"2025-05-11T23:49:54.070719Z","shell.execute_reply.started":"2025-05-11T23:49:54.059853Z","shell.execute_reply":"2025-05-11T23:49:54.069584Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"\n\ndef CODE_COMPILER_IA(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Combines the preprocessing script with the model training script and outputs the final result.\n    \"\"\"\n    print(\"\\n--- CODE_COMPILER_IA (Combining Scripts) ---\")\n    \n    # Get preprocessing script\n    preprocessing_script = [str]\n    \n    # Get modeling script - try to find in scientist_output\n    analyst_output = state.get(\"analyst_llm_output\")\n    scientist_output = state.get(\"scientist_llm_output\")\n    preprocessing_script.append(analyst_output)\n    preprocessing_script.append(scientist_output)\n    \n    messages = state.get(\"messages\", [])\n    \n    # If we couldn't find modeling script in state, search through messages\n    if not scientist_output and not analyst_output:\n        # Find preprocessing script first (from analyst)\n        for msg in reversed(messages):\n            if isinstance(msg, AIMessage) and \"```python\" in msg.content and (\"CALOR-IA Analyst: LLM response.\" or \"CALOR-IA Scientist: LLM response.\") in msg.content:\n                match = re.search(r\"```python\\n(.*?)```\", msg.content, re.DOTALL)\n                if match:\n                    preprocessing_script = match.group(1).strip()\n                    print(\"CODE_COMPILER_IA found preprocessing script in message.\")\n                    break\n    \n    # Create the combined script\n    prompt_messages = [\n        CALOR_IA_CODE_COMPILER_SYSINT,\n        HumanMessage(content=(f\"\"\"\n        \n        you have to set up python code ready to run on Kaggle:\n        \n        data_analyst pipeline\n        {preprocessing_script[0]}.)\n        data)scientist pipeline\n        {preprocessing_script[1]}\n    ]\n    # Create the formatted final output ready to copy and paste on a kaggle cell preserve all code is crucial for this operation\n\n    \"\"\")\n                    )]\n    try:\n        print(\"CALOR-IA Scientist: LLM response.\")\n        llm_response = llm.invoke(prompt_messages)\n        print(\"CALOR-IA Scientist: LLM invocation successful.\")\n        summary_message_content = str(llm_response.content)\n        print(f\"CALOR_IA_DATA_SCIENTIST LLM response content: {summary_message_content}...\")\n    except Exception as e:\n        print(f\"--- CALOR_IA_DATA_SCIENTIST Runtime Error during LLM call ---\")\n        print(f\"Error Type: {type(e).__name__}\")\n        print(f\"Error Details: {e}\")\n        summary_message_content = f\"An error occurred while generating the final summary: {e}\"\n        if not state.get('error'):\n            state['error'] = summary_message_content\n\n\n                     \n    print(\"CODE_COMPILER_IA: Combined script generated successfully.\")\n    \n    # Return the complete combined script\n    return {\n        \"messages\": messages + [AIMessage(content=summary_message_content)],\n        \"code_final\": summary_message_content,\n        \"final_answer_generated\": True,\n        \"next_agent\": \"Supervisor\"\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:49:55.129595Z","iopub.execute_input":"2025-05-11T23:49:55.129903Z","iopub.status.idle":"2025-05-11T23:49:55.139198Z","shell.execute_reply.started":"2025-05-11T23:49:55.129882Z","shell.execute_reply":"2025-05-11T23:49:55.138109Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"workflow = StateGraph(GraphState)\n\n# Add nodes\nworkflow.add_node(\"Supervisor\", supervisor_node)\nworkflow.add_node(\"DataAnalyst\", data_analyst_node)\nworkflow.add_node(\"DataAnalyst_AI\", CALOR_IA_DATA_ANALYST_NODE)\nworkflow.add_node(\"DataSciientist_AI\", CALOR_IA_DATA_SCIENTIST_NODE)\nworkflow.add_node(\"DataScientist\", data_scientist_node)\nworkflow.add_node(\"HumanInterpreter\", bot_to_human_interpreter) # Use this exact name\nworkflow.add_node(\"code_compiler_AI\", CODE_COMPILER_IA)\n# Define entry point\nworkflow.set_entry_point(\"Supervisor\")\n\n# --- ADD THESE EDGES ---\n# After Analyst runs, go back to Supervisor\nworkflow.add_edge(\"DataAnalyst\", \"Supervisor\")\n\nworkflow.add_edge(\"DataScientist\", \"Supervisor\")\n\nworkflow.add_edge(\"HumanInterpreter\", \"Supervisor\")\n\nworkflow.add_edge(\"DataAnalyst_AI\", \"Supervisor\")\n\nworkflow.add_edge(\"DataSciientist_AI\", \"Supervisor\")\n\nworkflow.add_edge(\"code_compiler_AI\", \"Supervisor\")\n\n# --- END OF ADDED EDGES ---\n\n# Define conditional edges FROM SUPERVISOR ONLY\nworkflow.add_conditional_edges(\n    \"Supervisor\",\n    # Function to decide route based on supervisor's decision\n    lambda state: state.get(\"next_agent\"),\n    # Mapping decision to node name\n    {\n        \"DataAnalyst\": \"DataAnalyst\",\n        \"DataScientist\": \"DataScientist\",\n        \"HumanInterpreter\": \"HumanInterpreter\", # Ensure this matches add_node name\n        \"DataAnalyst_AI\": \"DataAnalyst_AI\",\n        \"DataScientist_AI\": \"DataSciientist_AI\",\n        \"code_compiler_AI\":\"code_compiler_AI\",\n        \"Supervisor\": \"Supervisor\", # Allow looping back if needed (e.g., waiting)\n        \"__end__\": END # Map \"__end__\" string to the graph's end state\n    }\n)\n\n# Compile the graph\napp = workflow.compile()\n# --- Visualize the Graph (Optional, Unchanged) ---\n'''try:\n    from PIL import Image\n    import io\n    img_bytes = app.get_graph().draw_mermaid_png()\n    img = Image.open(io.BytesIO(img_bytes))\nexcept Exception as e:\n    print(f\"\\nCould not generate graph visualization: {e}. (Might need `pip install pygraphviz` and graphviz system library)\")\n\nimg'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:49:57.476895Z","iopub.execute_input":"2025-05-11T23:49:57.477315Z","iopub.status.idle":"2025-05-11T23:49:57.514657Z","shell.execute_reply.started":"2025-05-11T23:49:57.477257Z","shell.execute_reply":"2025-05-11T23:49:57.513415Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'try:\\n    from PIL import Image\\n    import io\\n    img_bytes = app.get_graph().draw_mermaid_png()\\n    img = Image.open(io.BytesIO(img_bytes))\\nexcept Exception as e:\\n    print(f\"\\nCould not generate graph visualization: {e}. (Might need `pip install pygraphviz` and graphviz system library)\")\\n\\nimg'"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"import json # Make sure json is imported if you haven't already\n\n# Assuming imports, node definitions, graph setup, and compilation are done above\n\n# --- Run the Graph ---\n\n# Initial state setup (unchanged)\ninitial_input_message = f\"\"\" Start working ğŸ’ª\n\"\"\"\ninitial_state = {\n    \"messages\": [HumanMessage(content=initial_input_message)],\n    \"final_answer_generated\": False,\n    \"current_task_description\": \"Preprocess and analyze training and test data.\", \n}\n\nprint(\"\\n--- STARTING WORKFLOW (Using DataFrame Variable Names) ---\")\n\n# Keep track of seen message contents to avoid reprinting supervisor messages repeatedly\nseen_message_contents = set()\n\n\nfor step, event in enumerate(app.stream(initial_state, {\"recursion_limit\":20})):\n    print(f\"\\n--- Workflow Step {step + 1} ---\")\n    for node_name, update in event.items():\n        print(f\"Processing update from node: {node_name}\")\n\n        # Check if the update is valid and process messages\n        if update is None:\n            print(\"Node returned None, no state update.\")\n            continue # Skip processing if the node returned None\n\n        # Process messages added in this update\n        if 'messages' in update:\n            new_messages_in_update = [\n                msg for msg in update['messages']\n                if isinstance(msg, (AIMessage, HumanMessage)) and msg.content not in seen_message_contents\n            ]\n            for msg in new_messages_in_update:\n                if isinstance(msg, AIMessage):\n                    print(f\"ğŸ¤– {node_name} says: {msg.content}\")\n                elif isinstance(msg, HumanMessage):\n                    print(f\"ğŸ§‘â€ğŸ’» User says (via state): {msg.content}\") # User messages might reappear if state is passed\n                seen_message_contents.add(msg.content)\n\n\n        # The HumanInterpreter output is already formatted with the code block\n        # We don't need special processing here if it's added to messages\n        # The loop above processing 'messages' will print it when the HumanInterpreter runs\n        # If you wanted to handle HumanInterpreter output *differently* here, you could add:\n        # if node_name == 'HumanInterpreter':\n        #     # Access the state after the update to get the new message\n        #     final_message = update.get('messages', [])[-1] if update.get('messages') else None\n        #     if final_message and isinstance(final_message, AIMessage):\n        #          print(\"\\n--- FINAL REPORT ---\")\n        #          print(final_message.content) # Print the full markdown content\n        #          print(\"--- END FINAL REPORT ---\")\n\n\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# After the loop finishes, the graph execution has completed (__end__ reached or limit hit)\nprint(\"\\n--- Workflow Finished ---\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:49:58.556537Z","iopub.execute_input":"2025-05-11T23:49:58.556845Z","iopub.status.idle":"2025-05-11T23:50:08.073724Z","shell.execute_reply.started":"2025-05-11T23:49:58.556821Z","shell.execute_reply":"2025-05-11T23:50:08.072391Z"}},"outputs":[{"name":"stdout","text":"\n--- STARTING WORKFLOW (Using DataFrame Variable Names) ---\n\n--- SUPERVISOR ---\nSupervisor reviewing state. Last message type: HumanMessage\nSupervisor received content:  Start working ğŸ’ª\n\nOriginal Analyst output available: False\nScientist output available: False\nInterpreter finished signal: False\nFinal answer generated flag: False\nSupervisor: Initial human message received. Assigning initial task to Original Data Analyst.\nDEBUG: Supervisor RETURNING next_agent: 'DataAnalyst' (Type: <class 'str'>)\nDEBUG: Supervisor RETURNING final_answer_generated: False\nDEBUG: Supervisor RETURNING interpreter_finished: False\n\n--- Workflow Step 1 ---\nProcessing update from node: Supervisor\nğŸ§‘â€ğŸ’» User says (via state):  Start working ğŸ’ª\n\nğŸ¤– Supervisor says: Okay, starting the data analysis workflow with the Data Analyst.\n\n==================================================\n\n\n--- DATA ANALYST ---\nAnalyst expects input DataFrame variable: 'None'\nAnalyst expects input DataFrame variable: 'None'\nAnalyst will simulate creating output DataFrames: 'processed_None' and 'processed_None'\n\n--- Workflow Step 2 ---\nProcessing update from node: DataAnalyst\n\n==================================================\n\n\n--- SUPERVISOR ---\nSupervisor reviewing state. Last message type: AIMessage\nSupervisor received content: Okay, starting the data analysis workflow with the Data Analyst.\nOriginal Analyst output available: True\nScientist output available: False\nInterpreter finished signal: False\nFinal answer generated flag: False\nSupervisor: Original Data Analyst successfully generated code and data names. Routing to Data Scientist.\nSupervisor propagating names from original analyst output to Scientist: train='processed_None', test='processed_None'\nDEBUG: Supervisor RETURNING next_agent: 'DataScientist' (Type: <class 'str'>)\nDEBUG: Supervisor RETURNING final_answer_generated: False\nDEBUG: Supervisor RETURNING interpreter_finished: False\n\n--- Workflow Step 3 ---\nProcessing update from node: Supervisor\n\n==================================================\n\n\n--- DATA SCIENTIST (Code Generation Only) ---\nScientist generating code for task: Build and evaluate LightGBM and XGBoost models using the processed data.\nWill use processed train DataFrame variable: 'processed_None'\nWill use processed test DataFrame variable : 'processed_None'\n--- Code Generation Complete ---\n\n--- Workflow Step 4 ---\nProcessing update from node: DataScientist\n\n==================================================\n\n\n--- SUPERVISOR ---\nSupervisor reviewing state. Last message type: AIMessage\nSupervisor received content: Okay, starting the data analysis workflow with the Data Analyst.\nOriginal Analyst output available: True\nScientist output available: True\nInterpreter finished signal: False\nFinal answer generated flag: False\nSupervisor: Scientist code generation complete. Routing to HumanInterpreter for compilation.\nSupervisor propagating names from state for next step: train='processed_None', test='processed_None'\nDEBUG: Supervisor RETURNING next_agent: 'HumanInterpreter' (Type: <class 'str'>)\nDEBUG: Supervisor RETURNING final_answer_generated: False\nDEBUG: Supervisor RETURNING interpreter_finished: False\n\n--- Workflow Step 5 ---\nProcessing update from node: Supervisor\nğŸ¤– Supervisor says: Scientist code generation complete. Compiling final script.\n\n==================================================\n\n\n--- BOT TO HUMAN INTERPRETER (Code Compiler) ---\nInterpreter compiled code and generated message for CALOR-IA Analyst review.\n\n--- Workflow Step 6 ---\nProcessing update from node: HumanInterpreter\nğŸ¤– HumanInterpreter says: Here is the complete Python script generated by the workflow, combining preprocessing and modeling steps:\n\n```python\n# Combined Data Science and Data Analyst IA's Workflow Script\n# Generated by Analyst-IA's Agent Team\n\n# This script combines preprocessing and modeling steps.\n# Ensure you have the necessary libraries installed (e.g., pandas, numpy, sklearn, lightgbm, xgboost, matplotlib).\n# Make sure your initial 'train' and 'test' DataFrames (or equivalent data loading) exist before running.\n\n# --- 1. Data Preprocessing ---\n# This section handles steps like identifying column types, imputation, scaling, and encoding.\n# It prepares the raw data into processed DataFrames ready for modeling.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\n# Ensure correct variable names are used in the code\ntrain = None\ntest = None  # Use the initial test name here\n\n# === Separate target variable from train ===\ntarget = train['Calories']\ntrain = train.drop(columns='Calories')\n\n# 1) Identify numerical vs. categorical columns\nnum_cols = train.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(\"Numerical columns:\", num_cols)\nprint(\"Categorical columns:\", cat_cols)\n\n# 2) Build preprocessing pipelines\nnum_pipeline = Pipeline([\n    ('scaler', MinMaxScaler())\n])\ncat_pipeline = Pipeline([\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', num_pipeline, num_cols),\n    ('cat', cat_pipeline, cat_cols)\n])\n\n# 3) KNN imputer for any remaining NaNs\nimputer = KNNImputer()\n\n# 4) Fit on train, transform both train & test\ntrain_array = preprocessor.fit_transform(train)\ntrain_array = imputer.fit_transform(train_array)\n\ntest_array = preprocessor.transform(test)  # Use 'test' variable name here\ntest_array = imputer.transform(test_array)\n\n# 5) Recover feature names so we can build nice DataFrames\nnum_feats = preprocessor.named_transformers_['num']                .named_steps['scaler'].feature_names_in_\ncat_feats = preprocessor.named_transformers_['cat']                .named_steps['onehot'].get_feature_names_out(cat_cols)\nall_feats = list(num_feats) + list(cat_feats)\n\n# Create the processed DataFrames using the intended variable names\nprocessed_None = pd.DataFrame(train_array, columns=all_feats)\nprocessed_None['Calories'] = target \nprocessed_None  = pd.DataFrame(test_array,  columns=all_feats)\n\nprint(f\"Processed train shape: {processed_None.shape}\")  # Print using the variable name\nprint(f\"Processed test shape:  {processed_None.shape}\")   # Print using the variable name\n\n# Expected outputs from this section (as variables in your environment):\n# - processed_None: Processed training features (potentially including target)\n# - processed_None: Processed test features\n\n# --- 2. Model Training and Evaluation ---\n# This section uses the processed data to train machine learning models (LightGBM, XGBoost) and evaluate them.\n# It assumes variables like 'processed_None' and 'processed_None' exist from the previous section.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb\nimport warnings\nimport os # Import os to create directories if needed\n\n# --- Suppress Warnings ---\nwarnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nprint(\"\\n--- Data Scientist Code Execution Started ---\")\n\n# --- Configuration ---\n# These names are substituted when the string is created in the scientist node\nPROCESSED_TRAIN_NAME = 'processed_None'\nPROCESSED_TEST_NAME  = 'processed_None' # The variable name holding the target series (created by analyst code)\n\nOUTPUT_DIR = 'model_outputs'\n\ntrain = PROCESSED_TRAIN_NAME\ntest = PROCESSED_TEST_NAME\n# --- Setup ---\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Ensuring output directory exists: {OUTPUT_DIR}\") # Inner f-string: Use {}\n\n# --- Data Loading ---\nprint(f\"Loading data: train='{PROCESSED_TRAIN_NAME}', test='{PROCESSED_TEST_NAME}', target_var='{TARGET_VARIABLE_NAME}'\")\ntry:\n    # Attempt to load variables from the global scope where exec runs\n    # These variables are assumed to have been created by the Data Analyst's code\n    train = globals()[PROCESSED_TRAIN_NAME]        # Processed features for training\n    test = globals()[PROCESSED_TEST_NAME]         # Processed features for testing\n    target   = globals()[TARGET_VARIABLE_NAME]        # Target variable (Series)\n\n    print(f\"Loaded train shape: {train.shape}\")\n    print(f\"Loaded test shape: {test.shape}\")\n    print(f\"Loaded target shape: {target.shape}\")\n\n    # Basic sanity check: ensure number of rows match between features and target\n    if train.shape[0] != target.shape[0]:\n         print(f\"[ERROR] Row count mismatch between training features ({train.shape[0]}) and target ({target.shape[0]}).\")\n         exit(1)\n\nexcept KeyError as e:\n    print(f\"[ERROR] Required variable '{e.args[0]}' not found in the execution environment.\")\n    print(\"Please ensure the Data Analyst code ran successfully and created these variables with the expected names.\")\n    exit(1) # Exit the executed script with an error code\nexcept Exception as e:\n    print(f\"[ERROR] An unexpected error occurred during data loading: {e}\")\n    exit(1) # Exit the executed script with an error code\n\n\n# --- Feature/Target Split ---\n# X is the processed features DataFrame\n# y is the target Series\nX = train\ny = train['Calories']\n\nprint(f\"Feature shape (X): {X.shape}\")\nprint(f\"Target shape (y): {y.shape}\")\n\n# --- Align Test Columns ---\ntrain_cols_expected = X.columns # Get columns in order from X\nX_test = test.copy() # Start with a copy of the test set\n\n# Add missing columns with NaN\nmissing_cols_in_test = set(train_cols_expected) - set(X_test.columns)\nfor col in missing_cols_in_test:\n    print(f\"[Warning] Column '{col}' missing in test set. Adding with 0 (assuming one-hot encoded, 0 is safer than NaN).\") # Changed NaN to 0, common for OHE\n    X_test[col] = 0\n\n# Select and reorder columns to match training data exactly\nextra_cols_in_test = set(X_test.columns) - set(train_cols_expected)\nif extra_cols_in_test:\n    print(f\"[Warning] Extra columns in test set that are not in train: {extra_cols_in_test}. Dropping them.\")\n    X_test = X_test.drop(columns=list(extra_cols_in_test))\n\nX_test = X_test[train_cols_expected] # Ensure order matches train\n\nprint(f\"Feature shape (X): {X.shape}\")\nprint(f\"Test Feature shape (X_test): {X_test.shape}\")\nprint(f\"Matching columns: {list(X.columns) == list(X_test.columns)}\") # Verify column matching\n\n# --- Train/Validation Split ---\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nprint(f\"Train split shape: {X_tr.shape}, Validation split shape: {X_val.shape}\")\n\n# --- Metrics ---\ndef rmsle(y_true, y_pred):\n    y_pred_safe = np.maximum(0, y_pred) # Ensure non-negative predictions\n    epsilon = 1e-9 # Add epsilon to avoid log(0)\n    return np.sqrt(mean_squared_log_error(y_true + epsilon, y_pred_safe + epsilon))\n\ndef rmse(y_true, y_pred):\n     return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# --- Model Flags ---\nlgbm_success = False\nxgb_success = False\ncan_run_xgb = True # Assume XGBoost can run initially\nlgb_error = None\nxgb_error = None\ngbm = None # Initialize model variables\nbst = None\n\n# --- LightGBM ---\nprint(\"\\n>>> Training LightGBM...\")\ntry:\n    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n    lgb_val   = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n    lgb_params = { # Dictionary literal, braces are fine\n        'objective':'regression_l1', 'metric':'rmsle', 'verbosity':-1,\n        'n_estimators': 1000, 'learning_rate':0.05, 'feature_fraction': 0.8,\n        'bagging_fraction': 0.8, 'bagging_freq': 1, 'seed': 42, 'n_jobs': -1,\n        'boosting_type': 'gbdt',\n    }\n    lgb_evals = {} # Dictionary literal, braces are fine\n    callbacks = [\n        lgb.log_evaluation(period=100, show_stdv=False),\n        lgb.early_stopping(stopping_rounds=50, verbose=False)\n    ]\n\n    gbm = lgb.train(\n        params=lgb_params, train_set=lgb_train, valid_sets=[lgb_train, lgb_val],\n        valid_names=['train','val'], callbacks=callbacks, evals_result=lgb_evals\n    )\n    print(\"LightGBM training complete.\")\n    lgbm_success = True # Mark success\n\n    # Plotting\n    metrics_to_plot = [lgb_params['metric']] if isinstance(lgb_params['metric'], str) else lgb_params['metric']\n    # Filter to metrics actually present in evals_result keys (handle multiple metrics)\n    valid_metrics_to_plot = [m for m in metrics_to_plot if m in lgb_evals.get('train', {})]\n\n    if valid_metrics_to_plot:\n        metric_key = valid_metrics_to_plot[0] # Plot the first metric found\n        plt.figure(figsize=(8, 5))\n        plt.plot(lgb_evals['train'][metric_key], label=f'train {metric_key}')\n        plt.plot(lgb_evals['val'][metric_key],   label=f'val {metric_key}')\n        plt.title(f'LightGBM {metric_key.upper()}')\n        plt.legend()\n        plt.xlabel('Boosting Rounds')\n        plt.ylabel(metric_key.upper())\n        plt.grid(True)\n        lgb_plot_path = os.path.join(OUTPUT_DIR, 'lgb_metric_plot.png')\n        plt.savefig(lgb_plot_path)\n        plt.close()\n        print(f\"Saved LightGBM plot to {lgb_plot_path}\")\n    else:\n        print(f\"[Warning] Metrics {metrics_to_plot} not found in LightGBM evals results keys {list(lgb_evals.get('train', {}).keys())} for plotting.\")\n    # Predictions & Validation\n    best_iter = gbm.best_iteration if gbm.best_iteration else lgb_params.get('n_estimators', 1000) # Use best iter or total\n    y_val_pred_lgb  = gbm.predict(X_val, num_iteration=best_iter)\n    y_test_pred_lgb = gbm.predict(X_test, num_iteration=best_iter)\n    y_test_pred_lgb_safe = np.maximum(0, y_test_pred_lgb) # Ensure non-negative\n    val_rmsle_lgb = rmsle(y_val, y_val_pred_lgb)\n    print(f\"LightGBM Validation RMSLE = {val_rmsle_lgb:.5f} (using best iter: {best_iter})\")\n    lgb_pred_path = os.path.join(OUTPUT_DIR, 'lgb_test_preds.csv')\n    pd.DataFrame({'prediction': y_test_pred_lgb_safe}).to_csv(lgb_pred_path, index=False) # Dict literal OK\n    print(f\"Saved LightGBM predictions to {lgb_pred_path}\")\n\nexcept Exception as e:\n    lgb_error = str(e)\n    print(f\"[ERROR] During LightGBM training or prediction: {e}\")\n\n# --- XGBoost ---\nprint(\"\\n>>> Training XGBoost...\")\ntry:\n    # Check if DMatrix creation is possible\n    # XGBoost requires labels for training/validation sets\n    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n    dval   = xgb.DMatrix(X_val, label=y_val)\n    dtest  = xgb.DMatrix(X_test) # Test set does not need label\n\nexcept Exception as e:\n    xgb_error = f\"DMatrix creation failed: {e}\"\n    print(f\"[ERROR] Creating XGBoost DMatrix: {e}\")\n    can_run_xgb = False # Cannot run XGBoost if DMatrix fails\n\nif can_run_xgb:\n    try:\n        xgb_params = { # Dictionary literal, braces are fine\n            'objective':'reg:squarederror', 'eval_metric':'rmse', 'eta':0.05,\n            'max_depth': 6, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42\n        }\n        xgb_evals = {} # Dictionary literal, braces are fine\n        callbacks_xgb = [\n            xgb.callback.EvaluationMonitor(period=100),\n            xgb.callback.EarlyStopping(rounds=50, verbose=False)\n        ]\n\n        bst = xgb.train(\n            params=xgb_params, dtrain=dtrain, num_boost_round=1000,\n            evals=[(dtrain,'train'), (dval,'val')],\n            callbacks=callbacks_xgb,\n            evals_result=xgb_evals\n        )\n        print(\"XGBoost training complete.\")\n        xgb_success = True # Mark success\n\n        # Plotting\n        metrics_to_plot_xgb = [xgb_params['eval_metric']] if isinstance(xgb_params['eval_metric'], str) else xgb_params['eval_metric']\n         # Filter to metrics actually present\n        valid_metrics_to_plot_xgb = [m for m in metrics_to_plot_xgb if m in xgb_evals.get('train', {})]\n\n\n        if valid_metrics_to_plot_xgb:\n            metric_key_xgb = valid_metrics_to_plot_xgb[0] # Plot the first metric found\n            plt.figure(figsize=(8, 5))\n            plt.plot(xgb_evals['train'][metric_key_xgb], label=f'train {metric_key_xgb}')\n            plt.plot(xgb_evals['val'][metric_key_xgb],   label=f'val {metric_key_xgb}')\n            plt.title(f'XGBoost {metric_key_xgb.upper()}')\n            plt.legend()\n            plt.xlabel('Boosting Rounds')\n            plt.ylabel(metric_key_xgb.upper())\n            plt.grid(True)\n            xgb_plot_path = os.path.join(OUTPUT_DIR, 'xgb_metric_plot.png')\n            plt.savefig(xgb_plot_path)\n            plt.close()\n            print(f\"Saved XGBoost plot to {xgb_plot_path}\")\n        else:\n             print(f\"[Warning] Metrics {metrics_to_plot_xgb} not found in XGBoost evals results keys {list(xgb_evals.get('train', {}).keys())} for plotting.\")\n\n\n        # Predictions & Validation\n        best_iter_xgb = bst.best_iteration\n        y_val_pred_xgb  = bst.predict(dval, iteration_range=(0, best_iter_xgb)) # Use best iteration range\n        y_test_pred_xgb = bst.predict(dtest, iteration_range=(0, best_iter_xgb))\n        y_test_pred_xgb_safe = np.maximum(0, y_test_pred_xgb) # Ensure non-negative\n        val_rmsle_xgb = rmsle(y_val, y_val_pred_xgb) # Calculate RMSLE on validation set\n        print(f\"XGBoost Validation RMSLE = {val_rmsle_xgb:.5f} (using best iter: {best_iter_xgb})\") # Report RMSLE\n        xgb_pred_path = os.path.join(OUTPUT_DIR, 'xgb_test_preds.csv')\n        pd.DataFrame({'prediction': y_test_pred_xgb_safe}).to_csv(xgb_pred_path, index=False) # Dict literal OK\n        print(f\"Saved XGBoost predictions to {xgb_pred_path}\")\n\n    except Exception as e:\n        xgb_error = str(e)\n        print(f\"[ERROR] During XGBoost training or prediction: {e}\")\nelse:\n    print(\"Skipping XGBoost training due to DMatrix creation error.\")\n\n# --- Final Summary ---\nprint(\"\\n--- Model Training Summary ---\")\nif lgbm_success:\n    print(f\"LightGBM: Successfully trained. Validation RMSLE: {val_rmsle_lgb:.5f}\")\n    print(f\"LightGBM predictions saved to {os.path.join(OUTPUT_DIR, 'lgb_test_preds.csv')}\")\nelse:\nif lgb_error:\n     print(f\"LightGBM Error: {lgb_error}\")\n\nif xgb_success:\n    print(f\"XGBoost: Successfully trained. Validation RMSLE: {val_rmsle_xgb:.5f}\")\n    print(f\"XGBoost predictions saved to {os.path.join(OUTPUT_DIR, 'xgb_test_preds.csv')}\")\nelif can_run_xgb: # Only report failure if it was attempted\n    print(f\"XGBoost: Training failed.\")\n\nif xgb_error and can_run_xgb: # Only report specific error if attempt was made\n     print(f\"XGBoost Error: {xgb_error}\")\nelif not can_run_xgb and xgb_error: # Report DMatrix error if that was the cause\n    print(f\"XGBoost was skipped due to DMatrix creation failure: {xgb_error}\")\n\n\nprint(\"\\n--- Data Scientist Code Execution Finished ---\")\n\n# Expected outputs/artifacts from this section:\n# - Console output showing training progress and validation scores.\n# - 'lgb_metric_plot.png', 'xgb_metric_plot.png': Plots showing model performance during training.\n# - 'lgb_test_preds.csv', 'xgb_test_preds.csv': CSV files with predictions on the test set.\n\n# --- 3. Summary Notes ---\n# NOTE: The automated workflow appears to have completed successfully. Review the generated files and console output.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\n# Ensure correct variable names are used in the code\ntrain = None\ntest = None  # Use the initial test name here\n\n# === Separate target variable from train ===\ntarget = train['Calories']\ntrain = train.drop(columns='Calories')\n\n# 1) Identify numerical vs. categorical columns\nnum_cols = train.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(\"Numerical columns:\", num_cols)\nprint(\"Categorical columns:\", cat_cols)\n\n# 2) Build preprocessing pipelines\nnum_pipeline = Pipeline([\n    ('scaler', MinMaxScaler())\n])\ncat_pipeline = Pipeline([\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', num_pipeline, num_cols),\n    ('cat', cat_pipeline, cat_cols)\n])\n\n# 3) KNN imputer for any remaining NaNs\nimputer = KNNImputer()\n\n# 4) Fit on train, transform both train & test\ntrain_array = preprocessor.fit_transform(train)\ntrain_array = imputer.fit_transform(train_array)\n\ntest_array = preprocessor.transform(test)  # Use 'test' variable name here\ntest_array = imputer.transform(test_array)\n\n# 5) Recover feature names so we can build nice DataFrames\nnum_feats = preprocessor.named_transformers_['num']                .named_steps['scaler'].feature_names_in_\ncat_feats = preprocessor.named_transformers_['cat']                .named_steps['onehot'].get_feature_names_out(cat_cols)\nall_feats = list(num_feats) + list(cat_feats)\n\n# Create the processed DataFrames using the intended variable names\nprocessed_None = pd.DataFrame(train_array, columns=all_feats)\nprocessed_None['Calories'] = target \nprocessed_None  = pd.DataFrame(test_array,  columns=all_feats)\n\nprint(f\"Processed train shape: {processed_None.shape}\")  # Print using the variable name\nprint(f\"Processed test shape:  {processed_None.shape}\")   # Print using the variable name\n    \n# --- End of Analyst Script ---\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb\nimport warnings\nimport os # Import os to create directories if needed\n\n# --- Suppress Warnings ---\nwarnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nprint(\"\\n--- Data Scientist Code Execution Started ---\")\n\n# --- Configuration ---\n# These names are substituted when the string is created in the scientist node\nPROCESSED_TRAIN_NAME = 'processed_None'\nPROCESSED_TEST_NAME  = 'processed_None' # The variable name holding the target series (created by analyst code)\n\nOUTPUT_DIR = 'model_outputs'\n\ntrain = PROCESSED_TRAIN_NAME\ntest = PROCESSED_TEST_NAME\n# --- Setup ---\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nprint(f\"Ensuring output directory exists: {OUTPUT_DIR}\") # Inner f-string: Use {}\n\n# --- Data Loading ---\nprint(f\"Loading data: train='{PROCESSED_TRAIN_NAME}', test='{PROCESSED_TEST_NAME}', target_var='{TARGET_VARIABLE_NAME}'\")\ntry:\n    # Attempt to load variables from the global scope where exec runs\n    # These variables are assumed to have been created by the Data Analyst's code\n    train = globals()[PROCESSED_TRAIN_NAME]        # Processed features for training\n    test = globals()[PROCESSED_TEST_NAME]         # Processed features for testing\n    target   = globals()[TARGET_VARIABLE_NAME]        # Target variable (Series)\n\n    print(f\"Loaded train shape: {train.shape}\")\n    print(f\"Loaded test shape: {test.shape}\")\n    print(f\"Loaded target shape: {target.shape}\")\n\n    # Basic sanity check: ensure number of rows match between features and target\n    if train.shape[0] != target.shape[0]:\n         print(f\"[ERROR] Row count mismatch between training features ({train.shape[0]}) and target ({target.shape[0]}).\")\n         exit(1)\n\nexcept KeyError as e:\n    print(f\"[ERROR] Required variable '{e.args[0]}' not found in the execution environment.\")\n    print(\"Please ensure the Data Analyst code ran successfully and created these variables with the expected names.\")\n    exit(1) # Exit the executed script with an error code\nexcept Exception as e:\n    print(f\"[ERROR] An unexpected error occurred during data loading: {e}\")\n    exit(1) # Exit the executed script with an error code\n\n\n# --- Feature/Target Split ---\n# X is the processed features DataFrame\n# y is the target Series\nX = train\ny = train['Calories']\n\nprint(f\"Feature shape (X): {X.shape}\")\nprint(f\"Target shape (y): {y.shape}\")\n\n# --- Align Test Columns ---\ntrain_cols_expected = X.columns # Get columns in order from X\nX_test = test.copy() # Start with a copy of the test set\n\n# Add missing columns with NaN\nmissing_cols_in_test = set(train_cols_expected) - set(X_test.columns)\nfor col in missing_cols_in_test:\n    print(f\"[Warning] Column '{col}' missing in test set. Adding with 0 (assuming one-hot encoded, 0 is safer than NaN).\") # Changed NaN to 0, common for OHE\n    X_test[col] = 0\n\n# Select and reorder columns to match training data exactly\nextra_cols_in_test = set(X_test.columns) - set(train_cols_expected)\nif extra_cols_in_test:\n    print(f\"[Warning] Extra columns in test set that are not in train: {extra_cols_in_test}. Dropping them.\")\n    X_test = X_test.drop(columns=list(extra_cols_in_test))\n\nX_test = X_test[train_cols_expected] # Ensure order matches train\n\nprint(f\"Feature shape (X): {X.shape}\")\nprint(f\"Test Feature shape (X_test): {X_test.shape}\")\nprint(f\"Matching columns: {list(X.columns) == list(X_test.columns)}\") # Verify column matching\n\n# --- Train/Validation Split ---\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nprint(f\"Train split shape: {X_tr.shape}, Validation split shape: {X_val.shape}\")\n\n# --- Metrics ---\ndef rmsle(y_true, y_pred):\n    y_pred_safe = np.maximum(0, y_pred) # Ensure non-negative predictions\n    epsilon = 1e-9 # Add epsilon to avoid log(0)\n    return np.sqrt(mean_squared_log_error(y_true + epsilon, y_pred_safe + epsilon))\n\ndef rmse(y_true, y_pred):\n     return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# --- Model Flags ---\nlgbm_success = False\nxgb_success = False\ncan_run_xgb = True # Assume XGBoost can run initially\nlgb_error = None\nxgb_error = None\ngbm = None # Initialize model variables\nbst = None\n\n# --- LightGBM ---\nprint(\"\\n>>> Training LightGBM...\")\ntry:\n    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n    lgb_val   = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n    lgb_params = { # Dictionary literal, braces are fine\n        'objective':'regression_l1', 'metric':'rmsle', 'verbosity':-1,\n        'n_estimators': 1000, 'learning_rate':0.05, 'feature_fraction': 0.8,\n        'bagging_fraction': 0.8, 'bagging_freq': 1, 'seed': 42, 'n_jobs': -1,\n        'boosting_type': 'gbdt',\n    }\n    lgb_evals = {} # Dictionary literal, braces are fine\n    callbacks = [\n        lgb.log_evaluation(period=100, show_stdv=False),\n        lgb.early_stopping(stopping_rounds=50, verbose=False)\n    ]\n\n    gbm = lgb.train(\n        params=lgb_params, train_set=lgb_train, valid_sets=[lgb_train, lgb_val],\n        valid_names=['train','val'], callbacks=callbacks, evals_result=lgb_evals\n    )\n    print(\"LightGBM training complete.\")\n    lgbm_success = True # Mark success\n\n    # Plotting\n    metrics_to_plot = [lgb_params['metric']] if isinstance(lgb_params['metric'], str) else lgb_params['metric']\n    # Filter to metrics actually present in evals_result keys (handle multiple metrics)\n    valid_metrics_to_plot = [m for m in metrics_to_plot if m in lgb_evals.get('train', {})]\n\n    if valid_metrics_to_plot:\n        metric_key = valid_metrics_to_plot[0] # Plot the first metric found\n        plt.figure(figsize=(8, 5))\n        plt.plot(lgb_evals['train'][metric_key], label=f'train {metric_key}')\n        plt.plot(lgb_evals['val'][metric_key],   label=f'val {metric_key}')\n        plt.title(f'LightGBM {metric_key.upper()}')\n        plt.legend()\n        plt.xlabel('Boosting Rounds')\n        plt.ylabel(metric_key.upper())\n        plt.grid(True)\n        lgb_plot_path = os.path.join(OUTPUT_DIR, 'lgb_metric_plot.png')\n        plt.savefig(lgb_plot_path)\n        plt.close()\n        print(f\"Saved LightGBM plot to {lgb_plot_path}\")\n    else:\n        print(f\"[Warning] Metrics {metrics_to_plot} not found in LightGBM evals results keys {list(lgb_evals.get('train', {}).keys())} for plotting.\")\n    # Predictions & Validation\n    best_iter = gbm.best_iteration if gbm.best_iteration else lgb_params.get('n_estimators', 1000) # Use best iter or total\n    y_val_pred_lgb  = gbm.predict(X_val, num_iteration=best_iter)\n    y_test_pred_lgb = gbm.predict(X_test, num_iteration=best_iter)\n    y_test_pred_lgb_safe = np.maximum(0, y_test_pred_lgb) # Ensure non-negative\n    val_rmsle_lgb = rmsle(y_val, y_val_pred_lgb)\n    print(f\"LightGBM Validation RMSLE = {val_rmsle_lgb:.5f} (using best iter: {best_iter})\")\n    lgb_pred_path = os.path.join(OUTPUT_DIR, 'lgb_test_preds.csv')\n    pd.DataFrame({'prediction': y_test_pred_lgb_safe}).to_csv(lgb_pred_path, index=False) # Dict literal OK\n    print(f\"Saved LightGBM predictions to {lgb_pred_path}\")\n\nexcept Exception as e:\n    lgb_error = str(e)\n    print(f\"[ERROR] During LightGBM training or prediction: {e}\")\n\n# --- XGBoost ---\nprint(\"\\n>>> Training XGBoost...\")\ntry:\n    # Check if DMatrix creation is possible\n    # XGBoost requires labels for training/validation sets\n    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n    dval   = xgb.DMatrix(X_val, label=y_val)\n    dtest  = xgb.DMatrix(X_test) # Test set does not need label\n\nexcept Exception as e:\n    xgb_error = f\"DMatrix creation failed: {e}\"\n    print(f\"[ERROR] Creating XGBoost DMatrix: {e}\")\n    can_run_xgb = False # Cannot run XGBoost if DMatrix fails\n\nif can_run_xgb:\n    try:\n        xgb_params = { # Dictionary literal, braces are fine\n            'objective':'reg:squarederror', 'eval_metric':'rmse', 'eta':0.05,\n            'max_depth': 6, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42\n        }\n        xgb_evals = {} # Dictionary literal, braces are fine\n        callbacks_xgb = [\n            xgb.callback.EvaluationMonitor(period=100),\n            xgb.callback.EarlyStopping(rounds=50, verbose=False)\n        ]\n\n        bst = xgb.train(\n            params=xgb_params, dtrain=dtrain, num_boost_round=1000,\n            evals=[(dtrain,'train'), (dval,'val')],\n            callbacks=callbacks_xgb,\n            evals_result=xgb_evals\n        )\n        print(\"XGBoost training complete.\")\n        xgb_success = True # Mark success\n\n        # Plotting\n        metrics_to_plot_xgb = [xgb_params['eval_metric']] if isinstance(xgb_params['eval_metric'], str) else xgb_params['eval_metric']\n         # Filter to metrics actually present\n        valid_metrics_to_plot_xgb = [m for m in metrics_to_plot_xgb if m in xgb_evals.get('train', {})]\n\n\n        if valid_metrics_to_plot_xgb:\n            metric_key_xgb = valid_metrics_to_plot_xgb[0] # Plot the first metric found\n            plt.figure(figsize=(8, 5))\n            plt.plot(xgb_evals['train'][metric_key_xgb], label=f'train {metric_key_xgb}')\n            plt.plot(xgb_evals['val'][metric_key_xgb],   label=f'val {metric_key_xgb}')\n            plt.title(f'XGBoost {metric_key_xgb.upper()}')\n            plt.legend()\n            plt.xlabel('Boosting Rounds')\n            plt.ylabel(metric_key_xgb.upper())\n            plt.grid(True)\n            xgb_plot_path = os.path.join(OUTPUT_DIR, 'xgb_metric_plot.png')\n            plt.savefig(xgb_plot_path)\n            plt.close()\n            print(f\"Saved XGBoost plot to {xgb_plot_path}\")\n        else:\n             print(f\"[Warning] Metrics {metrics_to_plot_xgb} not found in XGBoost evals results keys {list(xgb_evals.get('train', {}).keys())} for plotting.\")\n\n\n        # Predictions & Validation\n        best_iter_xgb = bst.best_iteration\n        y_val_pred_xgb  = bst.predict(dval, iteration_range=(0, best_iter_xgb)) # Use best iteration range\n        y_test_pred_xgb = bst.predict(dtest, iteration_range=(0, best_iter_xgb))\n        y_test_pred_xgb_safe = np.maximum(0, y_test_pred_xgb) # Ensure non-negative\n        val_rmsle_xgb = rmsle(y_val, y_val_pred_xgb) # Calculate RMSLE on validation set\n        print(f\"XGBoost Validation RMSLE = {val_rmsle_xgb:.5f} (using best iter: {best_iter_xgb})\") # Report RMSLE\n        xgb_pred_path = os.path.join(OUTPUT_DIR, 'xgb_test_preds.csv')\n        pd.DataFrame({'prediction': y_test_pred_xgb_safe}).to_csv(xgb_pred_path, index=False) # Dict literal OK\n        print(f\"Saved XGBoost predictions to {xgb_pred_path}\")\n\n    except Exception as e:\n        xgb_error = str(e)\n        print(f\"[ERROR] During XGBoost training or prediction: {e}\")\nelse:\n    print(\"Skipping XGBoost training due to DMatrix creation error.\")\n\n# --- Final Summary ---\nprint(\"\\n--- Model Training Summary ---\")\nif lgbm_success:\n    print(f\"LightGBM: Successfully trained. Validation RMSLE: {val_rmsle_lgb:.5f}\")\n    print(f\"LightGBM predictions saved to {os.path.join(OUTPUT_DIR, 'lgb_test_preds.csv')}\")\nelse:\nif lgb_error:\n     print(f\"LightGBM Error: {lgb_error}\")\n\nif xgb_success:\n    print(f\"XGBoost: Successfully trained. Validation RMSLE: {val_rmsle_xgb:.5f}\")\n    print(f\"XGBoost predictions saved to {os.path.join(OUTPUT_DIR, 'xgb_test_preds.csv')}\")\nelif can_run_xgb: # Only report failure if it was attempted\n    print(f\"XGBoost: Training failed.\")\n\nif xgb_error and can_run_xgb: # Only report specific error if attempt was made\n     print(f\"XGBoost Error: {xgb_error}\")\nelif not can_run_xgb and xgb_error: # Report DMatrix error if that was the cause\n    print(f\"XGBoost was skipped due to DMatrix creation failure: {xgb_error}\")\n\n\nprint(\"\\n--- Data Scientist Code Execution Finished ---\")\n\n# --- End of Scientist Script ---\n# --- End of Script ---\n# Copy and paste the entire block above into your environment/notebook to run the workflow!\n```\n\n\n==================================================\n\n\n--- SUPERVISOR ---\nSupervisor reviewing state. Last message type: AIMessage\nSupervisor received content: Here is the complete Python script generated by the workflow, combining preprocessing and modeling steps:\n\n```python\n# Combined Data Science and Data Analyst IA's Workflow Script\n# Generated by Analys...\nOriginal Analyst output available: True\nScientist output available: True\nInterpreter finished signal: True\nFinal answer generated flag: False\nSupervisor: Scientist AI review complete. Final answer should be ready.\nDEBUG: Supervisor RETURNING next_agent: 'code_compiler_AI' (Type: <class 'str'>)\nDEBUG: Supervisor RETURNING final_answer_generated: True\nDEBUG: Supervisor RETURNING interpreter_finished: True\n\n--- Workflow Step 7 ---\nProcessing update from node: Supervisor\nğŸ¤– Supervisor says: Workflow complete. Final script and reviews have been generated.\n\n==================================================\n\n\n--- CODE_COMPILER_IA (Combining Scripts) ---\nCALOR-IA Scientist: LLM response.\nCALOR-IA Scientist: LLM invocation successful.\nCALOR_IA_DATA_SCIENTIST LLM response content: ```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor #Example model.  Change as needed.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the datasets\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')\n\n# Separate target variable from train\ntarget = train['Calories']\ntrain = train.drop(columns='Calories')\n\n# 1) Identify numerical vs. categorical columns\nnum_cols = train.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(\"Numerical columns:\", num_cols)\nprint(\"Categorical columns:\", cat_cols)\n\n# 2) Build preprocessing pipelines\nnum_pipeline = Pipeline([\n    ('scaler', MinMaxScaler())\n])\ncat_pipeline = Pipeline([\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', num_pipeline, num_cols),\n    ('cat', cat_pipeline, cat_cols)\n])\n\n# 3) KNN imputer for any remaining NaNs\nimputer = KNNImputer()\n\n# 4) Fit on train, transform both train & test\ntrain_array = preprocessor.fit_transform(train)\ntrain_array = imputer.fit_transform(train_array)\n\ntest_array = preprocessor.transform(test)\ntest_array = imputer.transform(test_array)\n\n# 5) Recover feature names\nnum_feats = preprocessor.named_transformers_['num'].named_steps['scaler'].feature_names_in_\ncat_feats = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)\nall_feats = list(num_feats) + list(cat_feats)\n\n# Create the processed DataFrames\nprocessed_train = pd.DataFrame(train_array, columns=all_feats)\nprocessed_train['Calories'] = target\nprocessed_test = pd.DataFrame(test_array, columns=all_feats)\n\nprint(f\"Processed train shape: {processed_train.shape}\")\nprint(f\"Processed test shape: {processed_test.shape}\")\n\n#Model Training (Example using RandomForestRegressor)\nX_train, X_val, y_train, y_val = train_test_split(processed_train.drop('Calories', axis=1), target, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(random_state=42) #Change model here if needed.\nmodel.fit(X_train, y_train)\n\n#Prediction\npredictions = model.predict(processed_test)\nsubmission['Calories'] = predictions\n\n#Feature Importance\nfeature_importances = pd.DataFrame({'Feature': all_feats, 'Importance': model.feature_importances_})\nfeature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importances)\nplt.title('Feature Importance')\nplt.show()\n\n\n#Save submission\nsubmission.to_csv('submission.csv', index=False)\n\n```...\nCODE_COMPILER_IA: Combined script generated successfully.\n\n--- Workflow Step 8 ---\nProcessing update from node: code_compiler_AI\nğŸ¤– code_compiler_AI says: ```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor #Example model.  Change as needed.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the datasets\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')\n\n# Separate target variable from train\ntarget = train['Calories']\ntrain = train.drop(columns='Calories')\n\n# 1) Identify numerical vs. categorical columns\nnum_cols = train.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(\"Numerical columns:\", num_cols)\nprint(\"Categorical columns:\", cat_cols)\n\n# 2) Build preprocessing pipelines\nnum_pipeline = Pipeline([\n    ('scaler', MinMaxScaler())\n])\ncat_pipeline = Pipeline([\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', num_pipeline, num_cols),\n    ('cat', cat_pipeline, cat_cols)\n])\n\n# 3) KNN imputer for any remaining NaNs\nimputer = KNNImputer()\n\n# 4) Fit on train, transform both train & test\ntrain_array = preprocessor.fit_transform(train)\ntrain_array = imputer.fit_transform(train_array)\n\ntest_array = preprocessor.transform(test)\ntest_array = imputer.transform(test_array)\n\n# 5) Recover feature names\nnum_feats = preprocessor.named_transformers_['num'].named_steps['scaler'].feature_names_in_\ncat_feats = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)\nall_feats = list(num_feats) + list(cat_feats)\n\n# Create the processed DataFrames\nprocessed_train = pd.DataFrame(train_array, columns=all_feats)\nprocessed_train['Calories'] = target\nprocessed_test = pd.DataFrame(test_array, columns=all_feats)\n\nprint(f\"Processed train shape: {processed_train.shape}\")\nprint(f\"Processed test shape: {processed_test.shape}\")\n\n#Model Training (Example using RandomForestRegressor)\nX_train, X_val, y_train, y_val = train_test_split(processed_train.drop('Calories', axis=1), target, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(random_state=42) #Change model here if needed.\nmodel.fit(X_train, y_train)\n\n#Prediction\npredictions = model.predict(processed_test)\nsubmission['Calories'] = predictions\n\n#Feature Importance\nfeature_importances = pd.DataFrame({'Feature': all_feats, 'Importance': model.feature_importances_})\nfeature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importances)\nplt.title('Feature Importance')\nplt.show()\n\n\n#Save submission\nsubmission.to_csv('submission.csv', index=False)\n\n```\n\n==================================================\n\n\n--- SUPERVISOR ---\nSupervisor reviewing state. Last message type: AIMessage\nSupervisor received content: ```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom ...\nOriginal Analyst output available: True\nScientist output available: True\nInterpreter finished signal: True\nFinal answer generated flag: True\nSupervisor: Final answer signal received. Ending workflow.\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor #Example model.  Change as needed.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the datasets\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')\n\n# Separate target variable from train\ntarget = train['Calories']\ntrain = train.drop(columns='Calories')\n\n# 1) Identify numerical vs. categorical columns\nnum_cols = train.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(\"Numerical columns:\", num_cols)\nprint(\"Categorical columns:\", cat_cols)\n\n# 2) Build preprocessing pipelines\nnum_pipeline = Pipeline([\n    ('scaler', MinMaxScaler())\n])\ncat_pipeline = Pipeline([\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', num_pipeline, num_cols),\n    ('cat', cat_pipeline, cat_cols)\n])\n\n# 3) KNN imputer for any remaining NaNs\nimputer = KNNImputer()\n\n# 4) Fit on train, transform both train & test\ntrain_array = preprocessor.fit_transform(train)\ntrain_array = imputer.fit_transform(train_array)\n\ntest_array = preprocessor.transform(test)\ntest_array = imputer.transform(test_array)\n\n# 5) Recover feature names\nnum_feats = preprocessor.named_transformers_['num'].named_steps['scaler'].feature_names_in_\ncat_feats = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)\nall_feats = list(num_feats) + list(cat_feats)\n\n# Create the processed DataFrames\nprocessed_train = pd.DataFrame(train_array, columns=all_feats)\nprocessed_train['Calories'] = target\nprocessed_test = pd.DataFrame(test_array, columns=all_feats)\n\nprint(f\"Processed train shape: {processed_train.shape}\")\nprint(f\"Processed test shape: {processed_test.shape}\")\n\n#Model Training (Example using RandomForestRegressor)\nX_train, X_val, y_train, y_val = train_test_split(processed_train.drop('Calories', axis=1), target, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(random_state=42) #Change model here if needed.\nmodel.fit(X_train, y_train)\n\n#Prediction\npredictions = model.predict(processed_test)\nsubmission['Calories'] = predictions\n\n#Feature Importance\nfeature_importances = pd.DataFrame({'Feature': all_feats, 'Importance': model.feature_importances_})\nfeature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importances)\nplt.title('Feature Importance')\nplt.show()\n\n\n#Save submission\nsubmission.to_csv('submission.csv', index=False)\n\n```\nDEBUG: Supervisor RETURNING next_agent: '__end__' (Type: <class 'str'>)\nDEBUG: Supervisor RETURNING final_answer_generated: True\nDEBUG: Supervisor RETURNING interpreter_finished: True\n\n--- Workflow Step 9 ---\nProcessing update from node: Supervisor\n\n==================================================\n\n\n--- Workflow Finished ---\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"# **Ready to Run!**\n\n**This code is designed to be copied directly into a cell in your Kaggle notebook. Just make sure you have already loaded your initial train.csv and test.csv files into DataFrames named train and test respectively, before this code block.**\n\n**The script will then:**\n\n**ğŸš€ Preprocess train and test using the defined pipelines and KNN Imputer, saving the results as processed_train_name and processed_test_name.**\n**ğŸ§  This version is ready to copy and paste so my goal was made and I am very produ of me if you read this message please leave a comment**\n**ğŸ“Š Now I  have to find how to make them make me win, or maybe you can do it for me**\n**ğŸ’¾ you can make a node to execute the code the we do not have to copy and paste the code, or you can play with LLM for each agent**\n**What's Next?**\n\n****\n\n**This workflow shows just how powerful collaborative AI agents can be in jumpstarting your machine learning projects. Go ahead, copy the code, run it, and see the magic happen! âœ¨ Let me know what you build next in the comments!**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor #Example model.  Change as needed.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the datasets\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')\n\n# Separate target variable from train\ntarget = train['Calories']\ntrain = train.drop(columns='Calories')\n\n# 1) Identify numerical vs. categorical columns\nnum_cols = train.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\n\nprint(\"Numerical columns:\", num_cols)\nprint(\"Categorical columns:\", cat_cols)\n\n# 2) Build preprocessing pipelines\nnum_pipeline = Pipeline([\n    ('scaler', MinMaxScaler())\n])\ncat_pipeline = Pipeline([\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', num_pipeline, num_cols),\n    ('cat', cat_pipeline, cat_cols)\n])\n\n# 3) KNN imputer for any remaining NaNs\nimputer = KNNImputer()\n\n# 4) Fit on train, transform both train & test\ntrain_array = preprocessor.fit_transform(train)\ntrain_array = imputer.fit_transform(train_array)\n\ntest_array = preprocessor.transform(test)\ntest_array = imputer.transform(test_array)\n\n# 5) Recover feature names\nnum_feats = preprocessor.named_transformers_['num'].named_steps['scaler'].feature_names_in_\ncat_feats = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)\nall_feats = list(num_feats) + list(cat_feats)\n\n# Create the processed DataFrames\nprocessed_train = pd.DataFrame(train_array, columns=all_feats)\nprocessed_train['Calories'] = target\nprocessed_test = pd.DataFrame(test_array, columns=all_feats)\n\nprint(f\"Processed train shape: {processed_train.shape}\")\nprint(f\"Processed test shape: {processed_test.shape}\")\n\n#Model Training (Example using RandomForestRegressor)\nX_train, X_val, y_train, y_val = train_test_split(processed_train.drop('Calories', axis=1), target, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(random_state=42) #Change model here if needed.\nmodel.fit(X_train, y_train)\n\n#Prediction\npredictions = model.predict(processed_test)\nsubmission['Calories'] = predictions\n\n#Feature Importance\nfeature_importances = pd.DataFrame({'Feature': all_feats, 'Importance': model.feature_importances_})\nfeature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importances)\nplt.title('Feature Importance')\nplt.show()\n\n\n#Save submission\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T23:57:27.920595Z","iopub.execute_input":"2025-05-11T23:57:27.920891Z"}},"outputs":[{"name":"stdout","text":"Numerical columns: ['id', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\nCategorical columns: ['Sex']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Processed train shape: (750000, 10)\nProcessed test shape: (250000, 9)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}